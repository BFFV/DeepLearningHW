{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPiZMvmoAha3"
   },
   "source": [
    "## README\n",
    "- Se hicieron todas las actividades.\n",
    "- Tuve problemas con los graficos en la ultima parte porque el runtime se me desconectaba constantemente (por problemas de conexion - COVID )\n",
    "\n",
    "Fuentes de insipiracion y base para los modelos/procesamiento utilizado:\n",
    "- https://github.com/bentrevett/pytorch-sentiment-analysis\n",
    "- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training-and-evaluating (Este lo encontre en el paper que se referencia en el SCAN DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PJ_OMs8HxJoF"
   },
   "source": [
    "# Parte 1 (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T0xp7mWCgxk"
   },
   "source": [
    "## Def RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEaYDaBWxVQu"
   },
   "outputs": [],
   "source": [
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_layers = output_dim\n",
    "\n",
    "    # Agregamos un embedding como entrada a la RNN. \n",
    "    self.embedding = nn.Embedding(input_dim,embedding_dim)\n",
    "\n",
    "    # A diferencia del Modulo presentado en el enunciado, no se usa batch first en este modulo.\n",
    "    # Es decir las dimensiones son (largo, batch_size)\n",
    "    self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    self.linear_out = nn.Linear(hidden_dim, 1)\n",
    "  def forward(self, input):\n",
    "    #text = [sent len, batch size]\n",
    "    \n",
    "    embedded = self.embedding(input)\n",
    "    \n",
    "    #embedded = [sent len, batch size, emb dim]\n",
    "    \n",
    "    output, hidden = self.rnn(embedded)\n",
    "    \n",
    "    #output = [sent len, batch size, hid dim]\n",
    "    #hidden = [1, batch size, hid dim]\n",
    "    assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "      \n",
    "    return self.linear_out(hidden.squeeze(0))\n",
    "  def init_hidden(self, batch_size):\n",
    "    hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    return hidden\n",
    " \n",
    "def num_trainable_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def list_trainable_parameters(model):\n",
    "  return [p.numel() for p in model.parameters() if p.requires_grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sY4O2uuoE4nu",
    "outputId": "d682f501-b99f-4945-c74a-0389b9d47796"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "aclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 39.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "TEXT = data.Field(lower=True,tokenize='spacy')\n",
    "## include_lengths = true para user padded sequences\n",
    "LABEL=  data.LabelField(dtype=torch.float)\n",
    "train,test = datasets.IMDB.splits(TEXT,LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsPfjcEkdDNr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1\n",
    "train, validation = train.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdmmOVg5d1XU"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, validation, test), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "THQgoAfaE7x9",
    "outputId": "7b93a385-a209-44ae-b6b5-163db880e269"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                          \n",
      "100%|█████████▉| 399178/400000 [00:25<00:00, 15167.46it/s]"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "TEXT.build_vocab(train,vectors='glove.6B.100d',max_size= MAX_VOCAB_SIZE,unk_init=torch.Tensor.normal_,min_freq=10)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LSaZ5RDh9Djc",
    "outputId": "9654dc4a-b743-40f5-babe-c012a0e1c8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17357"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZHkI0vql9qQm",
    "outputId": "8f4bcfb7-a0ad-4ba1-94ea-ac73e07f7209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1756001"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Agregamos a matriz de pesos el embedding pre-entrenado de GloVe\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "num_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mzx79aYeGCqd"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2FSF5MXhL9n"
   },
   "source": [
    "## Actividad 1\n",
    "(Respuesta Pregunta 1)\n",
    "- Si se aumenta el tamaño de las oraciones, se podria tomar la decision de modificiar el modelo (capas ocultas) para que se adapte de mejor manera. \n",
    "- Si aumenta el numero de palabras en el diccionario, cambiaria el Embedding y por lo tanto el numero de parametros. \n",
    "- Si el dataset se mantiene con el mismo largo de palabras y oraciones, es decir solo aumenta el tamaño en filas. Es el unico caso donde no cambiarian los numeros de parametros.\n",
    "(Respuesta Pregunta 2)\n",
    "- Disminuyendo el numero de palabras en el diccionario y por lo tanto en el embedding. \n",
    "- Disminuyendo el largo de las oraciones\n",
    "- Obtener solo el 95% de palabras mas frequentes por ejemplo. En esta tarea se elimino las palabras que tenian una frecuencia menor a 10 para eliminar correos o palabras mal escritas.  Esto reduce el numero de parametros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K-IjQYQuhgO4",
    "outputId": "e04a663b-384d-4045-d3ce-53b4538beebb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1756001"
      ]
     },
     "execution_count": 220,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trainable_parameters(model) ## hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RvWormLhoXA3",
    "outputId": "a39f11a4-7a81-4f0a-a78a-38658fb044da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1735700, 10000, 10000, 100, 100, 100, 1]"
      ]
     },
     "execution_count": 221,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_trainable_parameters(model) ## hidden =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fFJYGP2togfr",
    "outputId": "7ef88381-5073-4b5f-e77d-6a62f356b79d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1756001"
      ]
     },
     "execution_count": 222,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trainable_parameters(model) ## hidden = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NN8_xwaDoiWW",
    "outputId": "f9e400d8-7d36-45b6-af2b-ea7df30a1450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1735700, 10000, 10000, 100, 100, 100, 1]"
      ]
     },
     "execution_count": 223,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_trainable_parameters(model) # hidden = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYwfODnbh_OW"
   },
   "source": [
    "(Respuesta Pregunta 3)\n",
    "\n",
    "Podemos ver la descomposicion de los parametros entrenables:\n",
    "\n",
    "``[(Embedding),(Wxh RNN), (Whh RNN), (W_y RNN),(Bias RNN) (W LinearNN) , (Bias Linear NN)] ``\n",
    "\n",
    "*  Podemos ver que el numero de parametros entrenables para ``hidden_dim = 100`` es de ``1756001``\n",
    "* Cuando modificamos este a 50, tenemos ``1743351`` parametros entrenables. Una diferencia de 12650 la cual se descompone en:[link text](https://)\n",
    "- El embedding se mantiene en 1735700\n",
    "- Los pesos 50 x 100 antes eran 100 x 100 representan los Wxh (delta=5000)\n",
    "- Los siguientes pesos 50 x 50 antes eran 100 x 100 representan los Whh (delta=7500)\n",
    "- El siguiente es el Wyh del output que ahora tiene dimension 50\n",
    " y no 100 (delta=50)\n",
    "- El siguente es el bias de la RNN que cambia proporicional al numero de hidden states.(delta=50)\n",
    "-  El siguiente es cada peso de la LinearNN para cada hidden state (delta=50)\n",
    "- Por ultimo, el LinearNN  tiene 1 bias que no cambia. (delta=0)\n",
    "\n",
    "Por lo tanto:\n",
    "`5000+7500+50+50+50=12650`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kce_7LCKgzsd"
   },
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4i7Iz7ugufI"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z97ObY6yhFXU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SQgjA4hhgER"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-75soS8RvQSS"
   },
   "source": [
    "### Train **RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "x0M2iZ0JhkTX",
    "outputId": "67d221b7-81d9-4208-b67e-229e14f2dc6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.16%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.42%\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 52.14%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.59%\n",
      "Epoch: 03 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.56%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 53.05%\n",
      "Epoch: 04 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.73%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 53.29%\n",
      "Epoch: 05 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.690 | Train Acc: 53.09%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 54.41%\n",
      "Epoch: 06 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.689 | Train Acc: 53.63%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 54.82%\n",
      "Epoch: 07 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.687 | Train Acc: 55.16%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 55.70%\n",
      "Epoch: 08 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.683 | Train Acc: 56.27%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 57.34%\n",
      "Epoch: 09 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.667 | Train Acc: 59.80%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 56.29%\n",
      "Epoch: 10 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.677 | Train Acc: 57.96%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 49.66%\n",
      "Epoch: 11 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.33%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.99%\n",
      "Epoch: 12 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.65%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.08%\n",
      "Epoch: 13 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.68%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 49.70%\n",
      "Epoch: 14 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.52%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 53.04%\n",
      "Epoch: 15 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.62%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 49.79%\n",
      "Epoch: 16 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.76%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.61%\n",
      "Epoch: 17 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.37%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.28%\n",
      "Epoch: 18 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.15%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.05%\n",
      "Epoch: 19 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.81%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.04%\n",
      "Epoch: 20 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.93%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 53.04%\n",
      "Epoch: 21 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.79%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.34%\n",
      "Epoch: 22 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.18%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 53.11%\n",
      "Epoch: 23 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.01%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.48%\n",
      "Epoch: 24 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.16%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.96%\n",
      "Epoch: 25 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.98%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.52%\n",
      "Epoch: 26 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.60%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 53.17%\n",
      "Epoch: 27 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.82%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 53.36%\n",
      "Epoch: 28 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.56%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.74%\n",
      "Epoch: 29 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.81%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.51%\n",
      "Epoch: 30 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.58%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 50.60%\n",
      "Epoch: 31 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.03%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 52.47%\n",
      "Epoch: 32 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.90%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 53.31%\n",
      "Epoch: 33 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.49%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 53.68%\n",
      "Epoch: 34 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.18%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 53.56%\n",
      "Epoch: 35 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.47%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 53.64%\n",
      "Epoch: 36 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.690 | Train Acc: 52.77%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 54.09%\n",
      "Epoch: 37 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.690 | Train Acc: 52.69%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 54.48%\n",
      "Epoch: 38 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.690 | Train Acc: 52.32%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 53.84%\n",
      "Epoch: 39 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.690 | Train Acc: 52.76%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 54.68%\n",
      "Epoch: 40 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.688 | Train Acc: 53.14%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 54.68%\n",
      "Epoch: 41 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.688 | Train Acc: 53.52%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 54.20%\n",
      "Epoch: 42 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.686 | Train Acc: 54.21%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 54.93%\n",
      "Epoch: 43 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.685 | Train Acc: 54.53%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 55.94%\n",
      "Epoch: 44 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.683 | Train Acc: 55.35%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 57.29%\n",
      "Epoch: 45 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.679 | Train Acc: 56.46%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 55.82%\n",
      "Epoch: 46 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.677 | Train Acc: 57.80%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 52.79%\n",
      "Epoch: 47 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.671 | Train Acc: 58.56%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 54.18%\n",
      "Epoch: 48 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.668 | Train Acc: 59.54%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 57.10%\n",
      "Epoch: 49 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.665 | Train Acc: 60.50%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 59.56%\n",
      "Epoch: 50 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.663 | Train Acc: 60.52%\n",
      "\t Val. Loss: 0.636 |  Val. Acc: 65.85%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'RNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2TigGA3AK0aA",
    "outputId": "555064f2-1a70-4aa5-8c6c-43fbe15df4d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363853332349809"
      ]
     },
     "execution_count": 229,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8FUv44XBXBOL",
    "outputId": "303f4752-5081-4599-b5d3-637d6704f170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.638 | Test Acc: 65.07%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('RNN-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM9eQhZQxEsx"
   },
   "source": [
    "## Actividad 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06r5pVda7An0"
   },
   "source": [
    "### Def Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYTHQO678vc2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "class BiRNN(nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_layers = output_dim\n",
    "\n",
    "    # Agregamos un embedding como entrada a la RNN. \n",
    "    self.embedding = nn.Embedding(input_dim,embedding_dim)\n",
    "\n",
    "    # A diferencia del Modulo presentado en el enunciado, no se usa batch first en este modulo.\n",
    "    # Es decir las dimensiones son (largo, batch_size)\n",
    "    # bidireectional para agregar la capa y hacerla bidireccional\n",
    "    self.rnn = nn.RNN(embedding_dim, hidden_dim,bidirectional=True)\n",
    "    \n",
    "    self.linear_out = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "  def forward(self, input):\n",
    "    #text = [sent len, batch size]\n",
    "    \n",
    "    embedded = self.embedding(input)\n",
    "    \n",
    "    #embedded = [sent len, batch size, emb dim]\n",
    "    \n",
    "    output, hidden = self.rnn(embedded)\n",
    "    \n",
    "    #output = [sent len, batch size, hid dim]\n",
    "    #hidden = [1, batch size, hid dim]\n",
    "    \n",
    "    # assert torch.equal(output[-1,:,:], hidden.squeeze(2))\n",
    "    # hidden = hidden.view(hidden.size(0)*hidden.size(1), self.hidden_dim*2)\n",
    "    out= hidden[-2,:,:]+hidden[-1,:,:]\n",
    "    return self.linear_out(out)\n",
    "  def init_hidden(self, batch_size):\n",
    "    hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8gklSAe82um"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Agregamos a matriz de pesos el embedding pre-entrenado de GloVe\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = BiRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "num_trainable_parameters(model)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gw_bXagkCWze"
   },
   "source": [
    "### Train BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2MFgMeoXwYoJ",
    "outputId": "0eff4b3b-c58d-442a-83e0-4201c6962f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.695 | Train Acc: 50.17%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.31%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.690 | Train Acc: 53.01%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 55.00%\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.687 | Train Acc: 54.86%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 56.77%\n",
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.684 | Train Acc: 55.57%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 57.67%\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.681 | Train Acc: 56.88%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 58.21%\n",
      "Epoch: 06 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.678 | Train Acc: 58.21%\n",
      "\t Val. Loss: 0.673 |  Val. Acc: 59.09%\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.673 | Train Acc: 59.25%\n",
      "\t Val. Loss: 0.667 |  Val. Acc: 59.79%\n",
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.665 | Train Acc: 60.20%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 59.02%\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.651 | Train Acc: 62.59%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 60.95%\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.644 | Train Acc: 63.04%\n",
      "\t Val. Loss: 0.619 |  Val. Acc: 66.54%\n",
      "Epoch: 11 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.638 | Train Acc: 63.61%\n",
      "\t Val. Loss: 0.607 |  Val. Acc: 68.93%\n",
      "Epoch: 12 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.626 | Train Acc: 64.98%\n",
      "\t Val. Loss: 0.820 |  Val. Acc: 51.87%\n",
      "Epoch: 13 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.620 | Train Acc: 66.07%\n",
      "\t Val. Loss: 0.570 |  Val. Acc: 71.29%\n",
      "Epoch: 14 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.631 | Train Acc: 64.59%\n",
      "\t Val. Loss: 0.634 |  Val. Acc: 64.15%\n",
      "Epoch: 15 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.647 | Train Acc: 62.87%\n",
      "\t Val. Loss: 0.610 |  Val. Acc: 67.85%\n",
      "Epoch: 16 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.636 | Train Acc: 63.96%\n",
      "\t Val. Loss: 0.610 |  Val. Acc: 67.61%\n",
      "Epoch: 17 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.630 | Train Acc: 65.06%\n",
      "\t Val. Loss: 0.615 |  Val. Acc: 66.84%\n",
      "Epoch: 18 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.625 | Train Acc: 65.60%\n",
      "\t Val. Loss: 0.590 |  Val. Acc: 69.27%\n",
      "Epoch: 19 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.620 | Train Acc: 66.06%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 69.59%\n",
      "Epoch: 20 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.652 | Train Acc: 61.33%\n",
      "\t Val. Loss: 0.658 |  Val. Acc: 61.56%\n",
      "Epoch: 21 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.620 | Train Acc: 66.25%\n",
      "\t Val. Loss: 0.592 |  Val. Acc: 68.87%\n",
      "Epoch: 22 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.628 | Train Acc: 64.74%\n",
      "\t Val. Loss: 0.654 |  Val. Acc: 61.94%\n",
      "Epoch: 23 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.646 | Train Acc: 62.71%\n",
      "\t Val. Loss: 0.636 |  Val. Acc: 63.70%\n",
      "Epoch: 24 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.638 | Train Acc: 63.50%\n",
      "\t Val. Loss: 0.634 |  Val. Acc: 63.41%\n",
      "Epoch: 25 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.611 | Train Acc: 66.70%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 66.32%\n",
      "Epoch: 26 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.588 | Train Acc: 69.17%\n",
      "\t Val. Loss: 0.574 |  Val. Acc: 71.33%\n",
      "Epoch: 27 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.622 | Train Acc: 65.99%\n",
      "\t Val. Loss: 0.603 |  Val. Acc: 68.23%\n",
      "Epoch: 28 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.634 | Train Acc: 64.77%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 69.24%\n",
      "Epoch: 29 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.663 | Train Acc: 60.41%\n",
      "\t Val. Loss: 0.654 |  Val. Acc: 61.75%\n",
      "Epoch: 30 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.588 | Train Acc: 68.93%\n",
      "\t Val. Loss: 0.570 |  Val. Acc: 70.44%\n",
      "Epoch: 31 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.570 | Train Acc: 70.65%\n",
      "\t Val. Loss: 0.570 |  Val. Acc: 70.87%\n",
      "Epoch: 32 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.611 | Train Acc: 67.12%\n",
      "\t Val. Loss: 0.676 |  Val. Acc: 57.09%\n",
      "Epoch: 33 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.634 | Train Acc: 64.48%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 61.00%\n",
      "Epoch: 34 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.626 | Train Acc: 65.68%\n",
      "\t Val. Loss: 0.668 |  Val. Acc: 57.74%\n",
      "Epoch: 35 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.650 | Train Acc: 62.57%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 60.77%\n",
      "Epoch: 36 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.635 | Train Acc: 64.53%\n",
      "\t Val. Loss: 0.601 |  Val. Acc: 69.16%\n",
      "Epoch: 37 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.637 | Train Acc: 63.87%\n",
      "\t Val. Loss: 0.602 |  Val. Acc: 69.07%\n",
      "Epoch: 38 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.664 | Train Acc: 60.51%\n",
      "\t Val. Loss: 0.651 |  Val. Acc: 62.63%\n",
      "Epoch: 39 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.644 | Train Acc: 62.75%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 60.38%\n",
      "Epoch: 40 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.634 | Train Acc: 64.72%\n",
      "\t Val. Loss: 0.668 |  Val. Acc: 58.10%\n",
      "Epoch: 41 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.658 | Train Acc: 60.77%\n",
      "\t Val. Loss: 0.666 |  Val. Acc: 60.85%\n",
      "Epoch: 42 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.637 | Train Acc: 64.24%\n",
      "\t Val. Loss: 0.651 |  Val. Acc: 63.41%\n",
      "Epoch: 43 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.656 | Train Acc: 61.68%\n",
      "\t Val. Loss: 0.634 |  Val. Acc: 64.01%\n",
      "Epoch: 44 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.618 | Train Acc: 66.49%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 70.03%\n",
      "Epoch: 45 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.634 | Train Acc: 64.73%\n",
      "\t Val. Loss: 0.680 |  Val. Acc: 57.42%\n",
      "Epoch: 46 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.674 | Train Acc: 58.29%\n",
      "\t Val. Loss: 0.676 |  Val. Acc: 57.11%\n",
      "Epoch: 47 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.660 | Train Acc: 60.67%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 61.71%\n",
      "Epoch: 48 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.634 | Train Acc: 64.32%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 68.52%\n",
      "Epoch: 49 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.654 | Train Acc: 61.47%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 59.07%\n",
      "Epoch: 50 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.646 | Train Acc: 62.81%\n",
      "\t Val. Loss: 0.617 |  Val. Acc: 68.17%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'BiRNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4gp-p0_N2tgI",
    "outputId": "43c96b13-55f8-4db7-8c33-9242d6eb40e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.573 | Test Acc: 70.64%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('BiRNN-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjG5xZoJcQ9B"
   },
   "source": [
    "### Def DenseBidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_L6APaM1bhiJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "class DenseBiRNN(nn.Module):\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_layers = output_dim\n",
    "\n",
    "    # Agregamos un embedding como entrada a la RNN. \n",
    "    self.embedding = nn.Embedding(input_dim,embedding_dim)\n",
    "\n",
    "    # A diferencia del Modulo presentado en el enunciado, no se usa batch first en este modulo.\n",
    "    # Es decir las dimensiones son (largo, batch_size)\n",
    "    # bidireectional para agregar la capa y hacerla bidireccional\n",
    "    self.rnn = nn.RNN(embedding_dim, hidden_dim,bidirectional=True)\n",
    "    self.dense = nn.Linear(hidden_dim,100)\n",
    "    self.linear_out = nn.Linear(100, 1)\n",
    "    \n",
    "  def forward(self, input):\n",
    "    #text = [sent len, batch size]\n",
    "    \n",
    "    embedded = self.embedding(input)\n",
    "    \n",
    "    #embedded = [sent len, batch size, emb dim]\n",
    "    \n",
    "    output, hidden = self.rnn(embedded)\n",
    "    \n",
    "    #output = [sent len, batch size, hid dim]\n",
    "    #hidden = [1, batch size, hid dim]\n",
    "    \n",
    "    # assert torch.equal(output[-1,:,:], hidden.squeeze(2))\n",
    "    # hidden = hidden.view(hidden.size(0)*hidden.size(1), self.hidden_dim*2)\n",
    "    out= hidden[-2,:,:]+hidden[-1,:,:]\n",
    "    return self.linear_out(out)\n",
    "  def init_hidden(self, batch_size):\n",
    "    hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOUeC2Akb1kS"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Agregamos a matriz de pesos el embedding pre-entrenado de GloVe\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = DenseBiRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "num_trainable_parameters(model)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyVZC6lQCPnI"
   },
   "source": [
    "### Train DenseBiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ewacP70Nb6MC",
    "outputId": "083c9f49-1083-4dba-ced5-00917cac3362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.626 | Train Acc: 65.61%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 52.95%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.630 | Train Acc: 64.99%\n",
      "\t Val. Loss: 0.604 |  Val. Acc: 67.64%\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.643 | Train Acc: 63.23%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 58.88%\n",
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.669 | Train Acc: 59.09%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 58.69%\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.666 | Train Acc: 59.49%\n",
      "\t Val. Loss: 0.666 |  Val. Acc: 59.60%\n",
      "Epoch: 06 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.662 | Train Acc: 60.12%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 60.57%\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.657 | Train Acc: 60.87%\n",
      "\t Val. Loss: 0.657 |  Val. Acc: 61.33%\n",
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.649 | Train Acc: 61.85%\n",
      "\t Val. Loss: 0.645 |  Val. Acc: 63.25%\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.642 | Train Acc: 62.80%\n",
      "\t Val. Loss: 0.634 |  Val. Acc: 63.57%\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.639 | Train Acc: 63.41%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 64.33%\n",
      "Epoch: 11 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.639 | Train Acc: 63.31%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 65.10%\n",
      "Epoch: 12 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.631 | Train Acc: 64.66%\n",
      "\t Val. Loss: 0.639 |  Val. Acc: 64.44%\n",
      "Epoch: 13 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.628 | Train Acc: 64.81%\n",
      "\t Val. Loss: 0.622 |  Val. Acc: 65.61%\n",
      "Epoch: 14 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.624 | Train Acc: 65.37%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 68.30%\n",
      "Epoch: 15 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.621 | Train Acc: 65.82%\n",
      "\t Val. Loss: 0.608 |  Val. Acc: 67.71%\n",
      "Epoch: 16 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.623 | Train Acc: 65.87%\n",
      "\t Val. Loss: 0.593 |  Val. Acc: 69.09%\n",
      "Epoch: 17 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.614 | Train Acc: 66.59%\n",
      "\t Val. Loss: 0.639 |  Val. Acc: 62.39%\n",
      "Epoch: 18 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.608 | Train Acc: 66.79%\n",
      "\t Val. Loss: 0.622 |  Val. Acc: 65.54%\n",
      "Epoch: 19 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.605 | Train Acc: 67.01%\n",
      "\t Val. Loss: 0.564 |  Val. Acc: 71.08%\n",
      "Epoch: 20 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.581 | Train Acc: 69.68%\n",
      "\t Val. Loss: 0.523 |  Val. Acc: 74.08%\n",
      "Epoch: 21 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.611 | Train Acc: 66.42%\n",
      "\t Val. Loss: 0.605 |  Val. Acc: 67.98%\n",
      "Epoch: 22 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.642 | Train Acc: 63.27%\n",
      "\t Val. Loss: 0.618 |  Val. Acc: 65.58%\n",
      "Epoch: 23 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.646 | Train Acc: 62.45%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 56.74%\n",
      "Epoch: 24 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.674 | Train Acc: 57.63%\n",
      "\t Val. Loss: 0.674 |  Val. Acc: 56.66%\n",
      "Epoch: 25 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.658 | Train Acc: 60.90%\n",
      "\t Val. Loss: 0.624 |  Val. Acc: 65.16%\n",
      "Epoch: 26 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.650 | Train Acc: 61.54%\n",
      "\t Val. Loss: 0.662 |  Val. Acc: 60.02%\n",
      "Epoch: 27 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.652 | Train Acc: 61.78%\n",
      "\t Val. Loss: 0.633 |  Val. Acc: 62.95%\n",
      "Epoch: 28 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.621 | Train Acc: 66.06%\n",
      "\t Val. Loss: 0.632 |  Val. Acc: 64.34%\n",
      "Epoch: 29 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.590 | Train Acc: 69.46%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 59.16%\n",
      "Epoch: 30 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.618 | Train Acc: 65.60%\n",
      "\t Val. Loss: 0.559 |  Val. Acc: 72.09%\n",
      "Epoch: 31 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.595 | Train Acc: 68.19%\n",
      "\t Val. Loss: 0.566 |  Val. Acc: 71.86%\n",
      "Epoch: 32 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.599 | Train Acc: 67.67%\n",
      "\t Val. Loss: 0.662 |  Val. Acc: 60.43%\n",
      "Epoch: 33 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.603 | Train Acc: 67.37%\n",
      "\t Val. Loss: 0.578 |  Val. Acc: 69.95%\n",
      "Epoch: 34 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.606 | Train Acc: 66.91%\n",
      "\t Val. Loss: 0.620 |  Val. Acc: 64.95%\n",
      "Epoch: 35 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.580 | Train Acc: 69.96%\n",
      "\t Val. Loss: 0.526 |  Val. Acc: 74.96%\n",
      "Epoch: 36 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.608 | Train Acc: 66.86%\n",
      "\t Val. Loss: 0.650 |  Val. Acc: 62.59%\n",
      "Epoch: 37 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.620 | Train Acc: 65.53%\n",
      "\t Val. Loss: 0.607 |  Val. Acc: 67.23%\n",
      "Epoch: 38 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.598 | Train Acc: 67.85%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 68.10%\n",
      "Epoch: 39 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.605 | Train Acc: 66.94%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 61.68%\n",
      "Epoch: 40 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.631 | Train Acc: 64.72%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 64.61%\n",
      "Epoch: 41 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.626 | Train Acc: 64.79%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 53.77%\n",
      "Epoch: 42 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.625 | Train Acc: 65.27%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 58.90%\n",
      "Epoch: 43 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.624 | Train Acc: 65.93%\n",
      "\t Val. Loss: 0.605 |  Val. Acc: 67.71%\n",
      "Epoch: 44 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.623 | Train Acc: 66.01%\n",
      "\t Val. Loss: 0.606 |  Val. Acc: 67.56%\n",
      "Epoch: 45 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.620 | Train Acc: 66.08%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 64.83%\n",
      "Epoch: 46 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.617 | Train Acc: 66.41%\n",
      "\t Val. Loss: 0.667 |  Val. Acc: 60.43%\n",
      "Epoch: 47 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.624 | Train Acc: 65.32%\n",
      "\t Val. Loss: 0.606 |  Val. Acc: 67.51%\n",
      "Epoch: 48 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.618 | Train Acc: 66.10%\n",
      "\t Val. Loss: 0.608 |  Val. Acc: 67.46%\n",
      "Epoch: 49 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.611 | Train Acc: 67.03%\n",
      "\t Val. Loss: 0.629 |  Val. Acc: 65.85%\n",
      "Epoch: 50 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.618 | Train Acc: 66.47%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 66.47%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'DenseBiRNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y_VAkO5ofCgT",
    "outputId": "c262fb65-3808-414e-8a06-354fc3d958e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.543 | Test Acc: 72.73%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('DenseBiRNN-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzgrgBsj7czv"
   },
   "source": [
    "### Respuesta Actividad\n",
    "\n",
    "Se use vectores GloVe para inicializar los vectores en el Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye0yJ2dd7L_i"
   },
   "source": [
    "- BiRNN: Se vario el tamaño de los embeddings y se obtuvo mejores resultados con un embedding de 100\n",
    "- DenseBiRNN: Es el BiRNN + Capa dense extra en la salida.\n",
    "- Velocidad de convergencia(Menor a mayor): RNN -> BiRNN -> DenseBiRNN\n",
    "- Resultados (Menor a mayor accuracy): RNN -> BiRNN -> DenseBiRNN\n",
    "\n",
    "Se puede ver que agregarle la bidireccionalidad aumenta la complejidad del modelo pero representa de mejor manera el sentimiento de las oraciones. \n",
    "Lo mismo se puede decir de la capa densa extra, esta capa extra de pesos entrega mayor contexto al resolver el sentimiento de la oracion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRJ3J2zfcJ6F"
   },
   "source": [
    "## Actividad 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9CrbjD4CEZv"
   },
   "source": [
    "### Def LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQuk9V57cs8_"
   },
   "outputs": [],
   "source": [
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        \n",
    "        output_lengths, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1XfXrPvgc-qe",
    "outputId": "7951837a-a15f-46ed-9ba9-613f8fa042ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4046357"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTM(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTSpZc-4CLr_"
   },
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PxzMsSq_dW_X",
    "outputId": "d9fc80bb-4dc7-44f4-b1e2-ca1cb856f24a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.686 | Train Acc: 55.11%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.01%\n",
      "Epoch: 02 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.692 | Train Acc: 53.08%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 56.29%\n",
      "Epoch: 03 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.691 | Train Acc: 52.82%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 54.63%\n",
      "Epoch: 04 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.687 | Train Acc: 54.86%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 56.77%\n",
      "Epoch: 05 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.672 | Train Acc: 58.26%\n",
      "\t Val. Loss: 0.630 |  Val. Acc: 64.33%\n",
      "Epoch: 06 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.627 | Train Acc: 65.45%\n",
      "\t Val. Loss: 0.506 |  Val. Acc: 75.37%\n",
      "Epoch: 07 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.529 | Train Acc: 74.12%\n",
      "\t Val. Loss: 0.395 |  Val. Acc: 82.45%\n",
      "Epoch: 08 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.417 | Train Acc: 81.39%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 86.45%\n",
      "Epoch: 09 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.366 | Train Acc: 84.17%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 86.75%\n",
      "Epoch: 10 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.320 | Train Acc: 86.57%\n",
      "\t Val. Loss: 0.302 |  Val. Acc: 87.78%\n",
      "Epoch: 11 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.279 | Train Acc: 88.80%\n",
      "\t Val. Loss: 0.261 |  Val. Acc: 89.23%\n",
      "Epoch: 12 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.253 | Train Acc: 90.08%\n",
      "\t Val. Loss: 0.279 |  Val. Acc: 88.88%\n",
      "Epoch: 13 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.237 | Train Acc: 90.79%\n",
      "\t Val. Loss: 0.239 |  Val. Acc: 90.30%\n",
      "Epoch: 14 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.219 | Train Acc: 91.49%\n",
      "\t Val. Loss: 0.256 |  Val. Acc: 90.65%\n",
      "Epoch: 15 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.219 | Train Acc: 91.31%\n",
      "\t Val. Loss: 0.288 |  Val. Acc: 89.71%\n",
      "Epoch: 16 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.198 | Train Acc: 92.42%\n",
      "\t Val. Loss: 0.248 |  Val. Acc: 91.01%\n",
      "Epoch: 17 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.185 | Train Acc: 93.10%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 90.88%\n",
      "Epoch: 18 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.167 | Train Acc: 93.90%\n",
      "\t Val. Loss: 0.253 |  Val. Acc: 90.86%\n",
      "Epoch: 19 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.173 | Train Acc: 93.18%\n",
      "\t Val. Loss: 0.230 |  Val. Acc: 90.84%\n",
      "Epoch: 20 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.152 | Train Acc: 94.32%\n",
      "\t Val. Loss: 0.274 |  Val. Acc: 90.68%\n",
      "Epoch: 21 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.147 | Train Acc: 94.26%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 87.80%\n",
      "Epoch: 22 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.138 | Train Acc: 94.86%\n",
      "\t Val. Loss: 0.285 |  Val. Acc: 90.14%\n",
      "Epoch: 23 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.118 | Train Acc: 95.62%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 89.87%\n",
      "Epoch: 24 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.118 | Train Acc: 95.63%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 91.01%\n",
      "Epoch: 25 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.115 | Train Acc: 95.95%\n",
      "\t Val. Loss: 0.280 |  Val. Acc: 91.08%\n",
      "Epoch: 26 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.104 | Train Acc: 96.19%\n",
      "\t Val. Loss: 0.293 |  Val. Acc: 90.78%\n",
      "Epoch: 27 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.106 | Train Acc: 96.08%\n",
      "\t Val. Loss: 0.270 |  Val. Acc: 90.80%\n",
      "Epoch: 28 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.096 | Train Acc: 96.64%\n",
      "\t Val. Loss: 0.274 |  Val. Acc: 90.62%\n",
      "Epoch: 29 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.089 | Train Acc: 96.81%\n",
      "\t Val. Loss: 0.321 |  Val. Acc: 90.88%\n",
      "Epoch: 30 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.080 | Train Acc: 97.18%\n",
      "\t Val. Loss: 0.314 |  Val. Acc: 91.11%\n",
      "Epoch: 31 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.080 | Train Acc: 97.26%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 90.88%\n",
      "Epoch: 32 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.081 | Train Acc: 97.24%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 91.14%\n",
      "Epoch: 33 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.069 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.341 |  Val. Acc: 90.94%\n",
      "Epoch: 34 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.53%\n",
      "\t Val. Loss: 0.341 |  Val. Acc: 90.84%\n",
      "Epoch: 35 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.64%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 90.89%\n",
      "Epoch: 36 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.69%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 90.97%\n",
      "Epoch: 37 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.91%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 90.96%\n",
      "Epoch: 38 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.07%\n",
      "\t Val. Loss: 0.383 |  Val. Acc: 91.15%\n",
      "Epoch: 39 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.11%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 90.92%\n",
      "Epoch: 40 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.361 |  Val. Acc: 90.41%\n",
      "Epoch: 41 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.36%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 90.64%\n",
      "Epoch: 42 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.18%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 90.45%\n",
      "Epoch: 43 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.42%\n",
      "\t Val. Loss: 0.406 |  Val. Acc: 90.94%\n",
      "Epoch: 44 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 90.61%\n",
      "Epoch: 45 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.63%\n",
      "\t Val. Loss: 0.421 |  Val. Acc: 90.93%\n",
      "Epoch: 46 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.51%\n",
      "\t Val. Loss: 0.373 |  Val. Acc: 91.14%\n",
      "Epoch: 47 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.70%\n",
      "\t Val. Loss: 0.381 |  Val. Acc: 90.89%\n",
      "Epoch: 48 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.75%\n",
      "\t Val. Loss: 0.506 |  Val. Acc: 89.72%\n",
      "Epoch: 49 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.84%\n",
      "\t Val. Loss: 0.444 |  Val. Acc: 91.22%\n",
      "Epoch: 50 | Epoch Time: 1m 33s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.82%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 90.28%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'LSTM-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SXbcAlHjf1LW",
    "outputId": "8483b8db-da2e-43e3-8cfa-515b599501f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.259 | Test Acc: 89.72%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('LSTM-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gz0-yW41BoCu"
   },
   "source": [
    "### Respuesta Actividad 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_jpHns5BrxD"
   },
   "source": [
    "- Cada epoch es mas largo con LSTM. (Mas parametros que entrener)\n",
    "- Los resultados son significamente mejores. Es un LSTM bidireccional, pero la diferencia con el RNN bidireccional es de `70%` a `89.72%`. Un gran salto de accuraccy.\n",
    "- Mejoro considerablemente la prediccion con LSTM. El problema de gradiente ya no esta afectando la red como en RNN clasico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aO2SEjQlYj72"
   },
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwNXYu4ou3r-"
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "9v7d1UDLw9x0",
    "outputId": "033eddb2-2092-4c17-b09e-d18a88460b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-10 02:30:31--  https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_train_simple.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3267938 (3.1M) [text/plain]\n",
      "Saving to: ‘tasks_train_simple.txt.1’\n",
      "\n",
      "tasks_train_simple. 100%[===================>]   3.12M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2020-06-10 02:30:33 (34.4 MB/s) - ‘tasks_train_simple.txt.1’ saved [3267938/3267938]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_train_simple.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "e7uAVOA5xOYG",
    "outputId": "0dd1f6f4-bc0e-4b4a-e6c5-13e995f57ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-10 02:30:35--  https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_test_simple.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 812450 (793K) [text/plain]\n",
      "Saving to: ‘tasks_test_simple.txt.1’\n",
      "\n",
      "\r",
      "tasks_test_simple.t   0%[                    ]       0  --.-KB/s               \r",
      "tasks_test_simple.t 100%[===================>] 793.41K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2020-06-10 02:30:35 (16.1 MB/s) - ‘tasks_test_simple.txt.1’ saved [812450/812450]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/brendenlake/SCAN/master/simple_split/tasks_test_simple.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "JU71w-Y8BcvI",
    "outputId": "115093d2-7b69-443e-851c-cd7d6ce1d09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.5.0+cu101)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.9)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4hbnl5EZb-r"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXwon3YhMpvM"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        self.max_s_len =-1\n",
    "    def addSentence(self, sentence):\n",
    "        self.updateMax(sentence)\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def updateMax(self,sentence):\n",
    "        stce_len=len(sentence.split(\" \"))\n",
    "        if(stce_len>self.max_s_len):\n",
    "          self.max_s_len =stce_len\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_human, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_machine, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "uXiX-sXd0JWi",
    "outputId": "e0d98ed4-138b-44c2-eeb3-21be7f9df70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human 15\n",
      "machine 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [13],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 2],\n",
       "        [13],\n",
       "        [ 4],\n",
       "        [ 8],\n",
       "        [ 1]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = None\n",
    "with open('tasks_train_simple.txt') as train_file:\n",
    "  train = train_file.readlines()\n",
    "test = None\n",
    "with open('tasks_test_simple.txt') as test_file:\n",
    "  test = test_file.readlines()\n",
    "\n",
    "\n",
    "## Limpieza y separar labeles\n",
    "def get_values_labels(data):\n",
    "  pairs=[]\n",
    "  for row in data:\n",
    "    IN,OUT=row.split(\"OUT: \")\n",
    "    IN = IN.strip(\"IN: \")\n",
    "    OUT = OUT.strip()\n",
    "    pairs.append([IN,OUT])\n",
    "  return pairs\n",
    "\n",
    "\n",
    "## Guardamos nuestro dataset de train y test\n",
    "pairs = get_values_labels(train)\n",
    "test_pairs = get_values_labels(test)\n",
    "\n",
    "def prepare_data(pairs):\n",
    "  input_human = Lang('human')\n",
    "  output_machine = Lang('machine')\n",
    "  for pair in pairs:\n",
    "    input_human.addSentence(pair[0])\n",
    "    output_machine.addSentence(pair[1])\n",
    "  print(input_human.name,input_human.n_words)\n",
    "  print(output_machine.name,output_machine.n_words)\n",
    "  return input_human,output_machine,pairs\n",
    "    \n",
    "input_human,output_machine,pairs = prepare_data(pairs)\n",
    "MAX_LENGTH = max(input_human.max_s_len,output_machine.max_s_len)\n",
    "tensorFromSentence(input_human,random.choice(pairs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQRlbUiTchQd"
   },
   "source": [
    "##  Def Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQYHssPhXtAt"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output,hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden,cell_states):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOmYM0A6Tfky"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "          #  decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "          #       decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden  = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            #     decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def test(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        # decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "        #     decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgDsHXBoTzsQ"
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ojgmZWWczrE"
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRIYIAKmT3ig"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=5, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCTIBYabcfwV"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_human, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        # decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            #   decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_machine.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "\n",
    "        # return decoded_words, decoder_attentions[:di + 1]\n",
    "        l = len(decoded_words)\n",
    "        \n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQh-e_lu5FC1"
   },
   "outputs": [],
   "source": [
    "def testIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    testing_pairs = [tensorsFromPair(random.choice(test_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        testing_pair = testing_pairs[iter - 1]\n",
    "        input_tensor = testing_pair[0]\n",
    "        target_tensor = testing_pair[1]\n",
    "        print(target_tensor.shape)\n",
    "        print(input_tensor.shape)\n",
    "        loss = test(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    return plot_losses,print_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oh4YZwBsAuhP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def acc_score(pred_tags,trgs):\n",
    "  WRONG_WORD =\"<FOOBAR>\"\n",
    "  final_score=0 \n",
    "  for t in range(len(pred_tags)):\n",
    "    pred = pred_tags[t]\n",
    "    correct = trgs[t]\n",
    "    len_pred = len(pred)\n",
    "    len_correct = len(correct)\n",
    "    if len_correct > len_pred:\n",
    "      pred+=[WRONG_WORD]*(len_correct - len_pred)\n",
    "    elif len_correct < len_pred:\n",
    "      pred = pred[:len_correct]\n",
    "    assert len(correct)==len(pred)\n",
    "    score = sum([pred[i]==correct[i] for i in range(len(correct))])/len(correct)\n",
    "    final_score += score\n",
    "  return final_score/len(pred_tags)\n",
    "def calculate_score(data, encoder, decoder, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = datum[0]\n",
    "        trg = datum[1]\n",
    "        \n",
    "        pred_trg = evaluate(encoder,decoder,src)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append(trg)\n",
    "    return acc_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyz-hjB1sTOe"
   },
   "source": [
    "## RUN without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jlEDhUubtgur",
    "outputId": "12bcb37f-4883-46a8-d393-e4d472f1282a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797448"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_human.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = DecoderRNN(hidden_size, output_machine.n_words).to(device)\n",
    "num_trainable_parameters(encoder1)+num_trainable_parameters(attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zT5q-5Z3C69E"
   },
   "source": [
    "### Actividad 4\n",
    "- Tiene `797448` parametros entrenables\n",
    "- Vocabulario de origen (Humano) tiene 15\n",
    "- Vocabulario de destino (Machine) tiene 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DaOBYPzCc_CP",
    "outputId": "e9416a33-e0d1-417c-d408-41f637d9e7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 3s (- 8m 41s) (100 0%) 2.6187\n",
      "0m 5s (- 7m 12s) (200 1%) 2.2566\n",
      "0m 7s (- 6m 34s) (300 1%) 1.7689\n",
      "0m 9s (- 6m 17s) (400 2%) 1.6305\n",
      "0m 11s (- 6m 7s) (500 2%) 1.7668\n",
      "0m 13s (- 5m 57s) (600 3%) 1.4824\n",
      "0m 15s (- 5m 52s) (700 4%) 1.2745\n",
      "0m 17s (- 5m 48s) (800 4%) 1.3726\n",
      "0m 19s (- 5m 37s) (900 5%) 1.1633\n",
      "0m 21s (- 5m 35s) (1000 5%) 1.2777\n",
      "0m 23s (- 5m 30s) (1100 6%) 1.1427\n",
      "0m 25s (- 5m 30s) (1200 7%) 1.0631\n",
      "0m 27s (- 5m 27s) (1300 7%) 1.1620\n",
      "0m 29s (- 5m 26s) (1400 8%) 1.0853\n",
      "0m 31s (- 5m 23s) (1500 8%) 1.0203\n",
      "0m 33s (- 5m 20s) (1600 9%) 0.9244\n",
      "0m 36s (- 5m 18s) (1700 10%) 1.0757\n",
      "0m 38s (- 5m 15s) (1800 10%) 1.0717\n",
      "0m 40s (- 5m 12s) (1900 11%) 1.1056\n",
      "0m 42s (- 5m 9s) (2000 11%) 0.9450\n",
      "0m 44s (- 5m 8s) (2100 12%) 1.1041\n",
      "0m 46s (- 5m 5s) (2200 13%) 1.0473\n",
      "0m 48s (- 5m 2s) (2300 13%) 1.0014\n",
      "0m 50s (- 5m 0s) (2400 14%) 0.9732\n",
      "0m 52s (- 4m 57s) (2500 14%) 1.1199\n",
      "0m 54s (- 4m 55s) (2600 15%) 0.9586\n",
      "0m 56s (- 4m 52s) (2700 16%) 0.9516\n",
      "0m 58s (- 4m 50s) (2800 16%) 1.0309\n",
      "1m 0s (- 4m 47s) (2900 17%) 0.9416\n",
      "1m 2s (- 4m 45s) (3000 17%) 0.9685\n",
      "1m 4s (- 4m 43s) (3100 18%) 0.9606\n",
      "1m 6s (- 4m 41s) (3200 19%) 0.7947\n",
      "1m 8s (- 4m 39s) (3300 19%) 0.8750\n",
      "1m 10s (- 4m 36s) (3400 20%) 1.0410\n",
      "1m 12s (- 4m 34s) (3500 20%) 0.9230\n",
      "1m 14s (- 4m 32s) (3600 21%) 1.0029\n",
      "1m 16s (- 4m 29s) (3700 22%) 0.9540\n",
      "1m 18s (- 4m 27s) (3800 22%) 0.8269\n",
      "1m 20s (- 4m 25s) (3900 23%) 0.9641\n",
      "1m 22s (- 4m 23s) (4000 23%) 0.9054\n",
      "1m 24s (- 4m 21s) (4100 24%) 0.9306\n",
      "1m 26s (- 4m 19s) (4200 25%) 0.9031\n",
      "1m 28s (- 4m 16s) (4300 25%) 0.8593\n",
      "1m 30s (- 4m 14s) (4400 26%) 0.8217\n",
      "1m 32s (- 4m 12s) (4500 26%) 1.0449\n",
      "1m 34s (- 4m 10s) (4600 27%) 0.9326\n",
      "1m 36s (- 4m 7s) (4700 28%) 0.8981\n",
      "1m 38s (- 4m 5s) (4800 28%) 0.9137\n",
      "1m 40s (- 4m 3s) (4900 29%) 0.8185\n",
      "1m 42s (- 4m 1s) (5000 29%) 1.0321\n",
      "1m 44s (- 3m 58s) (5100 30%) 0.8504\n",
      "1m 46s (- 3m 56s) (5200 31%) 0.7939\n",
      "1m 48s (- 3m 54s) (5300 31%) 0.8575\n",
      "1m 50s (- 3m 52s) (5400 32%) 0.8007\n",
      "1m 52s (- 3m 50s) (5500 32%) 0.7284\n",
      "1m 54s (- 3m 47s) (5600 33%) 0.6778\n",
      "1m 56s (- 3m 45s) (5700 34%) 0.7380\n",
      "1m 58s (- 3m 44s) (5800 34%) 0.8567\n",
      "2m 0s (- 3m 41s) (5900 35%) 0.7828\n",
      "2m 3s (- 3m 39s) (6000 35%) 0.8531\n",
      "2m 5s (- 3m 37s) (6100 36%) 0.6742\n",
      "2m 7s (- 3m 35s) (6200 37%) 0.6999\n",
      "2m 9s (- 3m 33s) (6300 37%) 0.8068\n",
      "2m 11s (- 3m 31s) (6400 38%) 0.8538\n",
      "2m 13s (- 3m 29s) (6500 38%) 0.7920\n",
      "2m 15s (- 3m 27s) (6600 39%) 0.8828\n",
      "2m 17s (- 3m 25s) (6700 40%) 0.8935\n",
      "2m 19s (- 3m 23s) (6800 40%) 0.6984\n",
      "2m 21s (- 3m 20s) (6900 41%) 0.7157\n",
      "2m 23s (- 3m 18s) (7000 41%) 0.8073\n",
      "2m 25s (- 3m 16s) (7100 42%) 0.7094\n",
      "2m 27s (- 3m 14s) (7200 43%) 0.6900\n",
      "2m 29s (- 3m 12s) (7300 43%) 0.6808\n",
      "2m 31s (- 3m 10s) (7400 44%) 0.7631\n",
      "2m 33s (- 3m 8s) (7500 44%) 0.7678\n",
      "2m 35s (- 3m 6s) (7600 45%) 0.8268\n",
      "2m 37s (- 3m 4s) (7700 46%) 0.7516\n",
      "2m 39s (- 3m 2s) (7800 46%) 0.7359\n",
      "2m 41s (- 3m 0s) (7900 47%) 0.7583\n",
      "2m 43s (- 2m 58s) (8000 47%) 0.7107\n",
      "2m 45s (- 2m 56s) (8100 48%) 0.7315\n",
      "2m 47s (- 2m 54s) (8200 49%) 0.7115\n",
      "2m 49s (- 2m 52s) (8300 49%) 0.5906\n",
      "2m 51s (- 2m 50s) (8400 50%) 0.6712\n",
      "2m 54s (- 2m 48s) (8500 50%) 0.5488\n",
      "2m 56s (- 2m 46s) (8600 51%) 0.5651\n",
      "2m 58s (- 2m 44s) (8700 52%) 0.5962\n",
      "3m 0s (- 2m 42s) (8800 52%) 0.6242\n",
      "3m 2s (- 2m 40s) (8900 53%) 0.6187\n",
      "3m 4s (- 2m 38s) (9000 53%) 0.5729\n",
      "3m 6s (- 2m 36s) (9100 54%) 0.5912\n",
      "3m 8s (- 2m 34s) (9200 54%) 0.5774\n",
      "3m 10s (- 2m 32s) (9300 55%) 0.5427\n",
      "3m 12s (- 2m 30s) (9400 56%) 0.4766\n",
      "3m 14s (- 2m 28s) (9500 56%) 0.4661\n",
      "3m 16s (- 2m 26s) (9600 57%) 0.4543\n",
      "3m 18s (- 2m 23s) (9700 57%) 0.5119\n",
      "3m 20s (- 2m 21s) (9800 58%) 0.4230\n",
      "3m 22s (- 2m 19s) (9900 59%) 0.4794\n",
      "3m 24s (- 2m 17s) (10000 59%) 0.4186\n",
      "3m 26s (- 2m 15s) (10100 60%) 0.4146\n",
      "3m 28s (- 2m 13s) (10200 60%) 0.4440\n",
      "3m 30s (- 2m 11s) (10300 61%) 0.4563\n",
      "3m 32s (- 2m 9s) (10400 62%) 0.4174\n",
      "3m 34s (- 2m 7s) (10500 62%) 0.4982\n",
      "3m 36s (- 2m 5s) (10600 63%) 0.4702\n",
      "3m 38s (- 2m 3s) (10700 63%) 0.3675\n",
      "3m 40s (- 2m 1s) (10800 64%) 0.3820\n",
      "3m 42s (- 1m 59s) (10900 65%) 0.5150\n",
      "3m 44s (- 1m 57s) (11000 65%) 0.5371\n",
      "3m 46s (- 1m 54s) (11100 66%) 0.4409\n",
      "3m 48s (- 1m 52s) (11200 66%) 0.4158\n",
      "3m 50s (- 1m 50s) (11300 67%) 0.4333\n",
      "3m 52s (- 1m 48s) (11400 68%) 0.5235\n",
      "3m 54s (- 1m 46s) (11500 68%) 0.4092\n",
      "3m 56s (- 1m 44s) (11600 69%) 0.4112\n",
      "3m 58s (- 1m 42s) (11700 69%) 0.4162\n",
      "4m 0s (- 1m 40s) (11800 70%) 0.4469\n",
      "4m 2s (- 1m 38s) (11900 71%) 0.3581\n",
      "4m 4s (- 1m 36s) (12000 71%) 0.3991\n",
      "4m 6s (- 1m 34s) (12100 72%) 0.4378\n",
      "4m 8s (- 1m 32s) (12200 72%) 0.3721\n",
      "4m 10s (- 1m 30s) (12300 73%) 0.3235\n",
      "4m 12s (- 1m 28s) (12400 74%) 0.2406\n",
      "4m 14s (- 1m 26s) (12500 74%) 0.2507\n",
      "4m 16s (- 1m 23s) (12600 75%) 0.3313\n",
      "4m 18s (- 1m 21s) (12700 75%) 0.3105\n",
      "4m 20s (- 1m 19s) (12800 76%) 0.2831\n",
      "4m 22s (- 1m 17s) (12900 77%) 0.2807\n",
      "4m 24s (- 1m 15s) (13000 77%) 0.3588\n",
      "4m 26s (- 1m 13s) (13100 78%) 0.3629\n",
      "4m 28s (- 1m 11s) (13200 78%) 0.3697\n",
      "4m 30s (- 1m 9s) (13300 79%) 0.4155\n",
      "4m 32s (- 1m 7s) (13400 80%) 0.4070\n",
      "4m 34s (- 1m 5s) (13500 80%) 0.3622\n",
      "4m 36s (- 1m 3s) (13600 81%) 0.4305\n",
      "4m 38s (- 1m 1s) (13700 81%) 0.3797\n",
      "4m 40s (- 0m 59s) (13800 82%) 0.4089\n",
      "4m 42s (- 0m 57s) (13900 83%) 0.3141\n",
      "4m 44s (- 0m 55s) (14000 83%) 0.3183\n",
      "4m 46s (- 0m 53s) (14100 84%) 0.2566\n",
      "4m 48s (- 0m 51s) (14200 84%) 0.2609\n",
      "4m 50s (- 0m 49s) (14300 85%) 0.2302\n",
      "4m 52s (- 0m 47s) (14400 86%) 0.2759\n",
      "4m 54s (- 0m 45s) (14500 86%) 0.3410\n",
      "4m 56s (- 0m 43s) (14600 87%) 0.2948\n",
      "4m 58s (- 0m 41s) (14700 87%) 0.2809\n",
      "5m 0s (- 0m 39s) (14800 88%) 0.2370\n",
      "5m 2s (- 0m 37s) (14900 89%) 0.2686\n",
      "5m 4s (- 0m 35s) (15000 89%) 0.2574\n",
      "5m 6s (- 0m 33s) (15100 90%) 0.2073\n",
      "5m 9s (- 0m 31s) (15200 90%) 0.2631\n",
      "5m 11s (- 0m 29s) (15300 91%) 0.2074\n",
      "5m 13s (- 0m 27s) (15400 92%) 0.2188\n",
      "5m 15s (- 0m 24s) (15500 92%) 0.1804\n",
      "5m 17s (- 0m 22s) (15600 93%) 0.1995\n",
      "5m 19s (- 0m 20s) (15700 93%) 0.2475\n",
      "5m 21s (- 0m 18s) (15800 94%) 0.2890\n",
      "5m 23s (- 0m 16s) (15900 95%) 0.2594\n",
      "5m 25s (- 0m 14s) (16000 95%) 0.2755\n",
      "5m 27s (- 0m 12s) (16100 96%) 0.2636\n",
      "5m 29s (- 0m 10s) (16200 96%) 0.2283\n",
      "5m 31s (- 0m 8s) (16300 97%) 0.2588\n",
      "5m 33s (- 0m 6s) (16400 98%) 0.2507\n",
      "5m 35s (- 0m 4s) (16500 98%) 0.2249\n",
      "5m 37s (- 0m 2s) (16600 99%) 0.2346\n",
      "5m 39s (- 0m 0s) (16700 99%) 0.1904\n",
      "[2.1251753966013593, 2.027854638452883, 3.5990879822480224, 3.059693857206331, 2.947558057480964, 4.091440056700672, 3.514751228084409, 2.7133286644280035, 3.192299522600676, 2.44098979826055, 2.9186287286542507, 2.2487886873881022, 2.4705661675080077, 2.090898203575748, 2.254079335665513, 1.8820935351885608, 2.146207951434582, 2.078600005119566, 1.496847604680027, 3.0745057779947915, 2.8452856966807714, 2.5844064719412985, 2.7622156698830658, 2.581594910805556, 2.5044883472750885, 1.9934921135947452, 2.549298523974472, 2.856054189553232, 1.9471914443775868, 1.9848427549997965, 1.7455112401783075, 2.1660040820542203, 2.379735743522644, 1.707810012795664, 2.2648841465646603, 2.3957414165678474, 1.7219295542697186, 1.8327857988914218, 2.006806371284253, 2.3014052531695124, 1.9599262068006726, 1.578425352611239, 1.5404843575613842, 1.8063242610703167, 1.9525529479458978, 1.4889641305109933, 1.235554483628701, 1.7768328469140187, 1.7393059474882822, 1.5203928968641491, 2.1144351597746613, 1.6084750128020509, 2.079467740891472, 1.5751382424386495, 1.9525639120271936, 1.7417472722249001, 2.1256899743551734, 1.9647572113741922, 1.9407996701726258, 1.6768864024943149, 1.916727415999474, 1.631958499484592, 1.7818859994769531, 1.9069248830841334, 1.8433211803675296, 1.9282149521904048, 1.553498426104674, 1.4918289154570425, 1.8189344804642826, 1.6708603363291783, 1.6420778003296295, 1.5349307162550745, 1.5025810781151356, 1.562763229480658, 1.7390584819840345, 1.5806901438756298, 1.3942500337793136, 1.384502962542704, 1.2607983985779776, 1.464964332113733, 1.227173854055859, 2.059944771206568, 1.2698712387992273, 1.6567241094584453, 1.7997123201525838, 1.9579010366005871, 1.7730349026739343, 1.5674815559387207, 1.39238235629551, 1.2736448518667536, 1.27005260820742, 1.5327375856908416, 1.9572945611698405, 1.4346885839945962, 1.985425484897645, 1.7274698724994413, 3.0387809040995726, 2.5173951193341297, 1.447582625112704, 2.447567685724615, 1.6356533163279174, 1.5401003157786834, 1.2359025102673156, 1.1314101156424652, 1.669171435273191, 1.6893897906216708, 1.2121462603152058, 2.0395121847959983, 1.6702206449035715, 1.7598946681669734, 1.7628217584658892, 1.6527270233191225, 1.3593632243092204, 1.4128882334782529, 1.2774829150528562, 1.3431005428906968, 1.3215150248264682, 1.469371985178145, 1.301234737206331, 1.1646981974137134, 1.2150821451835192, 1.251123976759858, 1.3505206533420964, 1.0303070044721294, 1.5467802921597495, 1.6737582419708545, 0.946289395682723, 1.4385652831106477, 1.0359780271217307, 1.3999874651690065, 1.44986900705119, 1.0797510534619525, 1.2037705350805212, 1.4065666839790594, 0.7784075221138046, 1.2730747544570957, 1.477841083348146, 1.3827062309029614, 1.1751564861214614, 1.3741202148593918, 1.8516965125245126, 1.0405401994554375, 1.2854762397135109, 1.7531018780629875, 1.2102044253774724, 1.3284922918709376, 1.2435696377713454, 1.2140554103806855, 1.6055988944586432, 1.0146971689039659, 1.371391884653412, 1.3490642975116596, 1.2190251152603715, 1.6194179114403084, 1.3760951464364672, 1.4568997642441923, 1.7248545361629894, 1.3636726837431656, 0.9236172592278683, 1.50074640424628, 1.123097399331473, 1.163681669634314, 1.285932528259408, 1.3079006419203798, 1.01903559366862, 1.5068225239813147, 0.9997935816287498, 1.0402463964806559, 1.3679855473836262, 1.0912390796220297, 1.3440152625635522, 0.8306896007162893, 1.0405373634830597, 1.177933540646992, 1.0057934884671813, 1.130434942942091, 1.116748049853843, 1.235207146345967, 1.2583230471873024, 1.2204294331931314, 0.9246789156339685, 1.4419188586148348, 1.0357182287895816, 1.3287770756399049, 1.0507599280972788, 1.2987086895983335, 1.3824914152801235, 1.473289464522099, 1.1729368073865252, 1.2715678230587975, 1.0683165460244204, 1.2505033470335463, 1.3846185282298498, 1.0268910418981794, 1.2547269935731764, 1.07378856196548, 1.18053015110376, 1.5587681875822745, 2.0206319297576605, 1.3552379979080569, 1.039293288706538, 1.0115304191907246, 0.944309271298922, 1.4331790288049515, 1.1312495474691517, 1.1595714701925004, 1.4171073312090154, 0.9665982299433761, 1.0663202057839696, 1.1213785113389658, 1.4728091996985597, 1.0862739523337306, 1.6337928672063917, 0.9470005655172253, 1.1021473229177492, 1.081369357881092, 0.8197647034433659, 1.1542925223437224, 0.8731663229643416, 1.3931097333748024, 1.0721013722758, 0.6638658273170841, 0.7608136823908023, 0.8756716049399842, 0.9497727950414022, 0.9113243287906311, 0.6536943951720904, 1.1028917595587253, 0.7232882201971442, 1.9788356819848896, 1.0709099416567511, 0.9064628323179562, 1.277446445423736, 1.169979394502915, 1.2359370218300005, 1.620533264024246, 0.956543171591758, 1.148019897479729, 1.3857606809567184, 0.7975829105150132, 1.011940628544131, 1.0973080847599939, 1.3343619569633844, 1.2695276165008544, 1.490174523685575, 1.1723635114930633, 1.191151231877944, 1.3166456337270325, 1.2477420126271044, 0.9384031500823227, 1.0630247668025614, 0.8437846893551704, 1.0984120939052424, 1.3930449228748636, 1.2089300200388777, 1.2498246079261857, 1.2075541076593685, 1.0894324607464856, 0.8763487279771637, 1.1395457665903586, 1.2270411385430229, 0.7477201032114553, 1.0498538749013038, 0.9500333353197494, 1.8235682926553978, 1.020117216867114, 1.0109558179439369, 1.066394834146319, 0.7030371127722931, 0.9769103112674895, 1.1283391142434485, 1.4390177526687071, 1.0020719799926836, 1.2585976156538539, 0.9921052677996572, 0.7773431393087429, 1.0781354230244955, 1.2017782494888976, 0.936148586974582, 1.3163473112854076, 0.8352852073808247, 1.0571052941235897, 1.2684900648333044, 1.069444659189258, 1.0610592774855785, 0.929880727973409, 0.9099851589574441, 1.2260412602872401, 1.1737495349940947, 0.9593568386501736, 0.7784636058297045, 0.8206081571925583, 0.7068588048222918, 0.8935030289507105, 1.4027244608025802, 1.2639941596475923, 1.1542730494236029, 1.2086699317750478, 0.8348467473968555, 0.8526446806570016, 1.132937777939663, 0.981364525310577, 0.8488037032478232, 0.8082165195867207, 0.9421450939701405, 1.005036505226396, 0.810763152764768, 1.0867200373415742, 0.6229021349407378, 1.2368641240150313, 0.727663170991056, 1.1242522412895137, 0.7670168388255529, 0.9928742296796036, 0.6967357532881991, 1.0062730704821072, 0.8239609741029283, 0.7031881427764892, 1.0950281324405329, 1.075601984424032, 0.9282336793976389, 1.5134458773063888, 0.795662925219295, 1.0258560990530345, 1.1143186020128657, 1.1801101315210736, 0.9433941795707556, 1.26819444647363, 1.3407661973036729, 1.1198829073754568, 0.8457191195539249, 1.180854066292644, 1.1531814137283636, 1.1648807043953961, 1.0286502016915215, 1.1595870172312122, 1.0361969674744222, 0.8506947462899344, 0.6992407435462588, 1.1656515824659235, 1.3379523553526105, 0.7058565009207952, 0.8642531095770778, 1.3745509320800042, 1.283286084991503, 0.9645737229342043, 0.7989970928546339, 1.1015454989618583, 1.2269710430954444, 0.7524391084226787, 0.9345125085109529, 1.2459527940336492, 1.0734503658699903, 1.1090366570154826, 1.0516896006452452, 0.7769716620204424, 1.444351754771756, 1.1169707696920539, 1.14271116050646, 1.1288732409143314, 0.9602076265758728, 1.4492592748006183, 1.2309328014937488, 1.2948851961952248, 1.0544260406677244, 1.3261698976244243, 1.0542198687605029, 0.9383089107011026, 1.3500182989112333, 0.847771912188757, 0.8676343637042574, 1.2084871930103933, 1.0037755485681388, 1.0672850493101105, 0.7043727650497899, 1.5683147198209912, 0.962906802330006, 1.2539297547562576, 1.0431086326256778, 0.9250960575937498, 0.8895138947661225, 0.6565156140460617, 1.0652929042612924, 0.8101438144988565, 0.8801803284256359, 0.8801974001225148, 0.8505602401342147, 0.7257154620321173, 1.143936114166722, 1.065797685469636, 0.8062292965993624, 1.0194650195003334, 0.9520695918497413, 1.0596875876216387, 0.7159328239503162, 0.9983160234752454, 0.9149765802358653, 0.6648695229349115, 1.235694633317891, 1.5639467354038161, 1.1960621364206712, 1.1432068008517628, 1.3478055161521547, 0.9884902908013655, 1.2226030487002748, 0.9942447645764003, 1.1914888869683717, 0.8272956741931392, 1.0418065632783677, 1.1585627398857845, 0.9676530706336542, 1.0147959259571808, 1.4098882901304663, 1.096390549202105, 1.3662873644353914, 1.1301996832809516, 0.966756807901733, 1.119825723171234, 1.1839083082629263, 0.7156997099569594, 1.0840637596980707, 0.9081381092723617, 1.2332831362956027, 0.8635443852176229, 0.9676266319554931, 0.9752177435720067, 0.9394080497207602, 1.6000765725338095, 0.6531959821499835, 0.9244959753502396, 0.6777262022012999, 0.5370665478327917, 1.225406064036843, 1.1512519091368505, 1.6116712481946938, 1.3170152066304133, 1.3025808563154615, 0.9071073319965149, 1.1966357741430869, 0.8712368484039033, 1.334208262833682, 1.094364621239206, 1.2184228307550602, 0.8393605236943349, 1.0539800875527519, 0.9396860681910093, 1.1550578951377517, 0.9154614442142958, 0.8287899378574256, 0.5188655555884731, 0.9509830677114268, 1.1735293029549811, 0.8554405316254847, 0.9413414239473346, 1.0265451150765434, 1.2635801465830572, 1.0294035477301087, 0.7122034191381429, 1.2499203449817053, 0.92634046792984, 1.4178452657212524, 0.8693712498591497, 0.7179570541367016, 1.0023743465147337, 1.0739533848352358, 0.9419040649021184, 0.7440350416686734, 0.7981041649079779, 0.712590941676387, 0.7788200637777016, 0.8374395587227562, 0.8973008732431034, 0.6788433678961006, 1.3028514766118238, 0.8090529032133563, 0.8592111362229046, 1.234488157640575, 1.295908397783998, 1.1927557195844, 1.298324570864711, 1.0388599318702183, 1.1208730034601122, 1.001898623318334, 0.8756446615385108, 0.9910296564516814, 0.7553458229378447, 1.0021535585847918, 0.9292229779561361, 1.0222769962065417, 1.335017123208766, 0.7286988888322506, 1.7538550259990078, 1.4846155305892703, 1.7240071359201639, 0.9806687330358494, 1.3431059537110506, 1.8197389570871991, 0.7236832640181372, 0.6791020354976902, 1.0884168498782425, 0.7313629569678471, 1.3281652448301735, 1.091363000869751, 0.8306651741793114, 0.8723662323384852, 0.72990275440453, 0.8865176067062244, 0.7444418886824922, 1.0843163287297992, 0.9698189325723631, 0.4718295851500902, 1.442121836485373, 0.8380274380273868, 1.0680204108909324, 1.1616459273082609, 0.8463260853925005, 0.9229243505251157, 1.2748695549014315, 0.8514246252362023, 1.0258475107431595, 1.1469782317797963, 0.9962917565439344, 0.952136953655893, 1.1358884071545519, 1.1404491377599313, 0.6751266254004158, 1.10972229253171, 1.1341479782291226, 0.8279084899208762, 1.2708897205664478, 0.9081643680289939, 0.5193638966521439, 1.0623218013394264, 1.0210497974881938, 0.8872275032384744, 0.6808177993729798, 0.6434860989927567, 0.8797626291937826, 0.9725605874206081, 1.067739980787664, 1.4729755812188237, 1.2401630089279778, 0.8052442045957651, 1.203556173723101, 1.1612281415530064, 1.0243537564933438, 1.024251266070457, 0.6352158099218131, 1.115528605191255, 0.9441398442922246, 1.1536129475949886, 1.1393057107052087, 1.628507467797705, 0.6886345933309832, 1.2679117803724984, 1.0515726422781895, 0.8287403764073155, 0.6342599462113602, 0.932809480378122, 0.6661080692727422, 0.9868927273267062, 1.2332623050087377, 0.9313651183157254, 1.0269822044978067, 0.6333044392301904, 0.8471134117671422, 0.9089286505203426, 0.8774042199161531, 0.8840052145867972, 0.8559776434182249, 1.0852201794024399, 0.7947939298765867, 0.6694652550179999, 0.736566728608221, 1.3197637465061285, 0.8809932745896376, 1.1525374174954597, 0.9332315681697605, 0.9313083170688513, 1.1421556604439431, 0.9874183446710759, 0.8980627691384517, 0.6445842430656599, 1.3517126624400801, 0.6999756903173115, 0.9222167537675237, 1.1008163766884214, 0.9607713215883361, 1.0556944122669594, 0.9171261825258771, 1.2500223393392083, 1.1390166356050033, 1.3349022312816683, 0.7879918804589441, 1.0141633591372576, 0.9934687899668739, 0.5542835992768258, 1.5992881258451022, 0.5913336431849133, 0.568014734071081, 1.112911617054659, 0.8681902510663934, 0.9631904003851128, 1.2122791703129228, 0.8385064093271891, 1.1819789825786244, 1.411030375348058, 0.7594665091022167, 1.2056990222273203, 0.8764504259920736, 0.6598867752403214, 0.8367835577432212, 0.8771850925021702, 0.9024076029571628, 0.8943222080610722, 1.1405033611010098, 0.5623552084965017, 1.0723697952978692, 1.1246217238121214, 0.7114191671513043, 0.5204233876730818, 0.948519076485919, 0.5352064011709129, 0.6335246796472337, 0.8368717525105703, 0.6592418323183655, 0.894171289742458, 0.7201108136346164, 0.8891335559601705, 0.9893129222995632, 1.1175930009321733, 0.9778167494971713, 0.8374865812501348, 0.8313747578615333, 0.8453006412449687, 0.5017893192667539, 0.7713695468863098, 0.71410222902463, 0.820969775620993, 0.850385163531584, 1.016143459562653, 1.1475594238923343, 0.8221504444966072, 1.4204274180100775, 0.3420732009339421, 0.834454295771692, 0.9255732708245491, 0.5191370019823799, 1.375297378076474, 0.682057480132971, 0.574102467986011, 0.6763878311581081, 0.8411669773533461, 0.7144324583534414, 0.6009791553529918, 0.8089146556854248, 1.2090087765601185, 0.7445501828434491, 0.8638210197891851, 1.3821875034059796, 1.07428461066179, 1.0062221583783015, 1.0470133309179515, 1.254341766142742, 1.0779902718180703, 1.105147190694209, 0.919480976520046, 0.9707758606387996, 0.844223376724111, 1.0642296769402244, 0.8490860628703285, 0.8915288709665272, 1.168384435084903, 0.9876394086895566, 0.996106813141812, 0.8581947168347916, 1.550252532999769, 0.8679443221684868, 0.7394968604627377, 1.548320883097666, 0.7594653445722419, 1.0658577690194377, 1.0441404545706425, 1.2427123378702505, 0.920202593192076, 0.8027591673044057, 0.9637952884664245, 0.6955644276419706, 0.6865524454828187, 1.0685288337928749, 0.7707949365383733, 0.8888750148129153, 1.239739744035821, 1.0739809933627726, 0.9153174365878302, 0.8972030115917258, 1.0494699285143898, 0.9153148446883355, 0.7035222334980433, 0.7555612083001437, 1.0301852754702874, 0.7782320233325024, 1.1655428781294435, 0.9622270870918544, 1.4658892115102726, 0.810345130390697, 1.3287604137703224, 1.6885401835808387, 0.9479900377557191, 1.340117211691983, 0.8337382085025078, 0.6282838030585591, 0.9250756130161342, 1.1393794435772278, 0.8820886631278725, 0.8249492901551305, 0.6495157926320416, 0.9573785184257897, 0.6158797572509587, 1.0840246831803095, 0.6547318984601069, 1.2761217157045999, 0.7915468001491809, 1.178215077997564, 1.1841708366513477, 1.3304587220272301, 0.8369963568833974, 1.3735212299737247, 0.9815866901102849, 0.755998068621188, 0.9180187839865477, 0.9001642703089727, 1.0235747003555298, 0.9229275291715717, 1.032097413656595, 0.7715049777712141, 0.6357982099385909, 0.35007366459300343, 1.1453890237802196, 1.0179734258558235, 0.700397708199241, 0.9769784237567644, 0.7421187689143183, 0.7108710963600844, 0.8551056034553983, 0.5928692303273747, 0.9118030685142756, 0.44422624053917437, 0.7850231103296881, 1.258366096786903, 0.6911825507704974, 0.9473838130526377, 0.7721281902098551, 0.822283537216, 0.7866182087475775, 0.8089998785011676, 0.7675743638139305, 0.7905522753308704, 1.2566328167227208, 0.9162113887544663, 1.447924183332003, 0.9599842130220854, 0.9618124189297286, 1.1665462534651796, 0.7149868279679668, 0.8921596566196024, 0.9340509914031472, 1.2004065941382147, 0.6458146342174192, 0.8843452069609639, 0.7360017748573041, 1.127981218356712, 1.3901190905837537, 0.7471651697626791, 0.9656657450861161, 0.8157516645277711, 1.2648279614954236, 0.87832049477737, 0.8572762894086894, 0.6917408124138327, 1.0096062372019001, 1.0143531504497734, 0.7880865606747773, 0.8480105642394309, 0.8518996750499064, 0.671729764170691, 0.9206297460969513, 0.9300933849261204, 0.575407053051522, 1.0453211376883769, 1.0185275488762078, 0.8767921851572229, 0.7654369384082245, 0.7082969614929023, 0.6323294615134214, 0.6586337243667757, 0.9954883851676153, 1.478715851470424, 1.224583768438277, 1.0939016471065859, 1.0250360700819228, 0.8813096151484585, 0.8259960667699829, 0.9602814889269091, 0.9621939538319906, 0.8758580222803181, 0.8487440712911599, 0.8241466525246552, 1.0120942696098707, 0.9443048534963884, 1.745136268277824, 0.9535688177257977, 0.985510414176517, 1.251603403548347, 1.138682378667182, 0.8094324985574272, 0.5494434550058591, 0.8063043252248614, 0.6781806670662585, 0.5349798906119385, 0.689720938758067, 0.4629476441974528, 0.795505764649211, 0.825419184301986, 0.5172750593752762, 0.9160785060615311, 0.9970676109396667, 0.946712561692947, 0.9846982312924932, 1.2036961258041274, 0.9707851480964335, 1.1812475555159578, 0.9009485517165838, 0.857643473974038, 0.7192247591798738, 1.090468683593078, 0.836612162806771, 0.7638061582347452, 0.9736627401357054, 1.4275010264301984, 0.7505533319279768, 0.8973251867015459, 0.9756496136902115, 0.7210843354483362, 0.7325109497872122, 1.0048960785367946, 0.874977302298235, 1.0926381696734513, 0.6855939859897879, 0.9147866722694914, 0.80488059234898, 1.0946063064651306, 1.2853055378306997, 0.8512291600557367, 0.703708241440001, 0.6300823865398284, 0.664565099173901, 0.9821315884410886, 0.7696326828264928, 0.7500983106961814, 1.0471381178356354, 0.7279069164545849, 0.7150891409668267, 0.6844507081599566, 0.6436283566795962, 1.0104829062062197, 0.6539926838874817, 0.792017578290811, 0.608116285632355, 0.6448230957554388, 1.2709845733642577, 0.8766735905867357, 0.6486732589353945, 0.9413995415317731, 1.0994623785307913, 0.9455099134418476, 0.971931430089434, 0.7685204887830598, 0.7044504470488543, 0.6787962685209332, 1.0806698952557334, 0.6431933297849527, 1.4112257698527253, 0.8354799361421603, 1.1264970682895545, 0.6521941158559534, 0.8307289477146655, 1.0214330926306046, 0.9259741836699886, 1.7478318630517253, 0.7689803775559124, 0.9534418642667581, 1.0026428330512274, 1.0650031709545371, 1.1872572858516985, 1.8775010270783394, 0.9556826636507795, 0.6039030941895076, 1.4287962912322283, 0.7793129405219517, 1.1004992164480503, 0.6481872310060444, 1.47571464850797, 1.0640573101532689, 1.1185197156828193, 0.7859458550009082, 0.6541407139692019, 1.0503174453951938, 0.6282556755718215, 0.8513138402947773, 0.9576807082899876, 1.08122479303275, 0.7164023929316439, 0.8238327981890118, 1.0033840460026222, 0.9951742377036658, 0.7940973328580759, 1.2572219137753842, 0.8058052155950775, 0.8393064837267964, 1.2706674910892746, 0.9254128291339458, 1.1462184972233243, 0.8944909654263826, 0.8386920410704137, 1.0933999621678911, 0.8475333716016138, 0.7790879687507114, 0.9263614711401681, 0.7440792852730098, 0.8453918467346133, 1.2200193750565873, 0.7982636644652504, 0.6347684229562109, 0.88725870552226, 0.7559760514138725, 0.968305459789846, 0.4559733581814365, 1.1028813820240269, 0.8264737776049611, 0.5489174205296999, 0.9763231071447714, 0.9184229043813852, 0.7113829613993408, 1.346112048754736, 1.117582299549312, 0.6255000942445053, 1.1246747023718697, 1.1888648824726706, 0.8777755286371931, 0.6182277160883716, 1.0124935887235424, 0.7213419406063907, 0.8460903337127285, 0.7380740287960921, 1.3879315605252494, 1.0570267484168046, 0.838950379839266, 0.7324608331508738, 0.8850134614660682, 0.7493473740261287, 0.6558502697489297, 0.8487136064669787, 0.5942159737638878, 0.7391348735385116, 0.9678965341056877, 0.7071838872416036, 0.7638629311606998, 0.8246001759351185, 1.2535664769735504, 0.8561183601918847, 0.8231132261780487, 0.7234250952886498, 0.7891532789132535, 0.7967372845690024, 0.9275067470276304, 0.8525237341786758, 0.6872559735456458, 1.0269005484330027, 0.783314283811129, 0.8963418136433484, 1.1556351169463128, 1.2896650232772622, 1.259861983073423, 1.462694634786326, 0.9251390812583784, 0.6935245922148398, 1.264247677061293, 0.695416707393927, 1.4411222364536669, 0.8216145587080412, 1.4953089476970818, 0.8711963891859178, 0.962231953938802, 0.6054379417782738, 0.8864801124092583, 0.7161917984298778, 0.8873151935937204, 1.3004504706636184, 1.0130318587270641, 1.2111459292457494, 0.6421609965047159, 1.105166042212284, 0.8034919202109396, 0.9644532925409722, 0.5253921670459565, 0.5513582639688316, 0.5193605249969118, 1.2817598069155658, 0.6929769784123839, 0.7875352546051666, 0.7401684524379132, 1.4793330152829487, 0.8335998875127834, 0.5915979749093324, 0.8710033227380458, 0.8209247844047795, 0.7400508372533411, 1.0253261737334423, 0.8202632038409894, 1.0040342657923538, 1.0398812962239519, 0.6562563576884084, 0.787387192443371, 0.7798777750560215, 0.5215361457925287, 0.870147288884872, 0.7271084177024048, 0.8513424110153247, 1.3745984742580315, 0.7326287539840658, 0.693475461902475, 0.8052677212198223, 0.573883540384803, 0.9351731298289774, 0.680383030815401, 0.5454849938833087, 0.7441659460756294, 0.7179036909902198, 0.8379756685670026, 0.8146457098517754, 0.7099251720407508, 0.435640314334724, 1.3584893983507915, 1.1162327653322464, 0.8638303526112898, 1.024485578715887, 0.45581688076319404, 0.5347859595203142, 0.695830934536031, 0.9053785821719046, 0.7334959992082604, 0.68337178560012, 0.8833818161415248, 1.596209323341977, 0.5946742111263854, 1.1972870060643663, 0.9919872910544069, 0.7257829950742676, 0.8289971136324332, 0.8166158073683949, 1.042549423187498, 0.6603359462517114, 0.6361363231166127, 1.1224622317722865, 0.6794022418265427, 0.5884015685506858, 0.5904638963281392, 0.9616363860589539, 0.8525055609323843, 0.7905537150503156, 0.9321244564304104, 0.8276694215909399, 0.9602978949420414, 0.7796599313739093, 0.6882211339971089, 1.0796312777201336, 0.5323638374791083, 0.8004016478308316, 0.672257078293922, 0.6252289248503927, 0.7049298263781725, 0.7957172944810657, 0.5782986681297343, 1.2098325061449313, 0.7099425111364583, 1.2548438388656569, 0.7017309688531905, 0.6725400366504803, 0.5734325144427392, 0.9230857002516807, 0.7253572023523647, 0.6115432629778941, 0.6881560972457045, 0.7061779721577963, 0.8340154261911389, 0.5951377167512766, 0.4213100160871234, 0.7046421714056106, 0.531310479929953, 0.6366752165141123, 0.6055385409344684, 0.6230879282572912, 0.6167027979341458, 0.7886651193527948, 0.7616389109579739, 0.643053323634817, 0.7038196207228161, 0.8355762881445912, 0.5958485055692269, 0.672795671704355, 0.6308362897829947, 0.7557930684972692, 0.6246868762625124, 0.6515007240444418, 0.7879171192078364, 0.7412496323496061, 0.6010312508384963, 0.6963745147753985, 0.5825130689793885, 0.6463651911756388, 0.6787185410508736, 0.5645242421356313, 0.5421566002566617, 0.7158355541736261, 1.1313384251780323, 0.7797445290316967, 0.7440026183275751, 0.6219471240757174, 0.5378467057023572, 0.66086126698388, 0.9666678764007905, 1.0239707288393791, 0.4570338818691206, 0.6929208618182586, 0.8577915372615965, 0.876612930050144, 0.7901088588756064, 0.7151120670025166, 0.7573163815135948, 0.6836488387077642, 0.8911598803478601, 0.6347981700345807, 0.6618129442026326, 1.2273681817359918, 1.0164806908859318, 0.6379354380513286, 0.8842499492606338, 1.3433664944734351, 1.0033478051110514, 1.2451317119598388, 0.80280852602637, 0.9739538645847535, 0.6105446227494772, 0.905096429208791, 0.7300135717728852, 1.0898359278009904, 0.5923413546819187, 0.5843252084088227, 0.6164688483318249, 0.5888870930774903, 0.9041363158978915, 0.580456930971765, 0.8610790630820748, 0.614813879263782, 0.9112913113373977, 0.6587780755906267, 0.5636321853591595, 0.8496735605799166, 0.754086304846264, 0.9247760961206339, 0.6170593774438302, 0.6238310026577707, 0.7917133253341435, 1.4043251487186976, 0.6348801815023359, 0.6575351750221781, 0.45709543765935684, 1.3156718611044569, 0.9419911181014047, 0.799931048908619, 1.183397927612224, 0.9268394921344033, 0.9052357171004985, 1.062524067721383, 1.125277406568411, 0.7974386120493201, 0.9624877227298798, 0.5512202185524835, 0.772958153811368, 0.8730087125662601, 0.7458399929594897, 0.5714415838180319, 0.8747361771207445, 0.8538513851265425, 0.9308374879161466, 1.0312318952716126, 0.7224308014389302, 0.5325287885599204, 0.8394487987771502, 0.48295424189497094, 0.7463352068994279, 0.6422656922640388, 0.5991400259103024, 0.6699792623519898, 0.6846882933846657, 0.5077332674242147, 0.5831504275441652, 0.719925530462554, 0.6000482857882321, 1.0666359210736824, 0.6176806827397046, 0.6944439575631306, 0.7271226596832276, 0.5767471427678771, 0.6665440779479417, 0.7798830893132594, 0.8216725731948438, 0.7576877893720354, 0.5388740544676953, 0.6674116015667286, 0.31577864640678077, 0.4717155866636687, 1.437569729465622, 0.7126808533883938, 0.7089108244578044, 0.8181510736939073, 0.5919113375010262, 0.628130533417494, 0.6197040145737784, 0.8677811235215286, 0.9665273389258943, 0.7019059009491161, 0.7322189384518247, 0.536275407698246, 0.586917886654026, 0.39588543377102786, 0.5899328608583664, 0.8311926873917599, 0.8173487405928354, 0.5563974046707154, 0.6972974580651926, 0.8094216861477145, 1.1556745629840428, 1.1103872880714083, 0.5683022222763452, 1.0347159898160683, 0.7655977959103055, 0.6498540301101228, 0.9316361184387081, 0.9778564363259535, 0.6305086458876437, 1.002270929629986, 0.75941462573551, 0.8106741627057394, 0.6691614894722149, 0.9455968059605849, 0.9754152627869365, 0.5018362113410673, 0.5832397493965182, 0.791679816965073, 0.8409244313384547, 0.5841146588624271, 0.7880002204454863, 0.5932561661540177, 1.6337285497972183, 0.7154249227680107, 0.9949935488630193, 1.1780545204350736, 0.7282226205788009, 1.0459199438233402, 0.6882930566738178, 0.8365577995122134, 0.8790412336315543, 0.667594473039261, 0.9204004294388778, 1.0287958869550642, 0.8356278513211958, 0.5975985069866174, 0.7285283730733637, 0.43116062241371234, 0.7232989207207905, 0.5422379001980138, 0.7584732709525008, 0.7666049812620497, 0.8201544342339753, 0.6461612491368651, 0.6950147837319749, 0.9895483381860144, 1.1902399931092194, 0.9890311458958774, 0.8176790155638255, 1.0750016386271555, 0.7224615436955792, 1.0378469571962463, 0.7409112698008345, 0.7625781922204534, 0.7450444741476149, 0.7154785390514946, 0.672054745595306, 0.5917453299035559, 0.8109586923064311, 0.5947003180793577, 0.8477823843306554, 0.5983637566841242, 0.9723951781237566, 1.288781268113143, 0.719822183233319, 0.42307284852674787, 0.7724553642291594, 0.6057339514735225, 1.2786632175172101, 1.0004941806926593, 1.1165319486196155, 0.9037628382791727, 1.1866027038449447, 1.2385275370617634, 1.2894017430830322, 0.5591827998424372, 0.8577862847753955, 0.970322733760082, 1.0927522720328409, 0.8941569113352943, 1.4053540716955673, 0.9596153445058055, 0.6902937363103918, 0.7717141502778171, 0.7796277565393079, 1.1782256406186384, 0.6358721921970318, 1.463084186721634, 0.9253808588622718, 0.7876074981631231, 0.9844816290994615, 0.858295359384446, 0.677613742419058, 0.3885059015207123, 0.9256006854891436, 0.5604220416082518, 0.9200969307135475, 0.6213003773316397, 0.6077728949028216, 0.9112646770387546, 0.7733752802868822, 0.358765822190318, 1.0160327140076462, 0.7460010928277034, 0.4586459518614269, 0.5569271928923472, 0.64866645971934, 0.9433882452648106, 0.5079769712781149, 0.7531444003429318, 0.8766591073094375, 1.008341125205711, 0.6057417853995052, 1.0091197141648447, 0.5638861553685194, 0.5166247166665258, 0.48456322199967916, 0.7341886923822241, 1.0228717952304416, 0.5720389784899699, 0.45679576026068797, 0.6196405237847633, 0.5279186024183394, 0.5536679951060902, 0.8461518572204876, 0.8261167089501289, 1.112196070019978, 0.7917681641631074, 0.9799807692330982, 0.505362732737672, 0.7462126738800962, 0.4063915833114464, 0.6734919476795254, 0.9445882888067336, 0.46972598277949534, 0.6702830796691305, 0.8540281903379858, 0.8716643682325849, 0.8182593783258876, 1.1509398067551024, 0.3411535459614311, 0.8762390863723543, 0.7082794103168306, 1.0240228257860458, 0.632483969097595, 0.6651756835706306, 1.0213788161466728, 0.7533662031199644, 1.1587432644306084, 1.0743279897425164, 0.9636071441496684, 0.6032103383785673, 0.8365663840628079, 0.9790349216503774, 0.6153362932967564, 0.5885428327133131, 0.463796624479316, 0.5019982364098576, 0.6669098085827297, 0.6431078944728645, 0.8248662138366386, 0.5457043925905465, 0.4388461867473278, 0.7746415657467313, 0.7322698893913856, 0.5464056401924557, 0.72468667246779, 0.9764631791548295, 0.7021969054119228, 0.777685738851065, 0.9608663310223852, 0.5092500333593349, 0.5325170654490374, 0.9874082823160881, 1.0271516713569084, 0.7029486602427912, 0.6120042726728652, 0.46963312976444554, 0.5978843237224378, 0.892703780034371, 0.9916513181734488, 0.74603453994733, 1.0957451246482068, 0.8064684902093349, 0.9023928822789873, 0.4259808344897159, 0.6620542719175777, 0.40663414209347054, 0.8412385455622413, 0.603358059072853, 0.5762317465285587, 0.6799157834586563, 0.6370713893306694, 0.6724542408852285, 0.3726581441192113, 0.5701480048497519, 0.8503697331326915, 0.6711427643840173, 0.8490390168071714, 0.4510785537342448, 0.5579876753262111, 0.7722587831802876, 0.6659077582917772, 0.7057515997563262, 0.5840164974300126, 0.8394729412788953, 0.8334427520947918, 0.8675202584685258, 0.6461988416860146, 0.4542527185456577, 0.6249490240760158, 0.8242493097102586, 0.4652505423807881, 0.7156187522144343, 1.0032768020932636, 0.40565198762818017, 0.6797478033750848, 0.8047868727612238, 1.178181275327405, 0.4759713504517, 0.8513218104564648, 0.48404943120162197, 1.1105933172919276, 0.8016197147532406, 0.7388069602345411, 0.3955332608275361, 0.6966987646805061, 0.6869938227624605, 0.9659979384680959, 0.798384111151736, 0.4284301740422619, 0.8181804110071973, 0.5606100575107741, 0.5403414135405181, 0.8973362837729841, 0.8985714097017189, 1.1288822901845705, 0.7223973972148652, 0.4820396099610059, 0.998557771955108, 0.45263363535841583, 1.1695689220704895, 0.6502386685573693, 0.6073916199919465, 0.6650817785198841, 0.592368149176622, 0.8482737442038772, 0.8023252246355769, 0.9001143769611882, 0.997675519519382, 0.7609472592671712, 0.46752808872487267, 0.7853427331375353, 0.3904988214912077, 1.1601451945201657, 0.7427284032285394, 1.1592662309588808, 1.2471724439836334, 0.5432224644419483, 0.8226839020720915, 0.5680674839567079, 1.16854297941927, 0.5651475423290616, 0.5870493610742079, 0.6903412601648471, 0.810468893918124, 1.5129837390151863, 1.088712978008643, 0.7701016713548684, 0.7453813556349758, 0.6674274717136751, 1.0288855871120532, 0.5175049267490763, 0.789578574833117, 0.8346017558146744, 0.9277171498221474, 0.6497454666249032, 0.8261293815314698, 0.5323184609107952, 0.8633834816928451, 0.8159151957795612, 1.1623163986319942, 0.9416343204902882, 1.2672180984403703, 0.40634805043231265, 0.4332410559438539, 0.6279616347445217, 0.9552360236675692, 0.7239262923138579, 0.7286836994689774, 0.6590689499484856, 0.6472008336385091, 0.47558189181673294, 0.5328396853085222, 0.597307508093679, 0.8216748934896213, 1.0145032469633712, 0.8789278652175158, 0.6057378270071323, 0.6100863982120276, 0.5623651312646412, 0.8817769968336908, 0.9972757846182517, 0.46379925066757677, 0.5572810770262555, 0.768678487377277, 0.7747895704375373, 0.6632027405681031, 1.0672834140193483, 0.47391770077386075, 0.8981535437750438, 0.6967714912172348, 0.5939577128230662, 0.585692217859328, 1.0219946537417965, 1.004564505985805, 0.6126872067702445, 0.7462150627820469, 0.6298570279240564, 0.8890606529179539, 0.6182062541569149, 0.5568258661980316, 0.8674783706665039, 0.6773301668922223, 0.7973340118403758, 0.906433311380366, 1.1938090236017291, 0.939130037630758, 0.8533748081402901, 0.5704697973998912, 0.5074258854275657, 0.5356337050602606, 0.6934822062607511, 0.8023588193648044, 1.072300270233082, 0.6808180486454682, 0.6282386419209235, 0.8673534036405686, 0.5701000396169797, 0.9339125152729473, 0.8896970173307374, 0.7084748980618906, 0.7796447840881793, 0.5528789780815385, 0.4911797255386115, 0.4229329290844145, 0.8965776361485638, 0.25170113321126525, 0.7880393977858062, 0.5539753594069645, 0.6999422562756792, 0.47677003648546, 0.7750298901611541, 0.7877638128068712, 0.7578399784867609, 0.9596337725377232, 1.0505508364081717, 0.7080922398126301, 0.625666715718117, 0.8341371813877302, 1.4502688446597778, 0.8724207076839372, 0.8061311509670356, 0.5490570612792129, 1.028637559664085, 0.6135606591389131, 0.5955578827887082, 0.8740443793210118, 0.494479385637476, 0.7165629108980263, 0.5013303587272222, 0.5905612367638641, 0.7944320038010605, 0.6915089214765109, 0.875007811046782, 0.7095201585231684, 0.29814229079814025, 0.5345379238388166, 0.5844540292566472, 0.5993280870399196, 0.6811624215096652, 0.5528311874079361, 0.5346384364592826, 0.6497039286879323, 1.1067391054943558, 0.5296157707305148, 0.7495939278462597, 0.8064577349301043, 0.5051576525270536, 1.2524214628363857, 0.6807097842744758, 0.49850056942425114, 0.9702982597275385, 0.5042215179084911, 1.1113686503068927, 0.7022756375723244, 0.6753714491437365, 0.6431215239376883, 0.5916908979415894, 0.4732114619842062, 0.5897289861191332, 0.44801848042056935, 0.9992485376504752, 0.4812714591968022, 0.6768150024414062, 0.5653390424117817, 0.6896541426049754, 0.5727701498364068, 0.7307400602349488, 0.5516057324458847, 0.7664492426352068, 0.37528521926140745, 0.43715014856787127, 0.31435159560740233, 0.7688570357941009, 0.5883440326435775, 0.5483233429933811, 0.6812141995251957, 0.5823211529554226, 0.43435265058645134, 0.4049763056711063, 0.9295532215055118, 0.42488830532898775, 0.6644000446385351, 0.7172442574934527, 0.6184325848108444, 0.8713842550338124, 1.072679069307115, 0.7197546347865351, 0.6574084122975667, 0.8245096114277001, 0.49778121643460416, 0.8408547069912864, 0.647159847846398, 0.549052712291178, 0.8866876194312736, 0.4002959124286048, 0.4150778413408953, 0.7765465850727533, 0.4689606987513028, 0.44457663862396685, 0.6794230772898746, 0.6928414908322421, 0.5788486554438499, 0.7485406950999625, 0.4566231331726847, 0.558191932797199, 0.9216703887135214, 0.36432764852135197, 0.6978528628969565, 0.4080344636994179, 0.35013771983776015, 0.3683221918163878, 0.6496201202115877, 0.41415606049633535, 0.6256011159047657, 0.357101762116134, 0.4643403790500543, 0.2813211062063958, 0.5301343338898938, 0.543272232046031, 0.7737041766486243, 0.7571868553058734, 0.7425336985599189, 0.5154186423681872, 0.931570018193419, 0.5197075353209696, 0.661872740741416, 0.5118551857451088, 0.51963446875355, 0.5025793404431377, 0.4320367812332296, 0.38905164355290134, 0.48255132074709295, 0.49031984737905476, 0.810108938861838, 0.4424794170263504, 0.5957916217735779, 0.502187183826321, 0.5894018055410946, 0.50305528651227, 0.5314148525848202, 0.6375633818580783, 0.4031680060886755, 1.1267635988087625, 0.6248830058357933, 0.4625879871205661, 0.6577716327614498, 0.3115806152237264, 0.685130005528391, 0.6282151277608281, 0.6673350967168366, 0.2409762923691797, 0.395834165091043, 0.6233535927682083, 1.2770662773078916, 0.46020970888416074, 0.4141484292348226, 0.47095276210903264, 0.35696312333738034, 0.3236977228625067, 0.5371866443829659, 0.5034981429313266, 0.9063464046623004, 1.194624351567382, 0.5314866359427749, 0.6536196524789538, 1.049061797826718, 0.8363905728921004, 0.6653617929176485, 0.7300714036671803, 0.2551380799113331, 0.5200532116714613, 0.6138535891776478, 0.5763470697921936, 0.7329306544758549, 0.6131567133569388, 1.4307740115296614, 0.8844149290225207, 0.718024479738848, 0.6941069472477001, 0.5954132455768008, 0.6573772893855775, 0.543977168562899, 0.44790358376175715, 0.7377136317510453, 0.3913019146211955, 0.6492790592078006, 0.5479681604758077, 0.45194168400454837, 0.3776804361367285, 0.8325724559222465, 0.5604873755917088, 0.28000489248920546, 0.6312292013749428, 0.543261217865443, 0.39800706060245783, 0.39520224040070173, 0.6573664276700163, 0.3523988583428519, 0.6462794736937765, 1.009752475325413, 0.7762860301454836, 0.7366648690869109, 0.42901492619945103, 0.5031463071734465, 0.4323405687704363, 0.39526247012697685, 0.4345366619286842, 0.7410935721688418, 0.38590045647743426, 0.7148520426696248, 0.5242409490816521, 0.4187607248393497, 0.7461382212963892, 0.7884242948303875, 0.37054444776786555, 0.4470316059449139, 0.9724335076608066, 0.7975847803134475, 0.5429913612921649, 0.5656233613618228, 0.7872780098027481, 0.4807530942146595, 0.4776527442472445, 0.5970979189695419, 0.42176464633045035, 0.5343365131742296, 0.3936265221841615, 0.6589662194893722, 0.45517648595213644, 0.7782327707364008, 0.6038099862047143, 0.3884149777140594, 0.639166493864858, 0.8143337725441236, 0.4687222387531208, 0.4024986978560602, 0.6068571649936207, 0.5224160250720814, 0.34932715603429026, 0.41204179021714554, 0.5512527297612719, 0.4841710677503066, 0.35129014072996195, 0.7151238784172402, 0.8204742589261798, 0.833029850731984, 0.3439804540338741, 0.500661337525026, 0.7034284418704463, 0.4667882971342405, 0.5834336346962804, 0.8477311626820228, 0.5540919412228098, 0.9163802859501808, 0.5825881263160635, 1.0139173866453626, 1.0368961153158138, 0.91649509610992, 0.45524195482125096, 0.39827403529247957, 0.4652688619604666, 0.42254605347614344, 0.3482927006139283, 0.4760509044834754, 0.538272117861852, 0.462343086136712, 0.8099675976474199, 0.3194421427018873, 0.4888980866969197, 0.37580759471766706, 0.6416892274220785, 0.305951754976059, 0.5952529138576073, 0.47434544111227056, 0.3087046540958186, 0.428713951785155, 0.44172408060877366, 0.6919766321429959, 0.3919756186517895, 0.3321257127434465, 0.8496779037800648, 0.4437072495847277, 0.6228472785560453, 0.31966704913222516, 0.44672768453154904, 0.8568411341167632, 0.380202279468672, 0.3279483897276599, 0.5435387067173414, 0.30212839762780774, 0.3231123508091041, 0.42621010370743584, 0.3897316090154759, 0.5822119674820831, 0.4301430687352621, 0.42243362145669716, 0.39832974235333307, 0.5391144544319402, 0.7836860032515093, 0.3168578630915294, 0.35084352708124855, 0.34201792811602244, 0.23623233627261153, 0.33215449811698794, 0.4372676741421878, 0.39326040031563525, 0.8856544152895609, 0.3767326517802913, 0.36870349997826857, 0.5788033414423724, 0.36275660550722516, 0.42749395895865555, 0.8015830468927693, 0.4460630348003994, 0.5226384947981153, 0.4155799151365791, 0.8803787308177728, 0.3944787852764885, 0.4779013588775677, 0.793781359962266, 0.23456716524788127, 0.420981978355648, 0.7696248639746928, 0.501803852635571, 0.29381389115986073, 0.37850994288552453, 0.4361388926690327, 0.48948188835703926, 0.37850873098744975, 0.45697878924283114, 0.3015043631054107, 0.3844699100714951, 0.4102790923295716, 0.249420047633807, 0.4184526504046751, 0.5523766108311206, 0.3263259118418808, 0.7218303380036414, 0.9054835302109167, 0.3718634880269063, 0.8392111389360133, 0.37799564940588815, 0.7205467962956691, 0.37609382407375114, 0.691621533754412, 0.38775925630666386, 0.636557574915792, 0.5497189342713618, 0.35385204640464757, 0.4678694437971019, 0.5210705708975744, 0.4077529310457634, 0.2999557068000661, 0.2912114231747792, 0.43935322639677254, 0.4090907098056341, 0.244338550642425, 0.49483402633024787, 0.27570494669144197, 0.6762570973725347, 0.42288165897518015, 0.3092857011159261, 0.36440812218153507, 0.608368159480476, 0.5998208408644705, 0.3874924934556332, 0.23098126685854944, 0.3304246490720718, 0.47620591141960833, 0.45333359065046797, 0.4159882216426756, 0.3751240854227809, 0.37596554197446264, 0.7343424117562856, 0.2749802042166163, 0.37028656289691014, 0.2945649372903924, 0.4551487388773861, 0.7611014329035182, 0.3020338614781698, 0.28113967401296025, 1.2311383363835844, 0.4014805845336203, 0.6973722086804024, 0.8279051229233666, 0.6156474717595705, 0.3662235949353556, 0.3178706325086436, 0.5169347647822521, 0.3942527848074344, 0.4263914647950807, 0.2932098412513733, 0.2908440783396701, 0.42272242452159076, 0.3213988462926842, 0.6627738710498967, 0.7471105533664312, 0.5507368965542718, 0.34133298701308795, 0.3949439899846651, 0.28833162483714875, 0.6940839694175885, 0.2561621939735566, 0.6447642236356618, 0.42686110375043873, 0.3719038996424799, 0.4317255092068554, 0.36340854166313463, 0.2537258474032084, 0.36619868043181186, 0.26122595447081104, 0.2554052002098347, 0.26338964259985725, 0.5510302018348616, 0.24707530278425952, 0.4109469277930982, 0.36882597907249687, 0.35721883897647266, 0.32494637324243264, 0.283048964593836, 0.23634995199318753, 0.24876424531672342, 0.33533101762864587, 0.48042236498369134, 0.3412464458837349, 0.41720933652060205, 0.4146289779111294, 0.45644498344686424, 0.2720738404002779, 0.3503411293029785, 0.5648072355479403, 0.8935210091191479, 0.397931187151985, 0.8205447796606948, 0.3183494707125562, 0.3689966586919931, 0.536795627735654, 0.27655841762380196, 0.3129296628778631, 0.21320431348511953, 0.7868672438375242, 0.6932801330872025, 0.45714440605857154, 0.3962195318794172, 0.5628680137899112, 0.3268422215611308, 0.43460354161640957, 0.5494179404189443, 0.47130537553981655, 0.41452638213319737, 0.8389104040531692, 0.3874329745769501, 0.24783639769968874, 0.3721655009587606, 0.23308165503011988, 0.3064506829221306, 0.49664643385543145, 0.5767625285662137, 0.4604029664917598, 0.577355996152406, 0.25204172572979167, 0.41871631542841586, 0.2846615985289625, 0.3127270618580883, 0.1842547802859304, 0.23739250961102937, 1.361278289313413, 0.5503637921654299, 0.4036923576256248, 0.5578155062312172, 0.3227367704120271, 0.5880397371031689, 0.5026457530797359, 0.20179817835489908, 0.5293901964778944, 0.33058226495301346, 0.3249951523933007, 0.14343179692289096, 0.4852819123031962, 0.6066371989972663, 0.5899665359670727, 0.5158516632839715, 0.5013595669197314, 0.39689324462806785, 0.22584899323155186, 0.2979802014689515, 0.21147430043306645, 0.31554620302867703, 0.6893036403375513, 0.27691470894548625, 0.28285022897970835, 0.4714734058272921, 0.7920637607574463, 0.4116485611793278, 0.47714960400438133, 0.4290447271231449, 0.3609201992626252, 0.8879684598234816, 0.5472266942956946, 0.31628427358774036, 0.7672000949360259, 0.4784978210476861, 0.49748817660958905, 0.30463920560750096, 0.3279387750634043, 0.6304102824831199, 0.5930980938917015, 0.37348942198711405, 0.3308917601456912, 0.4985242176055908, 0.3469711502617305, 0.4933737998259694, 0.7411663258177603, 0.35802136840242327, 0.6806098934543049, 0.42259938737620484, 0.8919647834388394, 0.3006477369369234, 0.4140740396140458, 0.23288431792854852, 0.35965077456406946, 0.617552047123971, 0.392755245748288, 0.544110673268636, 0.310672132506812, 0.513044091394172, 0.36989138382371123, 0.42589783937404524, 0.45094851713914136, 0.6261906965267963, 0.22946240655589695, 0.5342371829549117, 0.473966804243305, 0.5492705478265918, 0.7434595622085943, 0.3159768724527054, 0.39165663556897, 0.32642875572991753, 0.37527891373967337, 0.36719912080203787, 0.7256810876301356, 0.1957872396285249, 0.20738932301841873, 0.3481998454199897, 0.43705503347428704, 0.49983497722840414, 0.31763087289123365, 0.23288272197655596, 0.5248106076982286, 0.21061019461776706, 0.4548383786128117, 0.2938354512829801, 0.4312619250666918, 0.469148720137927, 0.2250362021720361, 0.3994868455545651, 0.3125659175765463, 0.7499231624603271, 0.5303419797360396, 0.4023635163985924, 0.2641504048634242, 0.532988403387237, 0.4234578079880375, 0.42619595950589256, 0.2781505430661715, 0.5193813885187174, 0.28448287667814054, 0.4831647027225602, 0.26070628629881226, 0.2503477461132414, 0.40204891564010026, 0.27380614673243875, 0.31470858113749045, 0.38097184994520045, 0.1515097635697394, 0.22874438343332107, 0.5717967929166179, 0.6178913100948179, 0.7946539987523372, 0.7643598889756477, 0.6340515546124391, 0.907394800055725, 0.5246732115454791, 0.5135197250547037, 0.39683697370382454, 0.44448775802630935, 0.48153027913879765, 0.35437529528074, 0.5114786704625838, 0.2596992167385025, 0.3250211296347525, 0.4908163378229151, 0.3772532468491429, 0.6465634387292903, 0.4549123275625049, 1.0392395589843857, 0.38708320024871234, 0.7175703011484948, 0.5196420392461492, 0.7913351312028609, 0.5156441794007407, 0.47001805795106844, 0.76250125486137, 0.6092880726102775, 0.44042585585143545, 0.319688768235464, 0.5034320745727179, 0.476407540592664, 0.5630489375526635, 0.4508066291588273, 0.2791279599780129, 0.5234215854944906, 0.46861328730507507, 0.43061818872179297, 0.4750816856032637, 0.8016022397015441, 0.7748146404807408, 0.5411885619581792, 0.37056918691854185, 0.3892894957245899, 0.4080494203848555, 0.3335132556994023, 0.4414421021741136, 0.35789448768373516, 0.6104281022538355, 0.2346566985669445, 0.3418341362659836, 0.5290053619361071, 0.5414547760505777, 0.3972178381023784, 0.24708989546449614, 0.3436041267463634, 0.33569851541296863, 0.30204528513117734, 0.5158148976734707, 0.3469359202262683, 0.42495431386873095, 0.5635809238542165, 0.6857188463535439, 0.5615211006988735, 0.5746182992723253, 0.25933453616641816, 0.339549998953695, 0.2858667309023005, 0.4186506400133837, 0.3115419631912595, 0.26737229007107394, 0.45648378686471414, 0.2809687482181898, 0.27692545539852775, 0.5404373511961913, 0.2250320149071408, 0.4683735890047891, 0.474669221297315, 0.5540444103061644, 0.3026944989684395, 0.3083184109575437, 0.6080441429254678, 0.17305815453072576, 0.506738047879821, 0.4413818637220374, 0.3412207607462786, 0.6489673415819804, 0.31528555994650964, 0.824325992928611, 0.3538661430549003, 0.49464595351151175, 0.4259505758670814, 0.5997891811861147, 0.6468640849282893, 0.3903555029157608, 0.19029955645671054, 0.4486203958521161, 0.3465048001384178, 0.29833942740051833, 0.4566091852674263, 0.5401927045267871, 0.4449678834143528, 0.3661010461875635, 0.5057501049753536, 0.7986255666339023, 0.7223186780380727, 0.5620602348348596, 0.3342513618267402, 0.447255546921256, 0.665461609893375, 0.21237086801163413, 0.6931472131941053, 0.4106745882317001, 0.6607190712507351, 0.449997651737198, 0.7585940505562724, 0.604132772765608, 0.3035879733012273, 0.5330597069848588, 0.20770928700764973, 0.8096011050123917, 0.29719435980825715, 0.36470070937981347, 0.4163413021299574, 0.3402587161638863, 0.31643979154551094, 0.5357946367023372, 0.7397709054770113, 0.42867019974002113, 0.3655909442506715, 0.45491218050320936, 0.49232126581198843, 0.24729729087973581, 0.3383472460209093, 0.4355130907364221, 0.3454645031094485, 0.4001059270372577, 0.3786320609377142, 0.26994804700614017, 0.33725977102915444, 0.43072220248617094, 0.2561748984429267, 0.749243425793118, 0.23877652737428043, 0.3049850748472916, 0.3207462576796597, 0.2815310192238297, 0.3443586976973565, 0.30769745045171903, 0.28592040075022956, 0.23343534698805213, 0.4657772886817145, 0.5541433471200304, 0.30779632733983486, 0.4545713196425468, 0.4602420766606735, 0.4348309281090728, 0.702518116401988, 0.7524055973688761, 0.7405920625249027, 0.49877752031598777, 0.36317189401195893, 0.24025721609741826, 0.20960215280099873, 0.15905377582947972, 0.21554381758302124, 0.391100421154669, 0.43596450255482705, 0.6208127646673294, 0.24294077786664578, 0.3542670799082058, 0.36284661467113194, 0.2673445481864118, 0.8004402476976636, 0.39952135270454336, 0.6909980746542557, 0.41236571109656134, 0.4172806166044247, 0.5008288551099371, 0.46793313310259865, 0.6330923156329955, 0.3920824847435083, 0.5459634251327989, 0.17517540852721808, 0.6666643042836533, 0.8081943012181021, 0.43399195417991054, 0.41165316632299714, 0.2986024366106306, 0.31075913123113574, 0.33370719898923384, 0.31931965761017383, 0.8293439137746417, 0.4782822538045937, 0.20817412596482496, 0.2621329555022411, 0.4703270517760096, 0.5824070808310914, 0.30959261302234364, 0.30043106710209566, 0.27956350948294084, 0.6326240245638677, 0.41072214967090687, 0.191027567835334, 0.452295885540543, 0.26983431713683503, 0.3221343295766167, 0.30775190958609944, 0.27159379807608147, 0.5910738529962933, 0.47847018046685824, 0.22771399319564561, 0.38592900452766826, 0.4591596141006007, 0.17001157910484505, 0.2771689730279412, 0.40389877173139876, 0.33693672271001907, 0.3945904823387714, 0.3871913751254074, 0.2278348451808428, 0.5658814342804194, 0.37491525833651146, 0.24600194443871964, 0.4411014169689045, 0.26776321293711947, 0.4190216558630637, 0.5127037191644983, 0.3151183274814061, 0.6455761054893593, 0.5130171199802396, 0.4177955771111822, 0.3783529674875867, 0.3653806814776674, 0.2721160772931639, 0.24956448918132637, 0.3248684723770578, 0.6964744157973833, 0.3620136444702809, 0.5784141799957723, 0.3927951632641619, 0.34052839505004207, 0.6524271144826188, 0.4622421317036324, 0.3582809815644793, 0.7963553388345809, 0.32377129630966267, 0.5672107805146112, 0.25962710023115054, 0.3828107323567328, 0.3961754066923745, 0.5050349440766778, 0.5186577125851864, 0.3364045248123316, 0.26773800374333884, 0.27356531482476454, 0.4309756363254631, 0.3498202248831173, 0.5639144985544207, 0.9254598344696893, 0.22583743816766982, 0.30678237025976823, 0.27789499856176836, 0.3599361358748542, 0.27080457460630186, 0.3529097495255647, 0.4279007502138873, 0.2305608043649982, 0.2825372162457259, 0.2557890278815097, 0.3678483902250229, 0.26985866450300117, 0.11036652618672424, 0.36938281695048014, 0.157509679071831, 0.40064554150454423, 1.0186439711258108, 0.45825726958933705, 0.3728729963302612, 0.4460867375740107, 0.3217551822371181, 0.29226413715797406, 0.26835732178318733, 0.3451640275856117, 0.4377971965227969, 0.6129278390086392, 0.2919340746423118, 0.3120770734231345, 0.19408921628910175, 0.17888641187829074, 0.19757719331907359, 0.2133688868898334, 0.2872256681728289, 0.14885461837526354, 0.192013971677962, 0.2987177397199144, 0.7233940397283083, 0.27112965340170614, 0.43653346415604605, 0.20654335112556627, 0.16554879995180788, 0.14643813067316858, 0.23993928899722325, 0.2952151283763704, 0.29367901099033844, 0.3150631051431212, 0.2291159809145153, 0.36728928640776987, 0.278477990912505, 0.31230184218240165, 0.2990816853307037, 0.15770394869113988, 0.16662015101266286, 0.2440294250093325, 0.17497122050604685, 0.21343532917436617, 0.3190128788151989, 0.1684515466112079, 0.21904512528958206, 0.1873570622597541, 0.31230370451161826, 0.28354856289886327, 0.1806471764392063, 0.13722626638063146, 0.1774488074713783, 0.14780145973354192, 0.10567395759497016, 0.22475843331908235, 0.281296838647329, 0.26871689542891486, 0.19287845235882384, 0.22437842677509973, 0.21088511973418367, 0.19572438796361288, 0.633294808988218, 0.35246778255118466, 0.3721954162915548, 0.27900137149778187, 0.2464948605046128, 0.37862618535111997, 0.492752511282474, 0.19292662758633022, 0.4720357733887511, 0.23925699374684886, 0.3323894610771766, 0.31039048505858563, 0.1802958959369568, 0.48619660059611, 0.22089506912386705, 0.15393204131311716, 0.45766942554050016, 0.6912770065428719, 0.33357779159747736, 0.2811808757991581, 0.3099894962553425, 0.23200426407396257, 0.4694080609321986, 0.2523694889472357, 0.138748245070679, 0.2276437392855078, 0.15302239189008887, 0.13070281476685494, 0.26765859528591757, 0.19160910304794965, 0.21449015374581243, 0.975383383194008, 0.27446244499900124, 0.23969349338534376, 0.18336888538893834, 0.10682482986493447, 0.29528921331678115, 0.31207796474387745, 0.2581047089179901, 0.5291868739025901, 0.3940691220760345, 0.19060105508708744, 0.3267710961533698, 0.31032076193180397, 0.6287548141246208, 0.45588552563773693, 0.311225954840315, 0.24033997853597006, 0.2898234208424886, 0.19032841029547581, 0.37404836558640964, 0.3069374915134768, 0.3538422226905823, 0.20712768335484846, 0.3121249159177144, 0.1858594457857803, 0.22467119313712813, 0.19844551225822027, 0.24735071459492963, 0.20991810323437518, 0.5281376059850057, 0.27479216768139764, 0.16555178130477055, 0.27339654669089714, 0.31217203601067806, 0.4367229553369375, 0.4434186046842544, 0.12627468991774488, 0.18394363989561666, 0.15729609781132514, 0.4545479766425924, 0.12115649325805797, 0.1708297147289544, 0.1234694052352124, 0.45320641688027435, 0.25277332673368147, 0.3116616754979639, 0.43223992665345784, 0.2589032415730242, 0.37560931804908954, 0.38833886254917493, 0.2276928251009827, 0.22331121300314685, 0.23268048893321644, 0.2400743917985396, 0.2186898208111739, 0.11901951460611253, 0.2054048551288708, 0.6333908631251408, 0.1864028119040536, 0.3822968123962901, 0.27287625223143486, 0.25359001446774593, 0.46231361006245464, 0.26098685918030917, 0.22967115610571306, 0.21036658376593217, 0.22536903309426518, 0.5618470249595348, 0.3389430293582735, 0.5167000709713756, 0.37011153473492653, 0.7408807058311535, 0.49749906181949494, 0.4901223566797045, 0.4978233964015276, 0.2523613223823355, 0.158113082683176, 0.7054606990102844, 0.26940819208552796, 0.20249518119927606, 0.20247656873294284, 0.16828593672815165, 0.16704614081304947, 0.29068676776134, 0.4925257707942774, 0.46105469891003203, 0.42322693304808423, 0.23422902346756053, 0.5130396528822003, 0.33511388840591694, 0.5177108298347559, 0.5359190139844526, 0.28997760022024244, 0.5411407540628921, 0.5062142392255568, 0.2662431564597739, 0.18217245075299987, 0.596061376730601, 0.42444050596217914, 0.42722938895225526, 0.12983696460723876, 0.26261181900656666, 0.37231770231565303, 0.711529963032729, 0.4363518705608869, 0.4412447981039683, 0.2220442612816683, 0.6014985072418249, 0.27164305752179263, 0.3943796844234714, 0.19209924141332904, 0.4054796729769025, 0.34091892799614987, 0.20939930876096086, 0.31020512595192606, 0.1271124593870228, 0.4450239420559189, 0.24684932169079796, 0.18065867030431354, 0.607838684251332, 0.38241339444148875, 0.26296890095026804, 0.33785526728242393, 0.2516064016680062, 0.8906386239033941, 0.7794158579173842, 0.6272549307288755, 0.45441727354412986, 0.6045115915938741, 0.412671669217992, 0.4627714297645971, 0.2924022204185355, 0.37979815142415896, 0.2528615462070566, 0.47166335930248887, 0.2616139145276569, 0.2969487401133095, 0.3887219709740546, 0.3895848485470506, 0.9912856889214318, 0.37819962293038634, 0.4097369941167905, 0.3045169188984833, 0.3759459868409579, 0.2572506350964946, 0.35853073476540925, 0.47484807712851246, 0.48873118186237835, 0.24088355303842782, 0.33341215286736897, 0.44595444214882163, 0.5388051155042944, 0.22035931930967875, 0.5131379379277804, 0.7111931750407585, 0.44555718532172583, 0.3827102594187694, 0.399519100189209, 0.28571054498082693, 0.17308261828017807, 0.4290612972745992, 0.35172408658695575, 0.4314326530032687, 0.31256733805786013, 0.25472608873148284, 0.30963639974594115, 0.21207347362596485, 0.5193882765618782, 0.171420675485569, 0.2375946148959073, 0.2128834193593197, 0.7454300940988604, 0.32153289421260006, 0.33686482429504394, 0.18639207225142818, 0.16362170896912476, 0.3028525056261005, 0.523180158100431, 0.34177559820129993, 0.34313649164213167, 1.0251219088913233, 0.760908868199303, 0.21945188737617127, 0.5091007618374295, 0.3203913603359486, 0.7313862271160703, 0.3067059867433152, 0.4929211744635996, 0.6664157125584431, 0.2176248041788737, 0.6189962073167166, 0.2106973307174549, 0.2209684332557058, 0.4488030745188395, 0.23003511680306937, 0.6656771891044848, 0.37788902115988565, 0.45012801925341284, 0.4995495388539776, 0.5778985989129196, 0.5326448000918378, 0.23070694120415364, 0.17605707434227055, 0.26849120950735905, 0.2089772691259851, 0.20809376873983237, 0.18702260321216158, 0.2595482733514574, 0.17320012296470816, 0.2814081498792955, 0.26220437372941896, 0.30790513432532546, 1.1290100861262609, 0.5673121283271096, 0.2749044146920693, 0.45213085518175467, 0.45135624242895983, 1.4584805252965627, 0.4227198254858326, 0.2948407549340022, 0.5229379565380707, 0.24268308684559176, 0.5436411431861068, 0.3953918672778888, 0.33605973146228546, 0.2957841547186006, 0.1881182788558628, 0.2512661102290433, 0.46517341782224825, 0.3307574371569084, 0.32610674021125635, 0.19793924331665042, 0.580705814134507, 0.14667955222073153, 0.13550541377043698, 0.17066571624190718, 0.4513562176777766, 0.4982940248216254, 0.3608228851706435, 0.2536218223647465, 0.13680687729812072, 0.22309521220938336, 0.2766264782431885, 0.47943209545428933, 0.4229084279125222, 0.32588137735708106, 0.1356405473237086, 0.3690036380183793, 0.2825313919711846, 0.474366835753123, 0.2629262073439171, 0.28081194532223236, 0.1550103852798889, 0.5874558003076584, 0.4897367599381782, 0.23350172678629555, 0.524038584948432, 0.2239684237752642, 0.3446411784391225, 0.4354159966660903, 0.16283443329581088, 0.22721708780759337, 0.16189819600847033, 0.2621152589260004, 0.4600389539532803, 0.20170821960155783, 0.30583098600165504, 0.32616981679742985, 0.2918431291417179, 0.30930153374949465, 0.34357162176960643, 0.23814540960888858, 0.6032082727380874, 0.2209505330191718, 0.5365621590582144, 0.18481435094818927, 0.2831857959691421, 0.4758584841168626, 0.26388722812516885, 0.2899280444412532, 0.1289484768647414, 0.14576064836856475, 0.2819484416452477, 0.1681850461149404, 0.13576192403340676, 0.19347352627425166, 0.13227933715589296, 0.19215385546587935, 0.44123809369405115, 0.21387624348169085, 0.20946524591845012, 0.30742589411109383, 0.24627675910728475, 0.30056074837813057, 0.4673682859005073, 0.21132698528680965, 0.3400461018360155, 0.21830263310848438, 0.2619356251311028, 0.2650043441916144, 0.25039761032319474, 0.5097699282719539, 0.1993042518542363, 0.13483870228903494, 0.12512702841078757, 0.27708259915917866, 0.21493075246125265, 0.10131921970349853, 0.28562612120872755, 0.09096306578699366, 0.6427267871438146, 0.16517441559504795, 0.1900868553643698, 0.2658653683132596, 0.2826512705751788, 0.26786961128210496, 0.16586709556897858, 0.14469264125871922, 0.4363674229046245, 0.24802732819677392, 0.7690213264060277, 0.1413408484465804, 0.18044741272489667, 0.17853035438739728, 0.10159992755405486, 0.1073439349007637, 0.18056141271627119, 0.333846734629737, 0.19037123091015226, 0.1972350359237057, 0.19952778749532635, 0.16919588883717857, 0.1076243594452575, 0.201280658351215, 0.30441023826599123, 0.14452276633685324, 0.15078130816264954, 0.9391328359527016, 0.14635303859327947, 0.16684372795151298, 0.1342478283728012, 0.2695931198172533, 0.4763435638157774, 0.17416571393792885, 0.4665215900488067, 0.23841468352891412, 0.21396835676444295, 0.2645349733820765, 0.36453427612642586, 0.1822827100753784, 0.21442171977938468, 0.14838989973215683, 0.24066621749432054, 0.27688616459297416, 0.18640977206987364, 0.7410588956268642, 0.08680527929601997, 0.19427423266284086, 0.3072224179278965, 0.6899118959615519, 0.2893415227833165, 0.5929799206501134, 0.2547697287117129, 0.5978964763738992, 0.3649013065168206, 0.19706683889447843, 0.5793190623574909, 0.44266091237812893, 0.2818228060952387, 0.17672535618438606, 0.2772700450708978, 0.21815820907379363, 0.17113704791610435, 0.16998755909441696, 0.282496031987702, 0.26292972017859223, 0.18645622513510968, 0.2873754361552051, 1.087939883927057, 0.3596560072073033, 0.3041464238154245, 0.5357250136214418, 0.12059917060590664, 0.13466575519576446, 0.30465425938477014, 0.28389030506422447, 0.13587410990599974, 0.22706648485083716, 0.20732874852350464, 0.19773139600563341, 0.40308710894991373, 0.18106746706143934, 0.18299625847074721, 0.20989330511487858, 0.33658178032870495, 0.2685484606424967, 0.2329204262191174, 0.28101039295324975, 0.9527286780186189, 0.22196173327961607, 0.24651017168114184, 0.30579483244154193, 0.5421856088710554, 0.10583909137199503, 0.3481542398472025, 0.12321895371038447, 0.12999092423711683, 0.2304548762394832, 0.13609327109502173, 0.0889156081459739, 0.43314545072907695, 0.22327789513200477, 0.24078090135554095, 0.16912716964790725, 0.23856940046573055, 0.1394320511847043, 0.15932953095845379, 0.12287917932435526, 0.21258514241581689, 0.174845229573461, 0.33068261509849906, 0.262654991695608, 0.2691789427704394, 0.37818066142443263, 0.08797062092502766, 0.4053058716897808, 0.13761314787412351, 0.31443461560721353, 0.19213708451747066, 0.4131278801320983, 0.17955659122026266, 0.2577683871569651, 0.25552504355687206, 0.20773857621324093, 0.17567438970803614, 0.14968455134655023, 0.43962110431106005, 0.3240563680264947, 0.23989772326638253, 0.14924237760368753, 0.20485041319086728, 0.2633240657451349, 0.5244693965702266, 0.23527089065712675, 0.10004205910460676, 0.1232809867222863, 0.14714717712237207, 0.20684552556911787, 0.6584952360623842, 0.4169582155772618, 0.2055581972712562, 0.37126938434980605, 0.2144289282448845, 0.2224952075353622, 0.16636381733371797, 0.18486255660835318, 0.13124954335418812, 0.25375818634870695, 0.13099712348581144, 0.14885195530443635, 0.18437784740381072, 0.17416858737499682, 0.2609832781000544, 0.4907774940104047, 0.4495432905697715, 0.22698512971562743, 0.173033555436047, 0.23954901261258757, 0.7138187038537229, 0.21280992659219528, 0.26830110861255, 0.2261405514700537, 0.2125665194594176, 0.2996357185777886, 0.22451157778028458, 0.272868722560359, 0.34751750111579893, 0.14894209118487667, 0.207713976491, 0.11274816881129635, 0.14084039718385727, 0.2061681269647711, 0.13995299626434138, 0.35471258601031475, 0.17829131285349525, 0.21716568438127082, 0.17118744316524437, 0.4264229652485986, 0.22264888645872793, 0.15003495760984223, 0.22634712102300286, 0.10844292627283499, 0.1206465092236664, 0.16967066830722646, 0.2421287547217475, 0.18589104218457744, 0.15813708040449354, 0.23741886063871265, 0.4480547781551586, 0.12363941033681233, 0.5160838468071274, 0.14426811843019013, 0.3404922126884783, 0.15338133294552284, 0.4193868701642093, 0.18847821730154532, 0.1929772123749003, 0.10564157447535476, 0.09720750117868557, 0.20060440108793126, 0.32770502733461787, 0.12396616049137595, 0.15021217720181335, 0.9065006168149358, 0.15911767049269243, 0.21445694111956065, 0.15900766749111553, 0.21322682939703305, 0.1299927678078046, 0.27219907587224784, 0.26330218372922953, 0.14399693044026693, 0.16772773875691874, 0.4261606343859291, 0.28822723313797194, 0.2563755614940937, 0.19148768520109521, 0.16338197469420196, 0.17671206316345722, 0.26788608801407626, 0.3374454487116242, 0.10225838882582527, 0.08102442575542439, 0.13398099668098218, 0.10000753190377314, 0.16178226506799684, 0.15388372173551, 0.15981926414236208, 0.315812417108966, 0.31411906514848986, 0.19932685330201597, 0.13449064256242754, 0.16715834708582944, 0.10359398145726792, 0.1753299148797723, 0.6733901581206878, 0.14415328111404027, 0.2599213935338057, 0.19788296852276954, 0.17924437349148814, 0.32195798414724847, 0.19553384196828089, 0.2028305327669584, 0.2156172977607099, 0.16455832336411808, 0.3892743174737621, 0.22735482282805863, 0.41867677661366676, 0.42938795721429035, 0.17371622179167973, 0.11743415774805, 0.07132370575880394, 0.15084441957019623, 0.1425100480370866, 0.14436804219188853, 0.12076051520486164, 0.10095373251263536, 0.1339215225643582, 0.08146757645577354, 0.1421382435229646, 0.1975397954220517, 0.15585596669724572, 0.12310587490834282, 0.12293952250389521, 0.14608473023501312, 0.2848761425252568, 0.07150497442075505, 0.20242532151770654, 0.18296300837990712, 0.2061571380827162, 0.18764796721307855, 0.15275205153445498, 0.25239270614019166, 0.2217922850803181, 0.14328384259148103, 0.5128618094408257, 0.12881666625216928, 0.2359885803069175, 0.1886750110360028, 0.2055679777267601, 0.16987650275063682, 0.13254532230845278, 0.2003590214546091, 0.163314707500782, 0.18464144797552198, 0.18663256963094077, 0.20063618165934066, 0.40797346412015106, 0.11867160723150894, 0.349863468484756, 0.091347617406482, 0.8463178963131375, 0.21409955721461485, 0.1449632514534591, 0.2179929862982874, 0.444480031142119, 0.07734503973887293, 0.1647670370900733, 0.17366793737137498, 0.13361361761550444, 0.09995463296435282, 0.05811597756559304, 0.532826155018016, 0.30131470263019483, 0.1687082705959197, 0.3268651648556295, 0.14571119876134964, 0.1533163981778281, 0.26011542327997506, 0.24280482812361287, 0.26927571347636037, 0.3435176672603621, 0.3456934710608588, 0.1408721645337871, 0.07163586897727771, 0.1109987921175573, 0.11201573241470206, 0.5408279512978934, 0.2691381892365384, 1.11299466306513, 0.32428010838491, 0.20283071625617244, 0.34295442297927337, 0.2949613772498237, 0.226472055606353, 0.2504183803493404, 0.15130129198805603, 0.8688705399505092, 0.47582964437611236, 0.25149856170018514, 0.2496191229533952, 0.13506431178525036, 0.18511580604434624, 0.08254431586038499, 0.11748570723874494, 0.16668195889625717, 0.11425915491013298, 0.39572352191901583, 0.6188976989769788, 0.18686160583539174, 0.13468244727140938, 0.33795972583058115, 0.11624206591875126, 0.12190594069774334, 0.11411463488806342, 0.16956347151104473, 0.2605913942200797, 0.12547833575913253, 0.1476391383579799, 0.1121822265319494, 0.23240375717154732, 0.8031738633863068, 0.08905417028364244, 0.28668936464812733, 0.4988227074739163, 0.4961844719216118, 0.20305805365484283, 0.25414986492889097, 0.5764637238967822, 0.5223955394540514, 0.1342898387225267, 0.10321706954878991, 0.23436730482114726, 0.14645104198744802, 0.20127344311800183, 0.721424280330986, 0.24050902013231398, 0.13054294284701298, 0.33484503498926815, 0.10998056689898172, 0.0830337823572613, 0.17993497080617138, 0.15515478659228632, 0.2607494750775789, 0.15366489240404968, 0.1900721846783461, 0.5496247649817935, 0.4491187090599674, 0.19220449144549007, 0.31173941455044585, 0.1380128997337383, 0.24045082924857972, 0.29305096589601953, 0.33719529123712605, 0.28852118525312936, 0.13842946857438052, 0.3222123322579814, 0.11484117576659861, 0.20397243246331914, 0.22710341138455456, 0.3938865597238139, 0.4659220766482745, 0.31473281589167856, 0.17459869296462446, 0.096729693356482, 0.33293410194435635, 0.11033466525209612, 0.19701604119049812, 0.15683282010827057, 0.0888147375614599, 0.37857973964453945, 0.2004644923951421, 0.2399146421750386, 0.11955196696283657, 0.1375598821244221, 0.13856268598275853, 0.11913640832589342, 0.17556029542188206, 0.18999803405147578, 0.09637732108434041, 1.096103843380056, 0.2273782349256129, 0.10921376932234991, 0.12051137599078092, 0.17790701944590648, 0.11787040640129984, 0.304245530780701, 0.2912479315175639, 0.6002918344461972, 0.3136292575551062, 0.15072504770839085, 0.19002130269713163, 0.3311257625936152, 0.2882064982042488, 0.3661914801190681, 0.38922713047228985, 0.3228046614921044, 0.14420035835604622, 0.2651876899457187, 0.2218754269111724, 0.11460059029715401, 0.1407941944074045, 0.3397277604883666, 0.20515566972585825, 0.7694258758756849, 0.15168445202798933, 0.4757676030439012, 0.22081872781117756, 0.13514440559006, 0.14878153108124043, 0.13712532676644124, 0.10120164779254368, 0.2274469076788109, 0.1370518935193728, 0.1377386675847994, 0.15746003200376757, 0.1473210849732067, 0.0759606529497164, 0.13326608998416778, 0.11914538624998812, 0.605597999615944, 0.12480333356846585, 0.33702047186020095, 0.2694181897179036, 0.19812872217788752, 0.13476469630745525, 0.08443260841426517, 0.09962629129016211, 0.6211290890073019, 0.21299437350199035, 0.09821211423629368, 0.328630577476024, 0.2317256558130658, 0.38140548996358176, 0.15575149239620897, 0.3559521401395985, 0.5155047498741041, 0.17418769253042593, 0.21044321735554866, 0.2677721535937386, 0.11375491967675282, 0.8050798838983766, 0.21120158187309257, 0.09231089316474066, 0.16176427406635016, 0.09275082434030404, 0.30380863275005543, 0.3576174004660146, 0.24249606654757544, 0.10226292747439761, 0.06021019805561413, 0.10056121717521918, 0.1071727436686319, 0.2614125629757019, 0.1464324308402235, 0.24315970786528904, 0.1012381879070349, 0.2559440164729383, 0.19896989603235266, 0.2821263822340693, 0.1625587832481466, 0.17387136091917627, 0.19197219488821624, 0.19027158825337384, 0.20283626168202132, 0.08382029151480042, 0.09093235554117145, 0.08195374977818612, 0.1561875347296397, 0.0981013944024354, 0.4854506366855496, 0.24472510526054783, 0.27384674365703876, 0.14263025369399632, 0.17915798553964185, 0.07084738772848378, 0.18412840838465097, 0.14734626801515688, 0.26493686438334585]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_loss = trainIters(encoder1, attn_decoder1, 16728, 100)\n",
    "print(all_loss)\n",
    "showPlot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xef41XgT1Ov7"
   },
   "outputs": [],
   "source": [
    "sample_ = evaluate(encoder1,attn_decoder1,'jump and look')\n",
    "sample_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezTnS_LEpwl5"
   },
   "outputs": [],
   "source": [
    "a=[2.1251753966013593, 2.027854638452883, 3.5990879822480224, 3.059693857206331, 2.947558057480964, 4.091440056700672, 3.514751228084409, 2.7133286644280035, 3.192299522600676, 2.44098979826055, 2.9186287286542507, 2.2487886873881022, 2.4705661675080077, 2.090898203575748, 2.254079335665513, 1.8820935351885608, 2.146207951434582, 2.078600005119566, 1.496847604680027, 3.0745057779947915, 2.8452856966807714, 2.5844064719412985, 2.7622156698830658, 2.581594910805556, 2.5044883472750885, 1.9934921135947452, 2.549298523974472, 2.856054189553232, 1.9471914443775868, 1.9848427549997965, 1.7455112401783075, 2.1660040820542203, 2.379735743522644, 1.707810012795664, 2.2648841465646603, 2.3957414165678474, 1.7219295542697186, 1.8327857988914218, 2.006806371284253, 2.3014052531695124, 1.9599262068006726, 1.578425352611239, 1.5404843575613842, 1.8063242610703167, 1.9525529479458978, 1.4889641305109933, 1.235554483628701, 1.7768328469140187, 1.7393059474882822, 1.5203928968641491, 2.1144351597746613, 1.6084750128020509, 2.079467740891472, 1.5751382424386495, 1.9525639120271936, 1.7417472722249001, 2.1256899743551734, 1.9647572113741922, 1.9407996701726258, 1.6768864024943149, 1.916727415999474, 1.631958499484592, 1.7818859994769531, 1.9069248830841334, 1.8433211803675296, 1.9282149521904048, 1.553498426104674, 1.4918289154570425, 1.8189344804642826, 1.6708603363291783, 1.6420778003296295, 1.5349307162550745, 1.5025810781151356, 1.562763229480658, 1.7390584819840345, 1.5806901438756298, 1.3942500337793136, 1.384502962542704, 1.2607983985779776, 1.464964332113733, 1.227173854055859, 2.059944771206568, 1.2698712387992273, 1.6567241094584453, 1.7997123201525838, 1.9579010366005871, 1.7730349026739343, 1.5674815559387207, 1.39238235629551, 1.2736448518667536, 1.27005260820742, 1.5327375856908416, 1.9572945611698405, 1.4346885839945962, 1.985425484897645, 1.7274698724994413, 3.0387809040995726, 2.5173951193341297, 1.447582625112704, 2.447567685724615, 1.6356533163279174, 1.5401003157786834, 1.2359025102673156, 1.1314101156424652, 1.669171435273191, 1.6893897906216708, 1.2121462603152058, 2.0395121847959983, 1.6702206449035715, 1.7598946681669734, 1.7628217584658892, 1.6527270233191225, 1.3593632243092204, 1.4128882334782529, 1.2774829150528562, 1.3431005428906968, 1.3215150248264682, 1.469371985178145, 1.301234737206331, 1.1646981974137134, 1.2150821451835192, 1.251123976759858, 1.3505206533420964, 1.0303070044721294, 1.5467802921597495, 1.6737582419708545, 0.946289395682723, 1.4385652831106477, 1.0359780271217307, 1.3999874651690065, 1.44986900705119, 1.0797510534619525, 1.2037705350805212, 1.4065666839790594, 0.7784075221138046, 1.2730747544570957, 1.477841083348146, 1.3827062309029614, 1.1751564861214614, 1.3741202148593918, 1.8516965125245126, 1.0405401994554375, 1.2854762397135109, 1.7531018780629875, 1.2102044253774724, 1.3284922918709376, 1.2435696377713454, 1.2140554103806855, 1.6055988944586432, 1.0146971689039659, 1.371391884653412, 1.3490642975116596, 1.2190251152603715, 1.6194179114403084, 1.3760951464364672, 1.4568997642441923, 1.7248545361629894, 1.3636726837431656, 0.9236172592278683, 1.50074640424628, 1.123097399331473, 1.163681669634314, 1.285932528259408, 1.3079006419203798, 1.01903559366862, 1.5068225239813147, 0.9997935816287498, 1.0402463964806559, 1.3679855473836262, 1.0912390796220297, 1.3440152625635522, 0.8306896007162893, 1.0405373634830597, 1.177933540646992, 1.0057934884671813, 1.130434942942091, 1.116748049853843, 1.235207146345967, 1.2583230471873024, 1.2204294331931314, 0.9246789156339685, 1.4419188586148348, 1.0357182287895816, 1.3287770756399049, 1.0507599280972788, 1.2987086895983335, 1.3824914152801235, 1.473289464522099, 1.1729368073865252, 1.2715678230587975, 1.0683165460244204, 1.2505033470335463, 1.3846185282298498, 1.0268910418981794, 1.2547269935731764, 1.07378856196548, 1.18053015110376, 1.5587681875822745, 2.0206319297576605, 1.3552379979080569, 1.039293288706538, 1.0115304191907246, 0.944309271298922, 1.4331790288049515, 1.1312495474691517, 1.1595714701925004, 1.4171073312090154, 0.9665982299433761, 1.0663202057839696, 1.1213785113389658, 1.4728091996985597, 1.0862739523337306, 1.6337928672063917, 0.9470005655172253, 1.1021473229177492, 1.081369357881092, 0.8197647034433659, 1.1542925223437224, 0.8731663229643416, 1.3931097333748024, 1.0721013722758, 0.6638658273170841, 0.7608136823908023, 0.8756716049399842, 0.9497727950414022, 0.9113243287906311, 0.6536943951720904, 1.1028917595587253, 0.7232882201971442, 1.9788356819848896, 1.0709099416567511, 0.9064628323179562, 1.277446445423736, 1.169979394502915, 1.2359370218300005, 1.620533264024246, 0.956543171591758, 1.148019897479729, 1.3857606809567184, 0.7975829105150132, 1.011940628544131, 1.0973080847599939, 1.3343619569633844, 1.2695276165008544, 1.490174523685575, 1.1723635114930633, 1.191151231877944, 1.3166456337270325, 1.2477420126271044, 0.9384031500823227, 1.0630247668025614, 0.8437846893551704, 1.0984120939052424, 1.3930449228748636, 1.2089300200388777, 1.2498246079261857, 1.2075541076593685, 1.0894324607464856, 0.8763487279771637, 1.1395457665903586, 1.2270411385430229, 0.7477201032114553, 1.0498538749013038, 0.9500333353197494, 1.8235682926553978, 1.020117216867114, 1.0109558179439369, 1.066394834146319, 0.7030371127722931, 0.9769103112674895, 1.1283391142434485, 1.4390177526687071, 1.0020719799926836, 1.2585976156538539, 0.9921052677996572, 0.7773431393087429, 1.0781354230244955, 1.2017782494888976, 0.936148586974582, 1.3163473112854076, 0.8352852073808247, 1.0571052941235897, 1.2684900648333044, 1.069444659189258, 1.0610592774855785, 0.929880727973409, 0.9099851589574441, 1.2260412602872401, 1.1737495349940947, 0.9593568386501736, 0.7784636058297045, 0.8206081571925583, 0.7068588048222918, 0.8935030289507105, 1.4027244608025802, 1.2639941596475923, 1.1542730494236029, 1.2086699317750478, 0.8348467473968555, 0.8526446806570016, 1.132937777939663, 0.981364525310577, 0.8488037032478232, 0.8082165195867207, 0.9421450939701405, 1.005036505226396, 0.810763152764768, 1.0867200373415742, 0.6229021349407378, 1.2368641240150313, 0.727663170991056, 1.1242522412895137, 0.7670168388255529, 0.9928742296796036, 0.6967357532881991, 1.0062730704821072, 0.8239609741029283, 0.7031881427764892, 1.0950281324405329, 1.075601984424032, 0.9282336793976389, 1.5134458773063888, 0.795662925219295, 1.0258560990530345, 1.1143186020128657, 1.1801101315210736, 0.9433941795707556, 1.26819444647363, 1.3407661973036729, 1.1198829073754568, 0.8457191195539249, 1.180854066292644, 1.1531814137283636, 1.1648807043953961, 1.0286502016915215, 1.1595870172312122, 1.0361969674744222, 0.8506947462899344, 0.6992407435462588, 1.1656515824659235, 1.3379523553526105, 0.7058565009207952, 0.8642531095770778, 1.3745509320800042, 1.283286084991503, 0.9645737229342043, 0.7989970928546339, 1.1015454989618583, 1.2269710430954444, 0.7524391084226787, 0.9345125085109529, 1.2459527940336492, 1.0734503658699903, 1.1090366570154826, 1.0516896006452452, 0.7769716620204424, 1.444351754771756, 1.1169707696920539, 1.14271116050646, 1.1288732409143314, 0.9602076265758728, 1.4492592748006183, 1.2309328014937488, 1.2948851961952248, 1.0544260406677244, 1.3261698976244243, 1.0542198687605029, 0.9383089107011026, 1.3500182989112333, 0.847771912188757, 0.8676343637042574, 1.2084871930103933, 1.0037755485681388, 1.0672850493101105, 0.7043727650497899, 1.5683147198209912, 0.962906802330006, 1.2539297547562576, 1.0431086326256778, 0.9250960575937498, 0.8895138947661225, 0.6565156140460617, 1.0652929042612924, 0.8101438144988565, 0.8801803284256359, 0.8801974001225148, 0.8505602401342147, 0.7257154620321173, 1.143936114166722, 1.065797685469636, 0.8062292965993624, 1.0194650195003334, 0.9520695918497413, 1.0596875876216387, 0.7159328239503162, 0.9983160234752454, 0.9149765802358653, 0.6648695229349115, 1.235694633317891, 1.5639467354038161, 1.1960621364206712, 1.1432068008517628, 1.3478055161521547, 0.9884902908013655, 1.2226030487002748, 0.9942447645764003, 1.1914888869683717, 0.8272956741931392, 1.0418065632783677, 1.1585627398857845, 0.9676530706336542, 1.0147959259571808, 1.4098882901304663, 1.096390549202105, 1.3662873644353914, 1.1301996832809516, 0.966756807901733, 1.119825723171234, 1.1839083082629263, 0.7156997099569594, 1.0840637596980707, 0.9081381092723617, 1.2332831362956027, 0.8635443852176229, 0.9676266319554931, 0.9752177435720067, 0.9394080497207602, 1.6000765725338095, 0.6531959821499835, 0.9244959753502396, 0.6777262022012999, 0.5370665478327917, 1.225406064036843, 1.1512519091368505, 1.6116712481946938, 1.3170152066304133, 1.3025808563154615, 0.9071073319965149, 1.1966357741430869, 0.8712368484039033, 1.334208262833682, 1.094364621239206, 1.2184228307550602, 0.8393605236943349, 1.0539800875527519, 0.9396860681910093, 1.1550578951377517, 0.9154614442142958, 0.8287899378574256, 0.5188655555884731, 0.9509830677114268, 1.1735293029549811, 0.8554405316254847, 0.9413414239473346, 1.0265451150765434, 1.2635801465830572, 1.0294035477301087, 0.7122034191381429, 1.2499203449817053, 0.92634046792984, 1.4178452657212524, 0.8693712498591497, 0.7179570541367016, 1.0023743465147337, 1.0739533848352358, 0.9419040649021184, 0.7440350416686734, 0.7981041649079779, 0.712590941676387, 0.7788200637777016, 0.8374395587227562, 0.8973008732431034, 0.6788433678961006, 1.3028514766118238, 0.8090529032133563, 0.8592111362229046, 1.234488157640575, 1.295908397783998, 1.1927557195844, 1.298324570864711, 1.0388599318702183, 1.1208730034601122, 1.001898623318334, 0.8756446615385108, 0.9910296564516814, 0.7553458229378447, 1.0021535585847918, 0.9292229779561361, 1.0222769962065417, 1.335017123208766, 0.7286988888322506, 1.7538550259990078, 1.4846155305892703, 1.7240071359201639, 0.9806687330358494, 1.3431059537110506, 1.8197389570871991, 0.7236832640181372, 0.6791020354976902, 1.0884168498782425, 0.7313629569678471, 1.3281652448301735, 1.091363000869751, 0.8306651741793114, 0.8723662323384852, 0.72990275440453, 0.8865176067062244, 0.7444418886824922, 1.0843163287297992, 0.9698189325723631, 0.4718295851500902, 1.442121836485373, 0.8380274380273868, 1.0680204108909324, 1.1616459273082609, 0.8463260853925005, 0.9229243505251157, 1.2748695549014315, 0.8514246252362023, 1.0258475107431595, 1.1469782317797963, 0.9962917565439344, 0.952136953655893, 1.1358884071545519, 1.1404491377599313, 0.6751266254004158, 1.10972229253171, 1.1341479782291226, 0.8279084899208762, 1.2708897205664478, 0.9081643680289939, 0.5193638966521439, 1.0623218013394264, 1.0210497974881938, 0.8872275032384744, 0.6808177993729798, 0.6434860989927567, 0.8797626291937826, 0.9725605874206081, 1.067739980787664, 1.4729755812188237, 1.2401630089279778, 0.8052442045957651, 1.203556173723101, 1.1612281415530064, 1.0243537564933438, 1.024251266070457, 0.6352158099218131, 1.115528605191255, 0.9441398442922246, 1.1536129475949886, 1.1393057107052087, 1.628507467797705, 0.6886345933309832, 1.2679117803724984, 1.0515726422781895, 0.8287403764073155, 0.6342599462113602, 0.932809480378122, 0.6661080692727422, 0.9868927273267062, 1.2332623050087377, 0.9313651183157254, 1.0269822044978067, 0.6333044392301904, 0.8471134117671422, 0.9089286505203426, 0.8774042199161531, 0.8840052145867972, 0.8559776434182249, 1.0852201794024399, 0.7947939298765867, 0.6694652550179999, 0.736566728608221, 1.3197637465061285, 0.8809932745896376, 1.1525374174954597, 0.9332315681697605, 0.9313083170688513, 1.1421556604439431, 0.9874183446710759, 0.8980627691384517, 0.6445842430656599, 1.3517126624400801, 0.6999756903173115, 0.9222167537675237, 1.1008163766884214, 0.9607713215883361, 1.0556944122669594, 0.9171261825258771, 1.2500223393392083, 1.1390166356050033, 1.3349022312816683, 0.7879918804589441, 1.0141633591372576, 0.9934687899668739, 0.5542835992768258, 1.5992881258451022, 0.5913336431849133, 0.568014734071081, 1.112911617054659, 0.8681902510663934, 0.9631904003851128, 1.2122791703129228, 0.8385064093271891, 1.1819789825786244, 1.411030375348058, 0.7594665091022167, 1.2056990222273203, 0.8764504259920736, 0.6598867752403214, 0.8367835577432212, 0.8771850925021702, 0.9024076029571628, 0.8943222080610722, 1.1405033611010098, 0.5623552084965017, 1.0723697952978692, 1.1246217238121214, 0.7114191671513043, 0.5204233876730818, 0.948519076485919, 0.5352064011709129, 0.6335246796472337, 0.8368717525105703, 0.6592418323183655, 0.894171289742458, 0.7201108136346164, 0.8891335559601705, 0.9893129222995632, 1.1175930009321733, 0.9778167494971713, 0.8374865812501348, 0.8313747578615333, 0.8453006412449687, 0.5017893192667539, 0.7713695468863098, 0.71410222902463, 0.820969775620993, 0.850385163531584, 1.016143459562653, 1.1475594238923343, 0.8221504444966072, 1.4204274180100775, 0.3420732009339421, 0.834454295771692, 0.9255732708245491, 0.5191370019823799, 1.375297378076474, 0.682057480132971, 0.574102467986011, 0.6763878311581081, 0.8411669773533461, 0.7144324583534414, 0.6009791553529918, 0.8089146556854248, 1.2090087765601185, 0.7445501828434491, 0.8638210197891851, 1.3821875034059796, 1.07428461066179, 1.0062221583783015, 1.0470133309179515, 1.254341766142742, 1.0779902718180703, 1.105147190694209, 0.919480976520046, 0.9707758606387996, 0.844223376724111, 1.0642296769402244, 0.8490860628703285, 0.8915288709665272, 1.168384435084903, 0.9876394086895566, 0.996106813141812, 0.8581947168347916, 1.550252532999769, 0.8679443221684868, 0.7394968604627377, 1.548320883097666, 0.7594653445722419, 1.0658577690194377, 1.0441404545706425, 1.2427123378702505, 0.920202593192076, 0.8027591673044057, 0.9637952884664245, 0.6955644276419706, 0.6865524454828187, 1.0685288337928749, 0.7707949365383733, 0.8888750148129153, 1.239739744035821, 1.0739809933627726, 0.9153174365878302, 0.8972030115917258, 1.0494699285143898, 0.9153148446883355, 0.7035222334980433, 0.7555612083001437, 1.0301852754702874, 0.7782320233325024, 1.1655428781294435, 0.9622270870918544, 1.4658892115102726, 0.810345130390697, 1.3287604137703224, 1.6885401835808387, 0.9479900377557191, 1.340117211691983, 0.8337382085025078, 0.6282838030585591, 0.9250756130161342, 1.1393794435772278, 0.8820886631278725, 0.8249492901551305, 0.6495157926320416, 0.9573785184257897, 0.6158797572509587, 1.0840246831803095, 0.6547318984601069, 1.2761217157045999, 0.7915468001491809, 1.178215077997564, 1.1841708366513477, 1.3304587220272301, 0.8369963568833974, 1.3735212299737247, 0.9815866901102849, 0.755998068621188, 0.9180187839865477, 0.9001642703089727, 1.0235747003555298, 0.9229275291715717, 1.032097413656595, 0.7715049777712141, 0.6357982099385909, 0.35007366459300343, 1.1453890237802196, 1.0179734258558235, 0.700397708199241, 0.9769784237567644, 0.7421187689143183, 0.7108710963600844, 0.8551056034553983, 0.5928692303273747, 0.9118030685142756, 0.44422624053917437, 0.7850231103296881, 1.258366096786903, 0.6911825507704974, 0.9473838130526377, 0.7721281902098551, 0.822283537216, 0.7866182087475775, 0.8089998785011676, 0.7675743638139305, 0.7905522753308704, 1.2566328167227208, 0.9162113887544663, 1.447924183332003, 0.9599842130220854, 0.9618124189297286, 1.1665462534651796, 0.7149868279679668, 0.8921596566196024, 0.9340509914031472, 1.2004065941382147, 0.6458146342174192, 0.8843452069609639, 0.7360017748573041, 1.127981218356712, 1.3901190905837537, 0.7471651697626791, 0.9656657450861161, 0.8157516645277711, 1.2648279614954236, 0.87832049477737, 0.8572762894086894, 0.6917408124138327, 1.0096062372019001, 1.0143531504497734, 0.7880865606747773, 0.8480105642394309, 0.8518996750499064, 0.671729764170691, 0.9206297460969513, 0.9300933849261204, 0.575407053051522, 1.0453211376883769, 1.0185275488762078, 0.8767921851572229, 0.7654369384082245, 0.7082969614929023, 0.6323294615134214, 0.6586337243667757, 0.9954883851676153, 1.478715851470424, 1.224583768438277, 1.0939016471065859, 1.0250360700819228, 0.8813096151484585, 0.8259960667699829, 0.9602814889269091, 0.9621939538319906, 0.8758580222803181, 0.8487440712911599, 0.8241466525246552, 1.0120942696098707, 0.9443048534963884, 1.745136268277824, 0.9535688177257977, 0.985510414176517, 1.251603403548347, 1.138682378667182, 0.8094324985574272, 0.5494434550058591, 0.8063043252248614, 0.6781806670662585, 0.5349798906119385, 0.689720938758067, 0.4629476441974528, 0.795505764649211, 0.825419184301986, 0.5172750593752762, 0.9160785060615311, 0.9970676109396667, 0.946712561692947, 0.9846982312924932, 1.2036961258041274, 0.9707851480964335, 1.1812475555159578, 0.9009485517165838, 0.857643473974038, 0.7192247591798738, 1.090468683593078, 0.836612162806771, 0.7638061582347452, 0.9736627401357054, 1.4275010264301984, 0.7505533319279768, 0.8973251867015459, 0.9756496136902115, 0.7210843354483362, 0.7325109497872122, 1.0048960785367946, 0.874977302298235, 1.0926381696734513, 0.6855939859897879, 0.9147866722694914, 0.80488059234898, 1.0946063064651306, 1.2853055378306997, 0.8512291600557367, 0.703708241440001, 0.6300823865398284, 0.664565099173901, 0.9821315884410886, 0.7696326828264928, 0.7500983106961814, 1.0471381178356354, 0.7279069164545849, 0.7150891409668267, 0.6844507081599566, 0.6436283566795962, 1.0104829062062197, 0.6539926838874817, 0.792017578290811, 0.608116285632355, 0.6448230957554388, 1.2709845733642577, 0.8766735905867357, 0.6486732589353945, 0.9413995415317731, 1.0994623785307913, 0.9455099134418476, 0.971931430089434, 0.7685204887830598, 0.7044504470488543, 0.6787962685209332, 1.0806698952557334, 0.6431933297849527, 1.4112257698527253, 0.8354799361421603, 1.1264970682895545, 0.6521941158559534, 0.8307289477146655, 1.0214330926306046, 0.9259741836699886, 1.7478318630517253, 0.7689803775559124, 0.9534418642667581, 1.0026428330512274, 1.0650031709545371, 1.1872572858516985, 1.8775010270783394, 0.9556826636507795, 0.6039030941895076, 1.4287962912322283, 0.7793129405219517, 1.1004992164480503, 0.6481872310060444, 1.47571464850797, 1.0640573101532689, 1.1185197156828193, 0.7859458550009082, 0.6541407139692019, 1.0503174453951938, 0.6282556755718215, 0.8513138402947773, 0.9576807082899876, 1.08122479303275, 0.7164023929316439, 0.8238327981890118, 1.0033840460026222, 0.9951742377036658, 0.7940973328580759, 1.2572219137753842, 0.8058052155950775, 0.8393064837267964, 1.2706674910892746, 0.9254128291339458, 1.1462184972233243, 0.8944909654263826, 0.8386920410704137, 1.0933999621678911, 0.8475333716016138, 0.7790879687507114, 0.9263614711401681, 0.7440792852730098, 0.8453918467346133, 1.2200193750565873, 0.7982636644652504, 0.6347684229562109, 0.88725870552226, 0.7559760514138725, 0.968305459789846, 0.4559733581814365, 1.1028813820240269, 0.8264737776049611, 0.5489174205296999, 0.9763231071447714, 0.9184229043813852, 0.7113829613993408, 1.346112048754736, 1.117582299549312, 0.6255000942445053, 1.1246747023718697, 1.1888648824726706, 0.8777755286371931, 0.6182277160883716, 1.0124935887235424, 0.7213419406063907, 0.8460903337127285, 0.7380740287960921, 1.3879315605252494, 1.0570267484168046, 0.838950379839266, 0.7324608331508738, 0.8850134614660682, 0.7493473740261287, 0.6558502697489297, 0.8487136064669787, 0.5942159737638878, 0.7391348735385116, 0.9678965341056877, 0.7071838872416036, 0.7638629311606998, 0.8246001759351185, 1.2535664769735504, 0.8561183601918847, 0.8231132261780487, 0.7234250952886498, 0.7891532789132535, 0.7967372845690024, 0.9275067470276304, 0.8525237341786758, 0.6872559735456458, 1.0269005484330027, 0.783314283811129, 0.8963418136433484, 1.1556351169463128, 1.2896650232772622, 1.259861983073423, 1.462694634786326, 0.9251390812583784, 0.6935245922148398, 1.264247677061293, 0.695416707393927, 1.4411222364536669, 0.8216145587080412, 1.4953089476970818, 0.8711963891859178, 0.962231953938802, 0.6054379417782738, 0.8864801124092583, 0.7161917984298778, 0.8873151935937204, 1.3004504706636184, 1.0130318587270641, 1.2111459292457494, 0.6421609965047159, 1.105166042212284, 0.8034919202109396, 0.9644532925409722, 0.5253921670459565, 0.5513582639688316, 0.5193605249969118, 1.2817598069155658, 0.6929769784123839, 0.7875352546051666, 0.7401684524379132, 1.4793330152829487, 0.8335998875127834, 0.5915979749093324, 0.8710033227380458, 0.8209247844047795, 0.7400508372533411, 1.0253261737334423, 0.8202632038409894, 1.0040342657923538, 1.0398812962239519, 0.6562563576884084, 0.787387192443371, 0.7798777750560215, 0.5215361457925287, 0.870147288884872, 0.7271084177024048, 0.8513424110153247, 1.3745984742580315, 0.7326287539840658, 0.693475461902475, 0.8052677212198223, 0.573883540384803, 0.9351731298289774, 0.680383030815401, 0.5454849938833087, 0.7441659460756294, 0.7179036909902198, 0.8379756685670026, 0.8146457098517754, 0.7099251720407508, 0.435640314334724, 1.3584893983507915, 1.1162327653322464, 0.8638303526112898, 1.024485578715887, 0.45581688076319404, 0.5347859595203142, 0.695830934536031, 0.9053785821719046, 0.7334959992082604, 0.68337178560012, 0.8833818161415248, 1.596209323341977, 0.5946742111263854, 1.1972870060643663, 0.9919872910544069, 0.7257829950742676, 0.8289971136324332, 0.8166158073683949, 1.042549423187498, 0.6603359462517114, 0.6361363231166127, 1.1224622317722865, 0.6794022418265427, 0.5884015685506858, 0.5904638963281392, 0.9616363860589539, 0.8525055609323843, 0.7905537150503156, 0.9321244564304104, 0.8276694215909399, 0.9602978949420414, 0.7796599313739093, 0.6882211339971089, 1.0796312777201336, 0.5323638374791083, 0.8004016478308316, 0.672257078293922, 0.6252289248503927, 0.7049298263781725, 0.7957172944810657, 0.5782986681297343, 1.2098325061449313, 0.7099425111364583, 1.2548438388656569, 0.7017309688531905, 0.6725400366504803, 0.5734325144427392, 0.9230857002516807, 0.7253572023523647, 0.6115432629778941, 0.6881560972457045, 0.7061779721577963, 0.8340154261911389, 0.5951377167512766, 0.4213100160871234, 0.7046421714056106, 0.531310479929953, 0.6366752165141123, 0.6055385409344684, 0.6230879282572912, 0.6167027979341458, 0.7886651193527948, 0.7616389109579739, 0.643053323634817, 0.7038196207228161, 0.8355762881445912, 0.5958485055692269, 0.672795671704355, 0.6308362897829947, 0.7557930684972692, 0.6246868762625124, 0.6515007240444418, 0.7879171192078364, 0.7412496323496061, 0.6010312508384963, 0.6963745147753985, 0.5825130689793885, 0.6463651911756388, 0.6787185410508736, 0.5645242421356313, 0.5421566002566617, 0.7158355541736261, 1.1313384251780323, 0.7797445290316967, 0.7440026183275751, 0.6219471240757174, 0.5378467057023572, 0.66086126698388, 0.9666678764007905, 1.0239707288393791, 0.4570338818691206, 0.6929208618182586, 0.8577915372615965, 0.876612930050144, 0.7901088588756064, 0.7151120670025166, 0.7573163815135948, 0.6836488387077642, 0.8911598803478601, 0.6347981700345807, 0.6618129442026326, 1.2273681817359918, 1.0164806908859318, 0.6379354380513286, 0.8842499492606338, 1.3433664944734351, 1.0033478051110514, 1.2451317119598388, 0.80280852602637, 0.9739538645847535, 0.6105446227494772, 0.905096429208791, 0.7300135717728852, 1.0898359278009904, 0.5923413546819187, 0.5843252084088227, 0.6164688483318249, 0.5888870930774903, 0.9041363158978915, 0.580456930971765, 0.8610790630820748, 0.614813879263782, 0.9112913113373977, 0.6587780755906267, 0.5636321853591595, 0.8496735605799166, 0.754086304846264, 0.9247760961206339, 0.6170593774438302, 0.6238310026577707, 0.7917133253341435, 1.4043251487186976, 0.6348801815023359, 0.6575351750221781, 0.45709543765935684, 1.3156718611044569, 0.9419911181014047, 0.799931048908619, 1.183397927612224, 0.9268394921344033, 0.9052357171004985, 1.062524067721383, 1.125277406568411, 0.7974386120493201, 0.9624877227298798, 0.5512202185524835, 0.772958153811368, 0.8730087125662601, 0.7458399929594897, 0.5714415838180319, 0.8747361771207445, 0.8538513851265425, 0.9308374879161466, 1.0312318952716126, 0.7224308014389302, 0.5325287885599204, 0.8394487987771502, 0.48295424189497094, 0.7463352068994279, 0.6422656922640388, 0.5991400259103024, 0.6699792623519898, 0.6846882933846657, 0.5077332674242147, 0.5831504275441652, 0.719925530462554, 0.6000482857882321, 1.0666359210736824, 0.6176806827397046, 0.6944439575631306, 0.7271226596832276, 0.5767471427678771, 0.6665440779479417, 0.7798830893132594, 0.8216725731948438, 0.7576877893720354, 0.5388740544676953, 0.6674116015667286, 0.31577864640678077, 0.4717155866636687, 1.437569729465622, 0.7126808533883938, 0.7089108244578044, 0.8181510736939073, 0.5919113375010262, 0.628130533417494, 0.6197040145737784, 0.8677811235215286, 0.9665273389258943, 0.7019059009491161, 0.7322189384518247, 0.536275407698246, 0.586917886654026, 0.39588543377102786, 0.5899328608583664, 0.8311926873917599, 0.8173487405928354, 0.5563974046707154, 0.6972974580651926, 0.8094216861477145, 1.1556745629840428, 1.1103872880714083, 0.5683022222763452, 1.0347159898160683, 0.7655977959103055, 0.6498540301101228, 0.9316361184387081, 0.9778564363259535, 0.6305086458876437, 1.002270929629986, 0.75941462573551, 0.8106741627057394, 0.6691614894722149, 0.9455968059605849, 0.9754152627869365, 0.5018362113410673, 0.5832397493965182, 0.791679816965073, 0.8409244313384547, 0.5841146588624271, 0.7880002204454863, 0.5932561661540177, 1.6337285497972183, 0.7154249227680107, 0.9949935488630193, 1.1780545204350736, 0.7282226205788009, 1.0459199438233402, 0.6882930566738178, 0.8365577995122134, 0.8790412336315543, 0.667594473039261, 0.9204004294388778, 1.0287958869550642, 0.8356278513211958, 0.5975985069866174, 0.7285283730733637, 0.43116062241371234, 0.7232989207207905, 0.5422379001980138, 0.7584732709525008, 0.7666049812620497, 0.8201544342339753, 0.6461612491368651, 0.6950147837319749, 0.9895483381860144, 1.1902399931092194, 0.9890311458958774, 0.8176790155638255, 1.0750016386271555, 0.7224615436955792, 1.0378469571962463, 0.7409112698008345, 0.7625781922204534, 0.7450444741476149, 0.7154785390514946, 0.672054745595306, 0.5917453299035559, 0.8109586923064311, 0.5947003180793577, 0.8477823843306554, 0.5983637566841242, 0.9723951781237566, 1.288781268113143, 0.719822183233319, 0.42307284852674787, 0.7724553642291594, 0.6057339514735225, 1.2786632175172101, 1.0004941806926593, 1.1165319486196155, 0.9037628382791727, 1.1866027038449447, 1.2385275370617634, 1.2894017430830322, 0.5591827998424372, 0.8577862847753955, 0.970322733760082, 1.0927522720328409, 0.8941569113352943, 1.4053540716955673, 0.9596153445058055, 0.6902937363103918, 0.7717141502778171, 0.7796277565393079, 1.1782256406186384, 0.6358721921970318, 1.463084186721634, 0.9253808588622718, 0.7876074981631231, 0.9844816290994615, 0.858295359384446, 0.677613742419058, 0.3885059015207123, 0.9256006854891436, 0.5604220416082518, 0.9200969307135475, 0.6213003773316397, 0.6077728949028216, 0.9112646770387546, 0.7733752802868822, 0.358765822190318, 1.0160327140076462, 0.7460010928277034, 0.4586459518614269, 0.5569271928923472, 0.64866645971934, 0.9433882452648106, 0.5079769712781149, 0.7531444003429318, 0.8766591073094375, 1.008341125205711, 0.6057417853995052, 1.0091197141648447, 0.5638861553685194, 0.5166247166665258, 0.48456322199967916, 0.7341886923822241, 1.0228717952304416, 0.5720389784899699, 0.45679576026068797, 0.6196405237847633, 0.5279186024183394, 0.5536679951060902, 0.8461518572204876, 0.8261167089501289, 1.112196070019978, 0.7917681641631074, 0.9799807692330982, 0.505362732737672, 0.7462126738800962, 0.4063915833114464, 0.6734919476795254, 0.9445882888067336, 0.46972598277949534, 0.6702830796691305, 0.8540281903379858, 0.8716643682325849, 0.8182593783258876, 1.1509398067551024, 0.3411535459614311, 0.8762390863723543, 0.7082794103168306, 1.0240228257860458, 0.632483969097595, 0.6651756835706306, 1.0213788161466728, 0.7533662031199644, 1.1587432644306084, 1.0743279897425164, 0.9636071441496684, 0.6032103383785673, 0.8365663840628079, 0.9790349216503774, 0.6153362932967564, 0.5885428327133131, 0.463796624479316, 0.5019982364098576, 0.6669098085827297, 0.6431078944728645, 0.8248662138366386, 0.5457043925905465, 0.4388461867473278, 0.7746415657467313, 0.7322698893913856, 0.5464056401924557, 0.72468667246779, 0.9764631791548295, 0.7021969054119228, 0.777685738851065, 0.9608663310223852, 0.5092500333593349, 0.5325170654490374, 0.9874082823160881, 1.0271516713569084, 0.7029486602427912, 0.6120042726728652, 0.46963312976444554, 0.5978843237224378, 0.892703780034371, 0.9916513181734488, 0.74603453994733, 1.0957451246482068, 0.8064684902093349, 0.9023928822789873, 0.4259808344897159, 0.6620542719175777, 0.40663414209347054, 0.8412385455622413, 0.603358059072853, 0.5762317465285587, 0.6799157834586563, 0.6370713893306694, 0.6724542408852285, 0.3726581441192113, 0.5701480048497519, 0.8503697331326915, 0.6711427643840173, 0.8490390168071714, 0.4510785537342448, 0.5579876753262111, 0.7722587831802876, 0.6659077582917772, 0.7057515997563262, 0.5840164974300126, 0.8394729412788953, 0.8334427520947918, 0.8675202584685258, 0.6461988416860146, 0.4542527185456577, 0.6249490240760158, 0.8242493097102586, 0.4652505423807881, 0.7156187522144343, 1.0032768020932636, 0.40565198762818017, 0.6797478033750848, 0.8047868727612238, 1.178181275327405, 0.4759713504517, 0.8513218104564648, 0.48404943120162197, 1.1105933172919276, 0.8016197147532406, 0.7388069602345411, 0.3955332608275361, 0.6966987646805061, 0.6869938227624605, 0.9659979384680959, 0.798384111151736, 0.4284301740422619, 0.8181804110071973, 0.5606100575107741, 0.5403414135405181, 0.8973362837729841, 0.8985714097017189, 1.1288822901845705, 0.7223973972148652, 0.4820396099610059, 0.998557771955108, 0.45263363535841583, 1.1695689220704895, 0.6502386685573693, 0.6073916199919465, 0.6650817785198841, 0.592368149176622, 0.8482737442038772, 0.8023252246355769, 0.9001143769611882, 0.997675519519382, 0.7609472592671712, 0.46752808872487267, 0.7853427331375353, 0.3904988214912077, 1.1601451945201657, 0.7427284032285394, 1.1592662309588808, 1.2471724439836334, 0.5432224644419483, 0.8226839020720915, 0.5680674839567079, 1.16854297941927, 0.5651475423290616, 0.5870493610742079, 0.6903412601648471, 0.810468893918124, 1.5129837390151863, 1.088712978008643, 0.7701016713548684, 0.7453813556349758, 0.6674274717136751, 1.0288855871120532, 0.5175049267490763, 0.789578574833117, 0.8346017558146744, 0.9277171498221474, 0.6497454666249032, 0.8261293815314698, 0.5323184609107952, 0.8633834816928451, 0.8159151957795612, 1.1623163986319942, 0.9416343204902882, 1.2672180984403703, 0.40634805043231265, 0.4332410559438539, 0.6279616347445217, 0.9552360236675692, 0.7239262923138579, 0.7286836994689774, 0.6590689499484856, 0.6472008336385091, 0.47558189181673294, 0.5328396853085222, 0.597307508093679, 0.8216748934896213, 1.0145032469633712, 0.8789278652175158, 0.6057378270071323, 0.6100863982120276, 0.5623651312646412, 0.8817769968336908, 0.9972757846182517, 0.46379925066757677, 0.5572810770262555, 0.768678487377277, 0.7747895704375373, 0.6632027405681031, 1.0672834140193483, 0.47391770077386075, 0.8981535437750438, 0.6967714912172348, 0.5939577128230662, 0.585692217859328, 1.0219946537417965, 1.004564505985805, 0.6126872067702445, 0.7462150627820469, 0.6298570279240564, 0.8890606529179539, 0.6182062541569149, 0.5568258661980316, 0.8674783706665039, 0.6773301668922223, 0.7973340118403758, 0.906433311380366, 1.1938090236017291, 0.939130037630758, 0.8533748081402901, 0.5704697973998912, 0.5074258854275657, 0.5356337050602606, 0.6934822062607511, 0.8023588193648044, 1.072300270233082, 0.6808180486454682, 0.6282386419209235, 0.8673534036405686, 0.5701000396169797, 0.9339125152729473, 0.8896970173307374, 0.7084748980618906, 0.7796447840881793, 0.5528789780815385, 0.4911797255386115, 0.4229329290844145, 0.8965776361485638, 0.25170113321126525, 0.7880393977858062, 0.5539753594069645, 0.6999422562756792, 0.47677003648546, 0.7750298901611541, 0.7877638128068712, 0.7578399784867609, 0.9596337725377232, 1.0505508364081717, 0.7080922398126301, 0.625666715718117, 0.8341371813877302, 1.4502688446597778, 0.8724207076839372, 0.8061311509670356, 0.5490570612792129, 1.028637559664085, 0.6135606591389131, 0.5955578827887082, 0.8740443793210118, 0.494479385637476, 0.7165629108980263, 0.5013303587272222, 0.5905612367638641, 0.7944320038010605, 0.6915089214765109, 0.875007811046782, 0.7095201585231684, 0.29814229079814025, 0.5345379238388166, 0.5844540292566472, 0.5993280870399196, 0.6811624215096652, 0.5528311874079361, 0.5346384364592826, 0.6497039286879323, 1.1067391054943558, 0.5296157707305148, 0.7495939278462597, 0.8064577349301043, 0.5051576525270536, 1.2524214628363857, 0.6807097842744758, 0.49850056942425114, 0.9702982597275385, 0.5042215179084911, 1.1113686503068927, 0.7022756375723244, 0.6753714491437365, 0.6431215239376883, 0.5916908979415894, 0.4732114619842062, 0.5897289861191332, 0.44801848042056935, 0.9992485376504752, 0.4812714591968022, 0.6768150024414062, 0.5653390424117817, 0.6896541426049754, 0.5727701498364068, 0.7307400602349488, 0.5516057324458847, 0.7664492426352068, 0.37528521926140745, 0.43715014856787127, 0.31435159560740233, 0.7688570357941009, 0.5883440326435775, 0.5483233429933811, 0.6812141995251957, 0.5823211529554226, 0.43435265058645134, 0.4049763056711063, 0.9295532215055118, 0.42488830532898775, 0.6644000446385351, 0.7172442574934527, 0.6184325848108444, 0.8713842550338124, 1.072679069307115, 0.7197546347865351, 0.6574084122975667, 0.8245096114277001, 0.49778121643460416, 0.8408547069912864, 0.647159847846398, 0.549052712291178, 0.8866876194312736, 0.4002959124286048, 0.4150778413408953, 0.7765465850727533, 0.4689606987513028, 0.44457663862396685, 0.6794230772898746, 0.6928414908322421, 0.5788486554438499, 0.7485406950999625, 0.4566231331726847, 0.558191932797199, 0.9216703887135214, 0.36432764852135197, 0.6978528628969565, 0.4080344636994179, 0.35013771983776015, 0.3683221918163878, 0.6496201202115877, 0.41415606049633535, 0.6256011159047657, 0.357101762116134, 0.4643403790500543, 0.2813211062063958, 0.5301343338898938, 0.543272232046031, 0.7737041766486243, 0.7571868553058734, 0.7425336985599189, 0.5154186423681872, 0.931570018193419, 0.5197075353209696, 0.661872740741416, 0.5118551857451088, 0.51963446875355, 0.5025793404431377, 0.4320367812332296, 0.38905164355290134, 0.48255132074709295, 0.49031984737905476, 0.810108938861838, 0.4424794170263504, 0.5957916217735779, 0.502187183826321, 0.5894018055410946, 0.50305528651227, 0.5314148525848202, 0.6375633818580783, 0.4031680060886755, 1.1267635988087625, 0.6248830058357933, 0.4625879871205661, 0.6577716327614498, 0.3115806152237264, 0.685130005528391, 0.6282151277608281, 0.6673350967168366, 0.2409762923691797, 0.395834165091043, 0.6233535927682083, 1.2770662773078916, 0.46020970888416074, 0.4141484292348226, 0.47095276210903264, 0.35696312333738034, 0.3236977228625067, 0.5371866443829659, 0.5034981429313266, 0.9063464046623004, 1.194624351567382, 0.5314866359427749, 0.6536196524789538, 1.049061797826718, 0.8363905728921004, 0.6653617929176485, 0.7300714036671803, 0.2551380799113331, 0.5200532116714613, 0.6138535891776478, 0.5763470697921936, 0.7329306544758549, 0.6131567133569388, 1.4307740115296614, 0.8844149290225207, 0.718024479738848, 0.6941069472477001, 0.5954132455768008, 0.6573772893855775, 0.543977168562899, 0.44790358376175715, 0.7377136317510453, 0.3913019146211955, 0.6492790592078006, 0.5479681604758077, 0.45194168400454837, 0.3776804361367285, 0.8325724559222465, 0.5604873755917088, 0.28000489248920546, 0.6312292013749428, 0.543261217865443, 0.39800706060245783, 0.39520224040070173, 0.6573664276700163, 0.3523988583428519, 0.6462794736937765, 1.009752475325413, 0.7762860301454836, 0.7366648690869109, 0.42901492619945103, 0.5031463071734465, 0.4323405687704363, 0.39526247012697685, 0.4345366619286842, 0.7410935721688418, 0.38590045647743426, 0.7148520426696248, 0.5242409490816521, 0.4187607248393497, 0.7461382212963892, 0.7884242948303875, 0.37054444776786555, 0.4470316059449139, 0.9724335076608066, 0.7975847803134475, 0.5429913612921649, 0.5656233613618228, 0.7872780098027481, 0.4807530942146595, 0.4776527442472445, 0.5970979189695419, 0.42176464633045035, 0.5343365131742296, 0.3936265221841615, 0.6589662194893722, 0.45517648595213644, 0.7782327707364008, 0.6038099862047143, 0.3884149777140594, 0.639166493864858, 0.8143337725441236, 0.4687222387531208, 0.4024986978560602, 0.6068571649936207, 0.5224160250720814, 0.34932715603429026, 0.41204179021714554, 0.5512527297612719, 0.4841710677503066, 0.35129014072996195, 0.7151238784172402, 0.8204742589261798, 0.833029850731984, 0.3439804540338741, 0.500661337525026, 0.7034284418704463, 0.4667882971342405, 0.5834336346962804, 0.8477311626820228, 0.5540919412228098, 0.9163802859501808, 0.5825881263160635, 1.0139173866453626, 1.0368961153158138, 0.91649509610992, 0.45524195482125096, 0.39827403529247957, 0.4652688619604666, 0.42254605347614344, 0.3482927006139283, 0.4760509044834754, 0.538272117861852, 0.462343086136712, 0.8099675976474199, 0.3194421427018873, 0.4888980866969197, 0.37580759471766706, 0.6416892274220785, 0.305951754976059, 0.5952529138576073, 0.47434544111227056, 0.3087046540958186, 0.428713951785155, 0.44172408060877366, 0.6919766321429959, 0.3919756186517895, 0.3321257127434465, 0.8496779037800648, 0.4437072495847277, 0.6228472785560453, 0.31966704913222516, 0.44672768453154904, 0.8568411341167632, 0.380202279468672, 0.3279483897276599, 0.5435387067173414, 0.30212839762780774, 0.3231123508091041, 0.42621010370743584, 0.3897316090154759, 0.5822119674820831, 0.4301430687352621, 0.42243362145669716, 0.39832974235333307, 0.5391144544319402, 0.7836860032515093, 0.3168578630915294, 0.35084352708124855, 0.34201792811602244, 0.23623233627261153, 0.33215449811698794, 0.4372676741421878, 0.39326040031563525, 0.8856544152895609, 0.3767326517802913, 0.36870349997826857, 0.5788033414423724, 0.36275660550722516, 0.42749395895865555, 0.8015830468927693, 0.4460630348003994, 0.5226384947981153, 0.4155799151365791, 0.8803787308177728, 0.3944787852764885, 0.4779013588775677, 0.793781359962266, 0.23456716524788127, 0.420981978355648, 0.7696248639746928, 0.501803852635571, 0.29381389115986073, 0.37850994288552453, 0.4361388926690327, 0.48948188835703926, 0.37850873098744975, 0.45697878924283114, 0.3015043631054107, 0.3844699100714951, 0.4102790923295716, 0.249420047633807, 0.4184526504046751, 0.5523766108311206, 0.3263259118418808, 0.7218303380036414, 0.9054835302109167, 0.3718634880269063, 0.8392111389360133, 0.37799564940588815, 0.7205467962956691, 0.37609382407375114, 0.691621533754412, 0.38775925630666386, 0.636557574915792, 0.5497189342713618, 0.35385204640464757, 0.4678694437971019, 0.5210705708975744, 0.4077529310457634, 0.2999557068000661, 0.2912114231747792, 0.43935322639677254, 0.4090907098056341, 0.244338550642425, 0.49483402633024787, 0.27570494669144197, 0.6762570973725347, 0.42288165897518015, 0.3092857011159261, 0.36440812218153507, 0.608368159480476, 0.5998208408644705, 0.3874924934556332, 0.23098126685854944, 0.3304246490720718, 0.47620591141960833, 0.45333359065046797, 0.4159882216426756, 0.3751240854227809, 0.37596554197446264, 0.7343424117562856, 0.2749802042166163, 0.37028656289691014, 0.2945649372903924, 0.4551487388773861, 0.7611014329035182, 0.3020338614781698, 0.28113967401296025, 1.2311383363835844, 0.4014805845336203, 0.6973722086804024, 0.8279051229233666, 0.6156474717595705, 0.3662235949353556, 0.3178706325086436, 0.5169347647822521, 0.3942527848074344, 0.4263914647950807, 0.2932098412513733, 0.2908440783396701, 0.42272242452159076, 0.3213988462926842, 0.6627738710498967, 0.7471105533664312, 0.5507368965542718, 0.34133298701308795, 0.3949439899846651, 0.28833162483714875, 0.6940839694175885, 0.2561621939735566, 0.6447642236356618, 0.42686110375043873, 0.3719038996424799, 0.4317255092068554, 0.36340854166313463, 0.2537258474032084, 0.36619868043181186, 0.26122595447081104, 0.2554052002098347, 0.26338964259985725, 0.5510302018348616, 0.24707530278425952, 0.4109469277930982, 0.36882597907249687, 0.35721883897647266, 0.32494637324243264, 0.283048964593836, 0.23634995199318753, 0.24876424531672342, 0.33533101762864587, 0.48042236498369134, 0.3412464458837349, 0.41720933652060205, 0.4146289779111294, 0.45644498344686424, 0.2720738404002779, 0.3503411293029785, 0.5648072355479403, 0.8935210091191479, 0.397931187151985, 0.8205447796606948, 0.3183494707125562, 0.3689966586919931, 0.536795627735654, 0.27655841762380196, 0.3129296628778631, 0.21320431348511953, 0.7868672438375242, 0.6932801330872025, 0.45714440605857154, 0.3962195318794172, 0.5628680137899112, 0.3268422215611308, 0.43460354161640957, 0.5494179404189443, 0.47130537553981655, 0.41452638213319737, 0.8389104040531692, 0.3874329745769501, 0.24783639769968874, 0.3721655009587606, 0.23308165503011988, 0.3064506829221306, 0.49664643385543145, 0.5767625285662137, 0.4604029664917598, 0.577355996152406, 0.25204172572979167, 0.41871631542841586, 0.2846615985289625, 0.3127270618580883, 0.1842547802859304, 0.23739250961102937, 1.361278289313413, 0.5503637921654299, 0.4036923576256248, 0.5578155062312172, 0.3227367704120271, 0.5880397371031689, 0.5026457530797359, 0.20179817835489908, 0.5293901964778944, 0.33058226495301346, 0.3249951523933007, 0.14343179692289096, 0.4852819123031962, 0.6066371989972663, 0.5899665359670727, 0.5158516632839715, 0.5013595669197314, 0.39689324462806785, 0.22584899323155186, 0.2979802014689515, 0.21147430043306645, 0.31554620302867703, 0.6893036403375513, 0.27691470894548625, 0.28285022897970835, 0.4714734058272921, 0.7920637607574463, 0.4116485611793278, 0.47714960400438133, 0.4290447271231449, 0.3609201992626252, 0.8879684598234816, 0.5472266942956946, 0.31628427358774036, 0.7672000949360259, 0.4784978210476861, 0.49748817660958905, 0.30463920560750096, 0.3279387750634043, 0.6304102824831199, 0.5930980938917015, 0.37348942198711405, 0.3308917601456912, 0.4985242176055908, 0.3469711502617305, 0.4933737998259694, 0.7411663258177603, 0.35802136840242327, 0.6806098934543049, 0.42259938737620484, 0.8919647834388394, 0.3006477369369234, 0.4140740396140458, 0.23288431792854852, 0.35965077456406946, 0.617552047123971, 0.392755245748288, 0.544110673268636, 0.310672132506812, 0.513044091394172, 0.36989138382371123, 0.42589783937404524, 0.45094851713914136, 0.6261906965267963, 0.22946240655589695, 0.5342371829549117, 0.473966804243305, 0.5492705478265918, 0.7434595622085943, 0.3159768724527054, 0.39165663556897, 0.32642875572991753, 0.37527891373967337, 0.36719912080203787, 0.7256810876301356, 0.1957872396285249, 0.20738932301841873, 0.3481998454199897, 0.43705503347428704, 0.49983497722840414, 0.31763087289123365, 0.23288272197655596, 0.5248106076982286, 0.21061019461776706, 0.4548383786128117, 0.2938354512829801, 0.4312619250666918, 0.469148720137927, 0.2250362021720361, 0.3994868455545651, 0.3125659175765463, 0.7499231624603271, 0.5303419797360396, 0.4023635163985924, 0.2641504048634242, 0.532988403387237, 0.4234578079880375, 0.42619595950589256, 0.2781505430661715, 0.5193813885187174, 0.28448287667814054, 0.4831647027225602, 0.26070628629881226, 0.2503477461132414, 0.40204891564010026, 0.27380614673243875, 0.31470858113749045, 0.38097184994520045, 0.1515097635697394, 0.22874438343332107, 0.5717967929166179, 0.6178913100948179, 0.7946539987523372, 0.7643598889756477, 0.6340515546124391, 0.907394800055725, 0.5246732115454791, 0.5135197250547037, 0.39683697370382454, 0.44448775802630935, 0.48153027913879765, 0.35437529528074, 0.5114786704625838, 0.2596992167385025, 0.3250211296347525, 0.4908163378229151, 0.3772532468491429, 0.6465634387292903, 0.4549123275625049, 1.0392395589843857, 0.38708320024871234, 0.7175703011484948, 0.5196420392461492, 0.7913351312028609, 0.5156441794007407, 0.47001805795106844, 0.76250125486137, 0.6092880726102775, 0.44042585585143545, 0.319688768235464, 0.5034320745727179, 0.476407540592664, 0.5630489375526635, 0.4508066291588273, 0.2791279599780129, 0.5234215854944906, 0.46861328730507507, 0.43061818872179297, 0.4750816856032637, 0.8016022397015441, 0.7748146404807408, 0.5411885619581792, 0.37056918691854185, 0.3892894957245899, 0.4080494203848555, 0.3335132556994023, 0.4414421021741136, 0.35789448768373516, 0.6104281022538355, 0.2346566985669445, 0.3418341362659836, 0.5290053619361071, 0.5414547760505777, 0.3972178381023784, 0.24708989546449614, 0.3436041267463634, 0.33569851541296863, 0.30204528513117734, 0.5158148976734707, 0.3469359202262683, 0.42495431386873095, 0.5635809238542165, 0.6857188463535439, 0.5615211006988735, 0.5746182992723253, 0.25933453616641816, 0.339549998953695, 0.2858667309023005, 0.4186506400133837, 0.3115419631912595, 0.26737229007107394, 0.45648378686471414, 0.2809687482181898, 0.27692545539852775, 0.5404373511961913, 0.2250320149071408, 0.4683735890047891, 0.474669221297315, 0.5540444103061644, 0.3026944989684395, 0.3083184109575437, 0.6080441429254678, 0.17305815453072576, 0.506738047879821, 0.4413818637220374, 0.3412207607462786, 0.6489673415819804, 0.31528555994650964, 0.824325992928611, 0.3538661430549003, 0.49464595351151175, 0.4259505758670814, 0.5997891811861147, 0.6468640849282893, 0.3903555029157608, 0.19029955645671054, 0.4486203958521161, 0.3465048001384178, 0.29833942740051833, 0.4566091852674263, 0.5401927045267871, 0.4449678834143528, 0.3661010461875635, 0.5057501049753536, 0.7986255666339023, 0.7223186780380727, 0.5620602348348596, 0.3342513618267402, 0.447255546921256, 0.665461609893375, 0.21237086801163413, 0.6931472131941053, 0.4106745882317001, 0.6607190712507351, 0.449997651737198, 0.7585940505562724, 0.604132772765608, 0.3035879733012273, 0.5330597069848588, 0.20770928700764973, 0.8096011050123917, 0.29719435980825715, 0.36470070937981347, 0.4163413021299574, 0.3402587161638863, 0.31643979154551094, 0.5357946367023372, 0.7397709054770113, 0.42867019974002113, 0.3655909442506715, 0.45491218050320936, 0.49232126581198843, 0.24729729087973581, 0.3383472460209093, 0.4355130907364221, 0.3454645031094485, 0.4001059270372577, 0.3786320609377142, 0.26994804700614017, 0.33725977102915444, 0.43072220248617094, 0.2561748984429267, 0.749243425793118, 0.23877652737428043, 0.3049850748472916, 0.3207462576796597, 0.2815310192238297, 0.3443586976973565, 0.30769745045171903, 0.28592040075022956, 0.23343534698805213, 0.4657772886817145, 0.5541433471200304, 0.30779632733983486, 0.4545713196425468, 0.4602420766606735, 0.4348309281090728, 0.702518116401988, 0.7524055973688761, 0.7405920625249027, 0.49877752031598777, 0.36317189401195893, 0.24025721609741826, 0.20960215280099873, 0.15905377582947972, 0.21554381758302124, 0.391100421154669, 0.43596450255482705, 0.6208127646673294, 0.24294077786664578, 0.3542670799082058, 0.36284661467113194, 0.2673445481864118, 0.8004402476976636, 0.39952135270454336, 0.6909980746542557, 0.41236571109656134, 0.4172806166044247, 0.5008288551099371, 0.46793313310259865, 0.6330923156329955, 0.3920824847435083, 0.5459634251327989, 0.17517540852721808, 0.6666643042836533, 0.8081943012181021, 0.43399195417991054, 0.41165316632299714, 0.2986024366106306, 0.31075913123113574, 0.33370719898923384, 0.31931965761017383, 0.8293439137746417, 0.4782822538045937, 0.20817412596482496, 0.2621329555022411, 0.4703270517760096, 0.5824070808310914, 0.30959261302234364, 0.30043106710209566, 0.27956350948294084, 0.6326240245638677, 0.41072214967090687, 0.191027567835334, 0.452295885540543, 0.26983431713683503, 0.3221343295766167, 0.30775190958609944, 0.27159379807608147, 0.5910738529962933, 0.47847018046685824, 0.22771399319564561, 0.38592900452766826, 0.4591596141006007, 0.17001157910484505, 0.2771689730279412, 0.40389877173139876, 0.33693672271001907, 0.3945904823387714, 0.3871913751254074, 0.2278348451808428, 0.5658814342804194, 0.37491525833651146, 0.24600194443871964, 0.4411014169689045, 0.26776321293711947, 0.4190216558630637, 0.5127037191644983, 0.3151183274814061, 0.6455761054893593, 0.5130171199802396, 0.4177955771111822, 0.3783529674875867, 0.3653806814776674, 0.2721160772931639, 0.24956448918132637, 0.3248684723770578, 0.6964744157973833, 0.3620136444702809, 0.5784141799957723, 0.3927951632641619, 0.34052839505004207, 0.6524271144826188, 0.4622421317036324, 0.3582809815644793, 0.7963553388345809, 0.32377129630966267, 0.5672107805146112, 0.25962710023115054, 0.3828107323567328, 0.3961754066923745, 0.5050349440766778, 0.5186577125851864, 0.3364045248123316, 0.26773800374333884, 0.27356531482476454, 0.4309756363254631, 0.3498202248831173, 0.5639144985544207, 0.9254598344696893, 0.22583743816766982, 0.30678237025976823, 0.27789499856176836, 0.3599361358748542, 0.27080457460630186, 0.3529097495255647, 0.4279007502138873, 0.2305608043649982, 0.2825372162457259, 0.2557890278815097, 0.3678483902250229, 0.26985866450300117, 0.11036652618672424, 0.36938281695048014, 0.157509679071831, 0.40064554150454423, 1.0186439711258108, 0.45825726958933705, 0.3728729963302612, 0.4460867375740107, 0.3217551822371181, 0.29226413715797406, 0.26835732178318733, 0.3451640275856117, 0.4377971965227969, 0.6129278390086392, 0.2919340746423118, 0.3120770734231345, 0.19408921628910175, 0.17888641187829074, 0.19757719331907359, 0.2133688868898334, 0.2872256681728289, 0.14885461837526354, 0.192013971677962, 0.2987177397199144, 0.7233940397283083, 0.27112965340170614, 0.43653346415604605, 0.20654335112556627, 0.16554879995180788, 0.14643813067316858, 0.23993928899722325, 0.2952151283763704, 0.29367901099033844, 0.3150631051431212, 0.2291159809145153, 0.36728928640776987, 0.278477990912505, 0.31230184218240165, 0.2990816853307037, 0.15770394869113988, 0.16662015101266286, 0.2440294250093325, 0.17497122050604685, 0.21343532917436617, 0.3190128788151989, 0.1684515466112079, 0.21904512528958206, 0.1873570622597541, 0.31230370451161826, 0.28354856289886327, 0.1806471764392063, 0.13722626638063146, 0.1774488074713783, 0.14780145973354192, 0.10567395759497016, 0.22475843331908235, 0.281296838647329, 0.26871689542891486, 0.19287845235882384, 0.22437842677509973, 0.21088511973418367, 0.19572438796361288, 0.633294808988218, 0.35246778255118466, 0.3721954162915548, 0.27900137149778187, 0.2464948605046128, 0.37862618535111997, 0.492752511282474, 0.19292662758633022, 0.4720357733887511, 0.23925699374684886, 0.3323894610771766, 0.31039048505858563, 0.1802958959369568, 0.48619660059611, 0.22089506912386705, 0.15393204131311716, 0.45766942554050016, 0.6912770065428719, 0.33357779159747736, 0.2811808757991581, 0.3099894962553425, 0.23200426407396257, 0.4694080609321986, 0.2523694889472357, 0.138748245070679, 0.2276437392855078, 0.15302239189008887, 0.13070281476685494, 0.26765859528591757, 0.19160910304794965, 0.21449015374581243, 0.975383383194008, 0.27446244499900124, 0.23969349338534376, 0.18336888538893834, 0.10682482986493447, 0.29528921331678115, 0.31207796474387745, 0.2581047089179901, 0.5291868739025901, 0.3940691220760345, 0.19060105508708744, 0.3267710961533698, 0.31032076193180397, 0.6287548141246208, 0.45588552563773693, 0.311225954840315, 0.24033997853597006, 0.2898234208424886, 0.19032841029547581, 0.37404836558640964, 0.3069374915134768, 0.3538422226905823, 0.20712768335484846, 0.3121249159177144, 0.1858594457857803, 0.22467119313712813, 0.19844551225822027, 0.24735071459492963, 0.20991810323437518, 0.5281376059850057, 0.27479216768139764, 0.16555178130477055, 0.27339654669089714, 0.31217203601067806, 0.4367229553369375, 0.4434186046842544, 0.12627468991774488, 0.18394363989561666, 0.15729609781132514, 0.4545479766425924, 0.12115649325805797, 0.1708297147289544, 0.1234694052352124, 0.45320641688027435, 0.25277332673368147, 0.3116616754979639, 0.43223992665345784, 0.2589032415730242, 0.37560931804908954, 0.38833886254917493, 0.2276928251009827, 0.22331121300314685, 0.23268048893321644, 0.2400743917985396, 0.2186898208111739, 0.11901951460611253, 0.2054048551288708, 0.6333908631251408, 0.1864028119040536, 0.3822968123962901, 0.27287625223143486, 0.25359001446774593, 0.46231361006245464, 0.26098685918030917, 0.22967115610571306, 0.21036658376593217, 0.22536903309426518, 0.5618470249595348, 0.3389430293582735, 0.5167000709713756, 0.37011153473492653, 0.7408807058311535, 0.49749906181949494, 0.4901223566797045, 0.4978233964015276, 0.2523613223823355, 0.158113082683176, 0.7054606990102844, 0.26940819208552796, 0.20249518119927606, 0.20247656873294284, 0.16828593672815165, 0.16704614081304947, 0.29068676776134, 0.4925257707942774, 0.46105469891003203, 0.42322693304808423, 0.23422902346756053, 0.5130396528822003, 0.33511388840591694, 0.5177108298347559, 0.5359190139844526, 0.28997760022024244, 0.5411407540628921, 0.5062142392255568, 0.2662431564597739, 0.18217245075299987, 0.596061376730601, 0.42444050596217914, 0.42722938895225526, 0.12983696460723876, 0.26261181900656666, 0.37231770231565303, 0.711529963032729, 0.4363518705608869, 0.4412447981039683, 0.2220442612816683, 0.6014985072418249, 0.27164305752179263, 0.3943796844234714, 0.19209924141332904, 0.4054796729769025, 0.34091892799614987, 0.20939930876096086, 0.31020512595192606, 0.1271124593870228, 0.4450239420559189, 0.24684932169079796, 0.18065867030431354, 0.607838684251332, 0.38241339444148875, 0.26296890095026804, 0.33785526728242393, 0.2516064016680062, 0.8906386239033941, 0.7794158579173842, 0.6272549307288755, 0.45441727354412986, 0.6045115915938741, 0.412671669217992, 0.4627714297645971, 0.2924022204185355, 0.37979815142415896, 0.2528615462070566, 0.47166335930248887, 0.2616139145276569, 0.2969487401133095, 0.3887219709740546, 0.3895848485470506, 0.9912856889214318, 0.37819962293038634, 0.4097369941167905, 0.3045169188984833, 0.3759459868409579, 0.2572506350964946, 0.35853073476540925, 0.47484807712851246, 0.48873118186237835, 0.24088355303842782, 0.33341215286736897, 0.44595444214882163, 0.5388051155042944, 0.22035931930967875, 0.5131379379277804, 0.7111931750407585, 0.44555718532172583, 0.3827102594187694, 0.399519100189209, 0.28571054498082693, 0.17308261828017807, 0.4290612972745992, 0.35172408658695575, 0.4314326530032687, 0.31256733805786013, 0.25472608873148284, 0.30963639974594115, 0.21207347362596485, 0.5193882765618782, 0.171420675485569, 0.2375946148959073, 0.2128834193593197, 0.7454300940988604, 0.32153289421260006, 0.33686482429504394, 0.18639207225142818, 0.16362170896912476, 0.3028525056261005, 0.523180158100431, 0.34177559820129993, 0.34313649164213167, 1.0251219088913233, 0.760908868199303, 0.21945188737617127, 0.5091007618374295, 0.3203913603359486, 0.7313862271160703, 0.3067059867433152, 0.4929211744635996, 0.6664157125584431, 0.2176248041788737, 0.6189962073167166, 0.2106973307174549, 0.2209684332557058, 0.4488030745188395, 0.23003511680306937, 0.6656771891044848, 0.37788902115988565, 0.45012801925341284, 0.4995495388539776, 0.5778985989129196, 0.5326448000918378, 0.23070694120415364, 0.17605707434227055, 0.26849120950735905, 0.2089772691259851, 0.20809376873983237, 0.18702260321216158, 0.2595482733514574, 0.17320012296470816, 0.2814081498792955, 0.26220437372941896, 0.30790513432532546, 1.1290100861262609, 0.5673121283271096, 0.2749044146920693, 0.45213085518175467, 0.45135624242895983, 1.4584805252965627, 0.4227198254858326, 0.2948407549340022, 0.5229379565380707, 0.24268308684559176, 0.5436411431861068, 0.3953918672778888, 0.33605973146228546, 0.2957841547186006, 0.1881182788558628, 0.2512661102290433, 0.46517341782224825, 0.3307574371569084, 0.32610674021125635, 0.19793924331665042, 0.580705814134507, 0.14667955222073153, 0.13550541377043698, 0.17066571624190718, 0.4513562176777766, 0.4982940248216254, 0.3608228851706435, 0.2536218223647465, 0.13680687729812072, 0.22309521220938336, 0.2766264782431885, 0.47943209545428933, 0.4229084279125222, 0.32588137735708106, 0.1356405473237086, 0.3690036380183793, 0.2825313919711846, 0.474366835753123, 0.2629262073439171, 0.28081194532223236, 0.1550103852798889, 0.5874558003076584, 0.4897367599381782, 0.23350172678629555, 0.524038584948432, 0.2239684237752642, 0.3446411784391225, 0.4354159966660903, 0.16283443329581088, 0.22721708780759337, 0.16189819600847033, 0.2621152589260004, 0.4600389539532803, 0.20170821960155783, 0.30583098600165504, 0.32616981679742985, 0.2918431291417179, 0.30930153374949465, 0.34357162176960643, 0.23814540960888858, 0.6032082727380874, 0.2209505330191718, 0.5365621590582144, 0.18481435094818927, 0.2831857959691421, 0.4758584841168626, 0.26388722812516885, 0.2899280444412532, 0.1289484768647414, 0.14576064836856475, 0.2819484416452477, 0.1681850461149404, 0.13576192403340676, 0.19347352627425166, 0.13227933715589296, 0.19215385546587935, 0.44123809369405115, 0.21387624348169085, 0.20946524591845012, 0.30742589411109383, 0.24627675910728475, 0.30056074837813057, 0.4673682859005073, 0.21132698528680965, 0.3400461018360155, 0.21830263310848438, 0.2619356251311028, 0.2650043441916144, 0.25039761032319474, 0.5097699282719539, 0.1993042518542363, 0.13483870228903494, 0.12512702841078757, 0.27708259915917866, 0.21493075246125265, 0.10131921970349853, 0.28562612120872755, 0.09096306578699366, 0.6427267871438146, 0.16517441559504795, 0.1900868553643698, 0.2658653683132596, 0.2826512705751788, 0.26786961128210496, 0.16586709556897858, 0.14469264125871922, 0.4363674229046245, 0.24802732819677392, 0.7690213264060277, 0.1413408484465804, 0.18044741272489667, 0.17853035438739728, 0.10159992755405486, 0.1073439349007637, 0.18056141271627119, 0.333846734629737, 0.19037123091015226, 0.1972350359237057, 0.19952778749532635, 0.16919588883717857, 0.1076243594452575, 0.201280658351215, 0.30441023826599123, 0.14452276633685324, 0.15078130816264954, 0.9391328359527016, 0.14635303859327947, 0.16684372795151298, 0.1342478283728012, 0.2695931198172533, 0.4763435638157774, 0.17416571393792885, 0.4665215900488067, 0.23841468352891412, 0.21396835676444295, 0.2645349733820765, 0.36453427612642586, 0.1822827100753784, 0.21442171977938468, 0.14838989973215683, 0.24066621749432054, 0.27688616459297416, 0.18640977206987364, 0.7410588956268642, 0.08680527929601997, 0.19427423266284086, 0.3072224179278965, 0.6899118959615519, 0.2893415227833165, 0.5929799206501134, 0.2547697287117129, 0.5978964763738992, 0.3649013065168206, 0.19706683889447843, 0.5793190623574909, 0.44266091237812893, 0.2818228060952387, 0.17672535618438606, 0.2772700450708978, 0.21815820907379363, 0.17113704791610435, 0.16998755909441696, 0.282496031987702, 0.26292972017859223, 0.18645622513510968, 0.2873754361552051, 1.087939883927057, 0.3596560072073033, 0.3041464238154245, 0.5357250136214418, 0.12059917060590664, 0.13466575519576446, 0.30465425938477014, 0.28389030506422447, 0.13587410990599974, 0.22706648485083716, 0.20732874852350464, 0.19773139600563341, 0.40308710894991373, 0.18106746706143934, 0.18299625847074721, 0.20989330511487858, 0.33658178032870495, 0.2685484606424967, 0.2329204262191174, 0.28101039295324975, 0.9527286780186189, 0.22196173327961607, 0.24651017168114184, 0.30579483244154193, 0.5421856088710554, 0.10583909137199503, 0.3481542398472025, 0.12321895371038447, 0.12999092423711683, 0.2304548762394832, 0.13609327109502173, 0.0889156081459739, 0.43314545072907695, 0.22327789513200477, 0.24078090135554095, 0.16912716964790725, 0.23856940046573055, 0.1394320511847043, 0.15932953095845379, 0.12287917932435526, 0.21258514241581689, 0.174845229573461, 0.33068261509849906, 0.262654991695608, 0.2691789427704394, 0.37818066142443263, 0.08797062092502766, 0.4053058716897808, 0.13761314787412351, 0.31443461560721353, 0.19213708451747066, 0.4131278801320983, 0.17955659122026266, 0.2577683871569651, 0.25552504355687206, 0.20773857621324093, 0.17567438970803614, 0.14968455134655023, 0.43962110431106005, 0.3240563680264947, 0.23989772326638253, 0.14924237760368753, 0.20485041319086728, 0.2633240657451349, 0.5244693965702266, 0.23527089065712675, 0.10004205910460676, 0.1232809867222863, 0.14714717712237207, 0.20684552556911787, 0.6584952360623842, 0.4169582155772618, 0.2055581972712562, 0.37126938434980605, 0.2144289282448845, 0.2224952075353622, 0.16636381733371797, 0.18486255660835318, 0.13124954335418812, 0.25375818634870695, 0.13099712348581144, 0.14885195530443635, 0.18437784740381072, 0.17416858737499682, 0.2609832781000544, 0.4907774940104047, 0.4495432905697715, 0.22698512971562743, 0.173033555436047, 0.23954901261258757, 0.7138187038537229, 0.21280992659219528, 0.26830110861255, 0.2261405514700537, 0.2125665194594176, 0.2996357185777886, 0.22451157778028458, 0.272868722560359, 0.34751750111579893, 0.14894209118487667, 0.207713976491, 0.11274816881129635, 0.14084039718385727, 0.2061681269647711, 0.13995299626434138, 0.35471258601031475, 0.17829131285349525, 0.21716568438127082, 0.17118744316524437, 0.4264229652485986, 0.22264888645872793, 0.15003495760984223, 0.22634712102300286, 0.10844292627283499, 0.1206465092236664, 0.16967066830722646, 0.2421287547217475, 0.18589104218457744, 0.15813708040449354, 0.23741886063871265, 0.4480547781551586, 0.12363941033681233, 0.5160838468071274, 0.14426811843019013, 0.3404922126884783, 0.15338133294552284, 0.4193868701642093, 0.18847821730154532, 0.1929772123749003, 0.10564157447535476, 0.09720750117868557, 0.20060440108793126, 0.32770502733461787, 0.12396616049137595, 0.15021217720181335, 0.9065006168149358, 0.15911767049269243, 0.21445694111956065, 0.15900766749111553, 0.21322682939703305, 0.1299927678078046, 0.27219907587224784, 0.26330218372922953, 0.14399693044026693, 0.16772773875691874, 0.4261606343859291, 0.28822723313797194, 0.2563755614940937, 0.19148768520109521, 0.16338197469420196, 0.17671206316345722, 0.26788608801407626, 0.3374454487116242, 0.10225838882582527, 0.08102442575542439, 0.13398099668098218, 0.10000753190377314, 0.16178226506799684, 0.15388372173551, 0.15981926414236208, 0.315812417108966, 0.31411906514848986, 0.19932685330201597, 0.13449064256242754, 0.16715834708582944, 0.10359398145726792, 0.1753299148797723, 0.6733901581206878, 0.14415328111404027, 0.2599213935338057, 0.19788296852276954, 0.17924437349148814, 0.32195798414724847, 0.19553384196828089, 0.2028305327669584, 0.2156172977607099, 0.16455832336411808, 0.3892743174737621, 0.22735482282805863, 0.41867677661366676, 0.42938795721429035, 0.17371622179167973, 0.11743415774805, 0.07132370575880394, 0.15084441957019623, 0.1425100480370866, 0.14436804219188853, 0.12076051520486164, 0.10095373251263536, 0.1339215225643582, 0.08146757645577354, 0.1421382435229646, 0.1975397954220517, 0.15585596669724572, 0.12310587490834282, 0.12293952250389521, 0.14608473023501312, 0.2848761425252568, 0.07150497442075505, 0.20242532151770654, 0.18296300837990712, 0.2061571380827162, 0.18764796721307855, 0.15275205153445498, 0.25239270614019166, 0.2217922850803181, 0.14328384259148103, 0.5128618094408257, 0.12881666625216928, 0.2359885803069175, 0.1886750110360028, 0.2055679777267601, 0.16987650275063682, 0.13254532230845278, 0.2003590214546091, 0.163314707500782, 0.18464144797552198, 0.18663256963094077, 0.20063618165934066, 0.40797346412015106, 0.11867160723150894, 0.349863468484756, 0.091347617406482, 0.8463178963131375, 0.21409955721461485, 0.1449632514534591, 0.2179929862982874, 0.444480031142119, 0.07734503973887293, 0.1647670370900733, 0.17366793737137498, 0.13361361761550444, 0.09995463296435282, 0.05811597756559304, 0.532826155018016, 0.30131470263019483, 0.1687082705959197, 0.3268651648556295, 0.14571119876134964, 0.1533163981778281, 0.26011542327997506, 0.24280482812361287, 0.26927571347636037, 0.3435176672603621, 0.3456934710608588, 0.1408721645337871, 0.07163586897727771, 0.1109987921175573, 0.11201573241470206, 0.5408279512978934, 0.2691381892365384, 1.11299466306513, 0.32428010838491, 0.20283071625617244, 0.34295442297927337, 0.2949613772498237, 0.226472055606353, 0.2504183803493404, 0.15130129198805603, 0.8688705399505092, 0.47582964437611236, 0.25149856170018514, 0.2496191229533952, 0.13506431178525036, 0.18511580604434624, 0.08254431586038499, 0.11748570723874494, 0.16668195889625717, 0.11425915491013298, 0.39572352191901583, 0.6188976989769788, 0.18686160583539174, 0.13468244727140938, 0.33795972583058115, 0.11624206591875126, 0.12190594069774334, 0.11411463488806342, 0.16956347151104473, 0.2605913942200797, 0.12547833575913253, 0.1476391383579799, 0.1121822265319494, 0.23240375717154732, 0.8031738633863068, 0.08905417028364244, 0.28668936464812733, 0.4988227074739163, 0.4961844719216118, 0.20305805365484283, 0.25414986492889097, 0.5764637238967822, 0.5223955394540514, 0.1342898387225267, 0.10321706954878991, 0.23436730482114726, 0.14645104198744802, 0.20127344311800183, 0.721424280330986, 0.24050902013231398, 0.13054294284701298, 0.33484503498926815, 0.10998056689898172, 0.0830337823572613, 0.17993497080617138, 0.15515478659228632, 0.2607494750775789, 0.15366489240404968, 0.1900721846783461, 0.5496247649817935, 0.4491187090599674, 0.19220449144549007, 0.31173941455044585, 0.1380128997337383, 0.24045082924857972, 0.29305096589601953, 0.33719529123712605, 0.28852118525312936, 0.13842946857438052, 0.3222123322579814, 0.11484117576659861, 0.20397243246331914, 0.22710341138455456, 0.3938865597238139, 0.4659220766482745, 0.31473281589167856, 0.17459869296462446, 0.096729693356482, 0.33293410194435635, 0.11033466525209612, 0.19701604119049812, 0.15683282010827057, 0.0888147375614599, 0.37857973964453945, 0.2004644923951421, 0.2399146421750386, 0.11955196696283657, 0.1375598821244221, 0.13856268598275853, 0.11913640832589342, 0.17556029542188206, 0.18999803405147578, 0.09637732108434041, 1.096103843380056, 0.2273782349256129, 0.10921376932234991, 0.12051137599078092, 0.17790701944590648, 0.11787040640129984, 0.304245530780701, 0.2912479315175639, 0.6002918344461972, 0.3136292575551062, 0.15072504770839085, 0.19002130269713163, 0.3311257625936152, 0.2882064982042488, 0.3661914801190681, 0.38922713047228985, 0.3228046614921044, 0.14420035835604622, 0.2651876899457187, 0.2218754269111724, 0.11460059029715401, 0.1407941944074045, 0.3397277604883666, 0.20515566972585825, 0.7694258758756849, 0.15168445202798933, 0.4757676030439012, 0.22081872781117756, 0.13514440559006, 0.14878153108124043, 0.13712532676644124, 0.10120164779254368, 0.2274469076788109, 0.1370518935193728, 0.1377386675847994, 0.15746003200376757, 0.1473210849732067, 0.0759606529497164, 0.13326608998416778, 0.11914538624998812, 0.605597999615944, 0.12480333356846585, 0.33702047186020095, 0.2694181897179036, 0.19812872217788752, 0.13476469630745525, 0.08443260841426517, 0.09962629129016211, 0.6211290890073019, 0.21299437350199035, 0.09821211423629368, 0.328630577476024, 0.2317256558130658, 0.38140548996358176, 0.15575149239620897, 0.3559521401395985, 0.5155047498741041, 0.17418769253042593, 0.21044321735554866, 0.2677721535937386, 0.11375491967675282, 0.8050798838983766, 0.21120158187309257, 0.09231089316474066, 0.16176427406635016, 0.09275082434030404, 0.30380863275005543, 0.3576174004660146, 0.24249606654757544, 0.10226292747439761, 0.06021019805561413, 0.10056121717521918, 0.1071727436686319, 0.2614125629757019, 0.1464324308402235, 0.24315970786528904, 0.1012381879070349, 0.2559440164729383, 0.19896989603235266, 0.2821263822340693, 0.1625587832481466, 0.17387136091917627, 0.19197219488821624, 0.19027158825337384, 0.20283626168202132, 0.08382029151480042, 0.09093235554117145, 0.08195374977818612, 0.1561875347296397, 0.0981013944024354, 0.4854506366855496, 0.24472510526054783, 0.27384674365703876, 0.14263025369399632, 0.17915798553964185, 0.07084738772848378, 0.18412840838465097, 0.14734626801515688, 0.2649368643833458\n",
    "   \n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "0Thz8u_5riQf",
    "outputId": "02e6f58e-b9b2-49f8-9600-665c8eec91c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f10a97e7da0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXwTdfoH8M/Tk/uuiBQoN4LcFfEWFAXZn3iAgrqi6673sequixerrCi6rrKsByuC9yreIJdyKiAUylGgnAXKDT2A0hZKr+f3RybpZDKTTJJJk0me9+vFi2QymXmaps9853sSM0MIIURsiQt3AEIIIWqfJH8hhIhBkvyFECIGSfIXQogYJMlfCCFiUEK4TtyiRQtOS0sL1+mFEMKW1q1bV8DMKcEeJ2zJPy0tDZmZmeE6vRBC2BIR7bPiOFLtI4QQMUiSvxBCxCBJ/kIIEYMk+QshRAyS5C+EEDFIkr8QQsQgSf5CCBGDbJn8f9tdgD35JeEOQwghbCtsg7yCcfu0DABA7qThYY5ECCHsyZYlfyGEEMExnfyJKJ6INhDRHJ3XkoloJhHlEFEGEaVZGaQQQghr+VPyfxzANoPX7gVwgpk7AXgLwGvBBiaEECJ0TCV/IkoFMBzABwa7jADwsfL4GwBXExEFH54QQohQMFvynwzgaQDVBq+3BnAAAJi5EkARgObanYjoPiLKJKLM/Pz8AMIVQghhBZ/Jn4h+ByCPmdcFezJmfp+Z05k5PSUl6OmohRBCBMhMyf9SADcQUS6ALwEMJqLPNPscAtAGAIgoAUBjAIUWximEEMJCPpM/Mz/DzKnMnAZgNIAlzHynZrfZAMYqj0cq+7ClkQohhLBMwIO8iGgCgExmng1gOoBPiSgHwHE4LhJCCCEilF/Jn5mXAVimPB6v2l4GYJSVgQkhhAgdGeErhBAxSJK/EELEIEn+QggRgyT5CyFEDJLkL4QQMUiSvxBCxCBJ/kIIEYMk+QshRAyS5C+EEDFIkr8QQsQgSf5CCBGDJPkLIUQMkuQvhBAxyHbJP+vAyXCHIIQQtme75F9QcjbcIQghhO2ZWcO3DhGtIaIsIsomopd09rmbiPKJaKPy74+hCRcgCtWRhRAidphZzOUsgMHMXEJEiQBWENF8Zl6t2W8mMz9ifYjuCJL9hRAiWD6Tv7IWb4nyNFH5F771eSX3CyFE0EzV+RNRPBFtBJAHYCEzZ+jsdgsRbSKib4iojcFx7iOiTCLKzM/PDyxgqfcRQoigmUr+zFzFzH0ApAIYQEQXaHb5EUAaM/cCsBDAxwbHeZ+Z05k5PSUlJaCAJfULIUTw/Ortw8wnASwFMFSzvZCZnd1wPgDQ35rwhBBChIKZ3j4pRNREeVwXwBAA2zX7tFI9vQHANiuDVJNqHyGECJ6Z3j6tAHxMRPFwXCy+YuY5RDQBQCYzzwbwGBHdAKASwHEAd4cqYMn9QggRPDO9fTYB6Kuzfbzq8TMAnrE2NH2S+4UQIni2G+Er2V8IIYJnu+Qvdf5CCBE82yV/Sf1CCBE8+yV/KfkLIUTQbJj8wx2BEELYn/2Sf7gDEEKIKGC/5C/ZXwghgma75C9lfyGECJ7tkr+U/IUQInj2S/7hDkAIIaKA7ZK/DPISQojg2S75S+4XQojg2S/5S8WPEEIEzX7JX3K/EEIEzXbJX+3LNfvDHYIQQtiS7ZK/uuQ/7rvN4QtECCFszMwyjnWIaA0RZRFRNhG9pLNPMhHNJKIcIsogorRQBAuYq/Of+stu3PLeb6EKQQghbM/MMo5nAQxm5hIiSgSwgojmM/Nq1T73AjjBzJ2IaDSA1wDcFoJ4TZk0f7vvnYQQIob5LPmzQ4nyNFH5x5rdRgD4WHn8DYCrKURzL7PHqYUQQvjLVJ0/EcUT0UYAeQAWMnOGZpfWAA4AADNXAigC0FznOPcRUSYRZebn5wcXuRBCiICZSv7MXMXMfQCkAhhARBcEcjJmfp+Z05k5PSUlJZBDCCGEsIBfvX2Y+SSApQCGal46BKANABBRAoDGAAqtCNAzhlAcVQghYouZ3j4pRNREeVwXwBAA2hbV2QDGKo9HAljCLGlaCCEilZnePq0AfExE8XBcLL5i5jlENAFAJjPPBjAdwKdElAPgOIDRIYtYCCFE0Hwmf2beBKCvzvbxqsdlAEZZG5oQQohQsd0IX62Jc7eGOwQhhLAd2yf/acv3hjsEIYSwHdslf2lGFkKI4Nku+QshhAieJH8hhIhBkvyFECIG2S75y8RuQggRPNslfyGEEMGT5C+EEDHIdslfunoKIUTwbJf8hRBCBE+SvxBCxCDbJX+p9RFCiODZLvkLIYQIniR/IYSIQWZW8mpDREuJaCsRZRPR4zr7XEVERUS0Ufk3Xu9YVpAFwoQQInhmVvKqBPAUM68nooYA1hHRQmbWTqS/nJl/Z32IQgghrOaz5M/MR5h5vfK4GMA2AK1DHZgQQojQ8avOn4jS4FjSMUPn5YuJKIuI5hNRD4P330dEmUSUmZ+f73ewgPT2EUIIK5hO/kTUAMC3AP7MzKc0L68H0I6ZewP4D4Af9I7BzO8zczozp6ekpAQasxBCiCCZSv5ElAhH4v+cmb/Tvs7Mp5i5RHk8D0AiEbWwNNIA5J0qC3cIQggRkcz09iEA0wFsY+Y3DfY5V9kPRDRAOW6hlYEGYuXugnCHIIQQEclMb59LAfwewGYi2qhsexZAWwBg5qkARgJ4kIgqAZwBMJpD1CdT76jlldVISpAhC0IIYZbP5M/MKwCQj33eBvC2VUH56/UF2/H877p7bJchAUIIoS8qisuLt+fpbpfkL4QQ+myY/D0zerVkeSGE8IsNk7+nONKvlZJLghBC6IuK5L+3oDTcIQghhK3YLvn7U8Mjk8AJIYQ+2yV/IYQQwYvq5C/lfiGE0Ge75C8JXQghgme75O8XuVIIIYSuqE7+LNlfCCF02S75+9fbJ3RxCCGEndku+ZeWV+puLznruV1yvxBC6LNd8l9qMI/PBX//CUt3uL8mJX8hhNBnu+SfEGcc8uo97ksISJ2/EELos13yT4w3nl1aW9KXkr8QQuizXfKPjzNO/tXVjEsnLXE9l+kdhBBCn5llHNsQ0VIi2kpE2UT0uM4+RERTiCiHiDYRUb/QhAskxBuHXM3AoZNnXM9fmJUdqjCEEMLWzJT8KwE8xczdAQwE8DARaZfNGgags/LvPgDvWRqlymWdjNeF92de/1W7C5E2bi72F562IiwhhLAVn8mfmY8w83rlcTGAbQBaa3YbAeATdlgNoAkRtbI8WgAD2jfDjpeHGsVq+jjfrj8IAFi9N+zrzAshRK3zq86fiNIA9AWQoXmpNYADqucH4XmBABHdR0SZRJSZn5/vX6QqyQnxuturA6zi/3T1PnR5fj6qAz2AEELYjOnkT0QNAHwL4M/MfCqQkzHz+8yczszpKSkpgRzCq0CXc3xpdjbKK6tRKclfCBEjTCV/IkqEI/F/zszf6exyCEAb1fNUZVut+jxjf0DvM1gFUgghopaZ3j4EYDqAbcz8psFuswHcpfT6GQigiJmPWBin5aQXqBAiliWY2OdSAL8HsJmINirbngXQFgCYeSqAeQCuB5AD4DSAe6wP1VrO0b+ks00IIaKdz+TPzCvgniP19mEAD1sVVG0iIhAIAMfU3cAlry7GFV1SMOmWXuEORQgRBrYb4RuI33IKMPWX3cY7xGCd/+GiMny59oDvHYUQUSkmkv/tH2Rg0vztpvatrmas23fc8hhOlVVYfkwhhAhUTCR/X5wFf2Zg+oq9uOW9Vfh1Z+DjELS2HCpCrxd/xqyNtd4BSgghdEnyV2EwduUVAwAOq+YICtbWI45hEct3FVh2TCGECEbsJn9V4666nz8p9wEx1PYrhIhBUZ/81Qu8FJSc9bovc82FwMqeP6/O22bdwUzak1+CFQZ3Glbe1Qgh7Cnqk39hSbnr8eNfbvBY69e9n39oRvueOF3hca5QG/yvX3DndO0UTA4PfLauFiMRQkSiqE/+6pW/VuYUYvwPWzz2cVX1hLijv94i8+EQKXEIIcIn+pN/gvuPmK9T9eOq6lFtC8Vo3yqLJo578+cd2HjgpCXHEkLEpqhP/vd8uNb0vo6Cf+gqZ+IsqlOasiQHN76z0pJjCSFiU9QnfzNcKTlEDb6u88TgSGIhRGSK+eT/1NdZKC2vAgAUlJ6tGfAVgnOpS/4VVdV44NN12Ho4oKURokJOXgnSxs017JUkhAidmEv+zm6Oesl9+JTlNU8CKPqfKqtA2ri5+Ns3m/R3UJX8dx4rxoLso3jq6yzXtuzDRSiO8GkgqqsZ+cXeu8yatTbXMY3GnE2HLTmeEMK8mEv+RWeMe7qUVVQHVTVz/yeOLpQzMw0mTFNdT5zXFvXphk9Zgbt9tFGEukeSL5MX78KFExfhaFFZWOMQQgQn5pK/2QodX3udKC3H0Mm/IregFB+u3IvcglJsPlTk45g1R3Xm8DjNb2DdvhPejxHmocdLth8DAMtK/0KI8DCzktcMIsojIs8O8o7XryKiIiLaqPwbb32YVvJetK/p8+/9KPO2HMH2o8X4z5IcvPTjVoyc+ptfUTjXGzY7xiBt3FykjZsbldNOhPuCJkQsMlPy/wjAUB/7LGfmPsq/CcGHZV7P1o392t9XtU5Nbx/3jFR6ttJrgj7lpTpJj/NIcX5WM4W72scuibq8shrVFo2rECIa+Uz+zPwrAOsnuLfIw4M6+rV/fvFZFJ0xblTVy8VHi8rQ4+8/YfqKvR6vOaty/B0UVu2q9Dd3p1FzvtqTW1Bq2wTa5fn5ePb7zeEOQ4iIZVWd/8VElEVE84moh9FORHQfEWUSUWZ+fnDz5bduUtd5VL/fu3DrMZw4Xa772oLsowDck+yBE6cBAPO3HHVt01YPMftXKndegPwv+fveJ/uw97YH9+PpHzAnrwRXvbEMU5bswvcbDqKyqtr0MSOFrFQmhDErkv96AO2YuTeA/wD4wWhHZn6fmdOZOT0lJSWoky7761XY8XJNbdSQ7i1xeecWpt77/YaDWLZD/+Jz7JRxQ6ZennYmTzNp35lnsw8XuUYe+3vpOnbKdy+b4VNWmD7eDwYLzMxY6bjLmbxoF56YmYUPNHc9oRywVlxWgZd+zEZZRVXoTiIi2p78EmTJFCYhFXTyZ+ZTzFyiPJ4HIJGIzGXhICTGxyE5Id4tCSXFm/txVuYU+txnzV7Pmi5ngj//hQWuKgXntmoTRfL5W45iZU6B28Au58Avs/cMn6zK9dhWUHIWaePmuvrN+2PH0RLd7f/L2O9+Dot69/y2uwAnDe66nN5ekoMPV+Z6xGClD1fuRdq4uQFNcmfXqjA7GfyvXzBCpjAJqaCTPxGdS+TIYEQ0QDmm7+waAvH+1qF4oa7i0TqjKpGqq33UjpeWY+n2PI/33vFBhlui91aCXro9D5mahK6Xd9YqF6rpyz3bJABg4CuLMUNVci8r979E7TxtMA2+5ZXVuH1aBsbOWON1v0rlhzRzQQ3UhytzAfh/UTt44jQ6PDsP36w7GIKohKg9Zrp6fgFgFYCuRHSQiO4logeI6AFll5EAthBRFoApAEZzLXZJUa+/a2XyBxwLwby7LMfjXGrqH7RUlVT/8NFa3PPRWv2SpdsqYp5dPWdnOUa83vPRWoycusp0vJXVrFuHf/RUGSbM2QrAMZ3zYdUALbMN1YH8RlfvKcTDn693xeRM5tuOOpbKNPptORvWQ/kt0pvJ1YxdeY47JefvSAi7MtPbZwwzt2LmRGZOZebpzDyVmacqr7/NzD2YuTczD2Rm/zq8B4moZjYeq2bNdBr9/mq8vmCH2zZtPaRR6XRPviNJ6DWUqhOu3vVq8bZjbs9fUK1BoD7djqPFqK5mVyJbtO0Y3l22Wzcep1Pank6q4/2w4RDSxs3VnWIir7gM+wtPez221j0frsXczUdcd0qqX5UpgU6rbabsEeg3paawIVU/wt4Swh2Alawu+Ts5/85zC0s96yENcgB5uRCp84bzgnVEVRrXvvPT1ftcj9UXm+sm/4qnh3ZFhxb1XdtmBtHDZeovjgvHwROeyzzO2XQEczYdCfjYaqG82/D3ff4mcW+/V6v9sjMfR4vO4LYL29baOUXsiJrkzwwkhCj5OxWUeDZU+kpkvnILETB/8xE8+Pl61TbzP0fWgZNuyd/f3OQMb8GWo9iuVMcEW9f+8pytaJ9S3/CziYQxDYEm8dqcldvZNiLJX4SC7ZO/+o8xLsTJX49RIvM2kMytwRfklvgd27ydz/2EOXnuvXUC/QTeWrhTdY4AD6Jwdgutk+ioVTxTXoV6STVfNe3hg71IeLzPxD7BTt0ttT7C7qJqYrf4MKyW4quU3PcfCz22qd+yO1+nq6WXH0N7tlwf9fDai4X2I3K+rt5uVWIrq3C0dzzz3WZTx62uZpwur2kgD2Wdv/Mz9vdnrWkoluwv7C1qkj8jdCV/bwk+kES5R5Xw9db1JS/ZXxuL9kfWVmeod3/o83XYW1Dq9vrp8irkFbsPHDOT2Py5zu4/7n6BMkrOkxZsR/fxP5k/sCXMtj8wPly5FyVl/o8LECIS2T75qydiS4wPTfL3tvD6ydP+L76iHi2bp9PP3Fti1eZNZuClH7e6nmuTu/piMW/zUdw+LcPt9c8z9mPAxMVu24xGP2vlF591TfHsjUfMBvt9oRnUFegdiPrXlTZuLkp1utuqP+L9hadxpMizkVtt1e5CvPTjVoyfnR1UbEJEiqhJ/gBw18VpITnHlMW7DF9bE8CoWl+8XcI+1yTIymp26ymktSGAIfJvqur/vbnjg9X4w0eZqPBz3h+jxGlVPu3y/Hy35yd12l9INaHeFf9ciotfXeL1mGWVju6qzrYcK5J/RVU1th2J3WU8RXjZPvn3Tm0CAPjT5R3QokESAMc8P1bK0JnqIZSsbLo4eMJc33xnTx9/7DzmqL5SJ8LNBz0nlXPeffhKmNrqIKv60ut9nP42+Gqr4qyo839l3jYM+/dy7Css9b2zEBazffJv3iAZuZOG45JOLdCkXhJmPXwp/j26T7jDCspXmdZNHfDEzCzfOwXgtGo0szoRbjjguRKZma6jOXklOB2iidycF9OjRWVYusNzyg1/lFc67nKsuC6t3++4Kzte6n2uIyFCwfbJX6t3myZu3QpFaKirwtSJUK+UvTvfUbI1Ki2v2lOIa978xSOhmp0/7WhRGZ7/YTMqq6rdJs2rickR1Yh3VrhmU9WL3ava70gmREhJlhQBcZaAAU0CNVlnNXnRTjSskwgAOHBcv7FVvebC8dJy/HvRTnRIaYCxl6S57ffMd5uwdEc+Ss9W4fsNnlNUO0NST9cdbJdNK9t7pe1YhIMk/wh1Smd+nUjCbo8dz9btO+F1lkz1RWLyol1ITjB/43njOytdXUa1yd85C+iq3fqTyerX+fu3glqhdnS3ZGxhc1FX7aOmnvbAbnq9+HO4Q/BKvd6Bs3rmlvd+w78Nekbp9Qg6W+m9l5B68RvtWAE9R00sdOM4LvvVqL47vwR/+dq97UQGeYVOTp7/nQ9qAzPjfxn7cSaAKdEjUdQm/9xJw/H3GwxXlBQWMtOgW1UdeLrUW5nst90F+HLNfnPn1yR6dVvCEtWaC7/lFOi+XW82U+nnHzp6Y1/8sbegNCSrwC3Znodnv9+M1xZsN9znk1W5hnegkSZqk7+oPWYToZVdGm+floFxJqeNIJBbaXKjauyDeoruE6crUHq2Ep+synXvZqpzlyC5P3S8jXD3paKqGoPeWIZH/rfe985+cq7NUVBifHEaPysbY6attvzcoSB1/iJoZvrjr95TiLs1PW2sOK5jP++vbzlUhHs+qjn3Le/VLDmhbbt4Zd42fJ6xH6lN62JwN2vHi4jQc47G/3WX/l1cMGpzOu/aYGYlrxlElEdEWwxeJyKaQkQ5RLSJiPpZH6aIZGa6ZE5eZDxK2hu9C4B2gRxf1T4HvAx0U08kV801PYzU4xh0V3AzOOd/f9mND1fu1X0t1G58ZyX++4v3xXwiWX7xWVRWVVszyFFuzXwyU+3zEYChXl4fBqCz8u8+AO8FH5Y1zm/VMNwhxAQzJfSNAUwz8fGqfWj/zDyP7doLSbmP6SXGz8o2fG1lTk39LDPrVjnolfgYjjaCo5qpNV6dv91trqXatPHASbw637g+OhhfrT2A6976NSTHBhwX4QsnLsILs7YENaQilIXzYKcBjzRmlnH8FYC3+Q1GAPiEHVYDaEJErawKMBjnNKwT7hBigtnBWFbJOuh+Idmw3/8Li57Hv9zoVy+e2z/IwPApyy05d6R7+ttN2HHM2l44q3YXIlOZG8vZg+anbN8TBZoRit5Y/i5DGumsaPBtDUC9duBBZZsHIrqPiDKJKDM/39zMkVa5qH2zWj1fLKntbo/LQ1Cf641+tY/j/8LScry9ZFdA8xBFUw3y1sOn8NgXG7zOgKs1ZtpqjJy6ym2boxtu4J9MbfTCipZuvrXa24eZ32fmdGZOT0lJqc1Th/R2MNZFU7dH/Wofz/3UP/IbP+90TXIXqx7533rMzjrsMaW4Wa5ZVi2Kp6KKMXHuVq8r6vkrmF5IkciK3j6HALRRPU9VtkWUaPvFRRJ/SnuR7pedjjvSqmrGmfIqLMg+oj/9hOaKV1nt37TWdpJ9uGamVsOSeZB/Xuq3+1tQu3XqKjAYXz9widv2acv34kxFFV6+sWdwwWlES2HHipL/bAB3Kb1+BgIoYuYjFhzXEh//YYDu9kFdU5D5/DW1HE10entpTrhDsIyzL/e4bzfj/PEL8MTMLN31DTzWIWbPXkha1dWMtSFY/4GZMX9zaP7k8ovPug2y8534gsuMgSTWNbnHsTb3hO77K6t8HzC3oFR/OVWNaKs9MNPV8wsAqwB0JaKDRHQvET1ARA8ou8wDsAdADoBpAB4KWbQBSFDWOSQC1jx3tWv7f3+fjhYNksMVVlRZtj24aZKDobdKlxXOBDBC9ITOqm7MjBW7ClBWUYUPVuzBqKmr8OtO4/aud5flYHbWYY/ta3OPY8fRYrw6bxsOnXS/E1myPQ8Pfm79oCYAKNbMMWWUSl1toSaSd3U1eyyQFM7EetUby3D1v34xvX+0lPx9Vvsw8xgfrzOAhy2LyGLOXxSRe++fJD8mFRPeVYXxryFcc+HrLaepZ8GWo67E3LN1YwDw6B6q9vqCHQCAG3qf57Z9lKphdE3ucXz/0KWu56H8DDzvcBh6dTz+NNKuzT3udbW4YK4DoWyMdca1IPtoyM5Rm6J+hG+0tMxHMj9XcbTU5a8vDct5td8rBrv1+FmxqwB3Ts/A5Z1buLZtPuS5ylkgtG0soRx56u8aC2b+2vQKCzWzrPo36V5t2HWs2GMsybFTZWjZyN5dyWOm+CsNvqFTUHIW6S8vCncYtWrLIfdFY6oZblUHd07PAKDfLfWLte7rMOv1SEkbN9fw3P5+k/NOlWGXjz76VdWMyYt26sTieZHzFpOZm8CEOOO0w25H818obkKHvPWrx+SClVHQySHqk7/2y/DMsG74+/91D08wUczbZFex4E+fZKLYZPuDc1Ca807g6W82BXVubarcV1jq1vg84JXFGPLWryivrMbS7XlYsMW9cbjodAX++k0WJi/ahX/McR+dbLZ6y5/FceLj9EZMR34yVd+RVFcz1u07bmp8R3llNRZutWbwmpWiP/kr/zt/cfdf2RH3XNre9fr430XnhSApPup/tRElP4BpiJ3VN4U+Lpze2ggAYL4mmV/5z2V4ee42j/26PD8f93y0Fg985t443HvCz/huvaN3tnqu+qXb8zBEM6WDYfL3o7Sul/xrThBc429tXUK+XX8Qt7y3Cj9u8t3L6o2fd+BPn2RG3FTPUZ8hWjRIAgCc36qR7utN6ye6Hv/w8KW6+9iS1HJFjXe0XWk12XHRNs/eVoGWNNUl8M8z9nu87msSPTPVLvF6cyUFkLVPlVVg4lztnUoo039N3HuUdan3G0xTvmjrMczaeEjZxzGxoHpZ0kgQ9Q2+Pc5rjG8fvBi9Upvovq5ux+nTRn8fPdf3PBfzNkduq39o/wiElaoZOHjiNFKb1tN9XVslUnymAhl7CpHarB7eW6Y/xkJv5TS1eZuP4NCJM5izyb1bqa+vjWFXT2e1j84OB46fRrP6Saif7Eg3elX+q/YUej2+nn/9tAMfr9rner7rWDFSGtZO921fcf7xk0wAwIg+rb1+NuEU9ckfAPq3M57Xp0ozMvO1W3rib99u9nq881s1QpN6SZbEZlaLBsl+1atHQXtUVCvSjAkYM201lj89WHdfbdLYU1CK295fjUZ1EnCqTL+dwdev/6EAxwXsyS9BYnyc4Z20nstfX4o+bZq47qzjdEr+znhKzlaavmkt1wzgGvLWrxh9YRuDvYOnDttZuNqVV4Lisgo0rJNo8K7IHRwW9dU+vmhb7W+7sK3P94wZ4PkFa1TH+Do6pHvwi4Is/cuVfu1vZmlFET7aGTJPlPo/B41R4gcCL2Wq36ctGAHADW+vxLB/G89katRwu/HASVRWVeOW937DSs1ymaOm/ub2PJiuq5n7Tvjc56Z3V+Lpb7J87mfGrI2HMXbGGlP7Rlqjdswn/2qDIvIdF7XFny5v77atTqLj4xrSvSVG9k91e+13mkE5andfkhZckIDXkoUe9R+xc3CREW8XLhEa5ZrF60u89BSavyWQ6kXGV5kHdNcfNuNUWQWW7jA38+6Uxbuw/ajjYubtopO57wTW7Tvh0RjtnJrBKZiCck6e+zQNeteRDftP4qvMg0Gcxf3Oar2PKcVrxjAEdUrLxfxfvV5/3dxJw12PS85W4os1B/DwoI54dHBnFJdVIqVhMlo1ruv2Hr1f7D2XpuHDlbk4r0ldzxcD8NjVnXHqTAU++i3Xr/fdmp7q9wCjpvUSdacrENZwjgMwI5ARvFXVjKe/2eT3FCbO0mlhiflzehutq+arHcLJ/Aha/7JpydlKnwu7/99/VqB9i/qYMqavx2vq64hfbWqubrCRJeZL/oO6nmNqv9ZN6qFOYrxbg5Kzx1piPOHOgW3RqrH7iL9HB3fGynGD0b5Ffax9LvhJ5J4c0gUv3tDD7/c1rmB7ASQAABSTSURBVJfktd5R70u5Yfy1fp9HBMfKRnrnhGaFpYGNv1gUgn7pZn+895b5XoqSmfH9Bv8mDx70xjKfgxE3HyrSnVsJcK+OirRSfCBiPvmntaiPuY9dhvmPX+51P736ujXPXYNf/zoIuyZejx7nNcbEmy5wez2eCK2VUn+oeyGMvbgdeqXWVO+0bFRzPgLQINn4Ju/8c8033onQsWr6ByDwEag/ZR9DXnEZJs7zHCeglX24yGMk8l0z1rg6Jmw7csrrSOVg/LjpCMoq/JtXJJCxGGrq8tNOk6uaFZacxfp9zhlHI+uKEfPJH3B0BzXqvTAq3dG4e3knz8VnWjRIRtvmNd3zPOrlfVReTrsrHU8P7Yq1z12DeY95v/iofXbvRR7bLu7YHLMfuczt2GrfP1Qz1/nk2/q4Hr92S0+8NrKX6XOL0DlbGdgkSc3qe/Y8c85KGki+yTpg7iK0WGd8wfHSctcC9ks0s71q2zn8teVQEVYo02WcDHOf+d355hat6f/yIhzxMUgvXCT5+9CvbVPkThruluSNaHO9t6qWOHI0HD90VSekNEz2qzvYZZ1bYNGTV+KWfqmG+zRVdUUlAjqd0xCZz1+DsRe3w/U9W+Hey9pjaI9zcduFbVE3MV73GLdf5Lvnk56ru5mrShPujDof1LY/KX3UfTH6yhpdcP5o8rhqC7Ycxe78EjwxcyN+958VfrWVWC1Su2wGKuYbfK3kTxc1f/7M9foudzqnARrq9NI5p2Ey8gxub1s0SMZLIxxVUy8YTGuxctxg1xD/V27qif/pjPL0Ra8kKnwLdGrsSOvWa2U0D3y2zsKjhVeE/ZrMlfyJaCgR7SCiHCIap/P63USUT0QblX9/tD7UyKedssTbL1v7WuO6xl05HxncycTZHSdf+MSVWP70IO/zp3jRuklddDqngc/9bkt3vyAN6lpTLUYEZL90nd/n/sOl7X3vFMVunxZYqTZcdwx2KAlrZ18Nhrc2hi/W+C4k2a6fPxHFA3gHwDAA3QGMISK9YuNMZu6j/PvA4jhtwaNLpx+/6/Oa1MWcRy/D9n8M9XhNb0Sk0fbG9RLRplk9t1h8TboVyB9xm2buP+sVXWqSfxwR6icnGN5dGHH+cTx3/fn+BxTDvA32CiWjO933lu3GGz/tCNl5/Wk7cDaiG73nSJHO+swq6tlRH/6f8ajoZ77zPitAJDJT8h8AIIeZ9zBzOYAvAYwIbVj21LJRHWx4YQj+fE1nAECdJOOP95FBnqX5C1o3Rh2d+vcm9fTvCswW7ru09F2SBxxVRka07QIPXtUJM+6uaVS+Nb0NJoxwdEN1JgVnfBe09q83UVyAdy0icoRyXeeKqmqMn5Xt13uMum9e/OoSw/fk5BWj03Pz/TqPN3as9mkN4IDq+UFlm9YtRLSJiL4hIt0JNojoPiLKJKLM/Hxzowftpmn9JPz5mi7InTQcyQn6Dam5k4bjL9d1NTxG7qTh+MeNNd1G6yXpN82YLbF3btnQ1H56383cScORO2k4tqnuSBLjCfFxhMHdWrqql+KIXEnfGZev8K45vyXuv6KDqdj81SA5ATf1df+atmlWF4+aqkITkSyQ9ZUDqRqzssoIcCT/b9cdxAfL91h63EBZ1dvnRwBpzNwLwEIAH+vtxMzvM3M6M6enpHh2nYwFTQ1K8Vq/H9jO5z5G1UH+8vcoiaq1ApzvJYKraOMsuDtL8Oef2wh92zbxqNP/YGw6nlFV8VhZMkpKiEOyZp3myzql4Klray66b9/uOYpTmOfr6+erSiVQl3gprVvJ3zr6F2f7vht56uss3bUWwsFMb59DANQl+VRlmwszq1cp+ADA68GHFn3mP365tYO93P74PL+ob97aG+tMTHTVQOk1pC0pGxmlmtfIVconoG/bpgCAq7s5JrLr387x/PqerTCo2zn4foO5+VSsuKRd0rG562KS3q4pzmtSFy9pRkcPu6AVgA0WnE3o+Wy1/z3FzNCup+tLdTVj40HP+XdKTa68Zpa/066Em5mS/1oAnYmoPRElARgNYLZ6ByJqpXp6A4DIuLRFmPNbNfJ7rhVv1CX/wd08Zw69uV8qJt7U0+dx6iUlIPul6zBuaDev+40Z4Oj3b9SQe0HrxsiZOAyDlH7+Pc5rjN2vXO96rjNJpE/xceQ2Md7jV3f2ur9zJDMRubpOjkpPxZQxfZGkuRNQX2SiaiGfWmKXdbEnL96l22W5x99/cnv+n8W7XI/7TvjZ8p8vwqr8fSd/Zq4E8AiAn+BI6l8xczYRTSCiG5TdHiOibCLKAvAYgLtDFbCo4fxq/uXaLh6JzV/1kxN8NrS+fOMF2DrhOiSoqn3evaM/0ts1dS0bmaBZPlLd5dSoP/qiJ6/EqmcG45HBnXBdj5YYle4+eE3dDfaOgW2RO2k4Xr1Z/6L2stJWwsxIjHecu1l9/QuuutrCn4V8hL1MUSV1b/6lmqAuFJMa2nJ6B2aex8xdmLkjM09Uto1n5tnK42eYuQcz92bmQcy8PZRBC4faXiEoPo48Gp+HdG+Jbx68xNQAN3WYvVXzEHU6pwFaNa6LFg2S8d/fp3tMk/HgVR09jqUeZ/CdMnVFckKcq4trx5QGGDfsfIwb1s1wxLFRzI9Jo7BPkTIaOZQirV++1WR6BxtzVvvY5SvqLPmM7J/qs5plzqOXud5TJzHetRaz81Y8Lo5wScfmABxTcOx+5Xpkv3QdBrRvhpn3DcRjV3dG47qJeODKjn53HX3yWveeWG/e2tuv98eCG99diZNnImtN2kiXfdja3kPBkuRvY+1b1AcAtG3me96hSOAsLMaruoUa6XFeI7RslIxJNxtPOjfj7gux5tmrHceMI1eV00Udmgc8wrnm2DVjGIb3amW43/1XdMCiJ6/E0B7nBnU+u9l0sAj1DbogRwur6/wjrUE4un97NjdlTF+c9tIj4aa+rdGmWT2kK71qIp2zekpvAW8tIkLGszVrIEy7Kx0f/ZaL5qp5g+okxusOirOCugE93suFatywbiAiTB7dB+O+3YTLO6fgqa+tWSIw0mlXzRL2Isk/gt3gZWlIwJEgL0wzXpw+0lzUwRHr//n4ufT0bdvU1ZU0VAZ1TdFdutDbXYTzDqZOYjwmj3aMG4iV5G80ajZaSJ2/EBbpmNIAuZOG45KOLcIdissFrRvhdWU9gxl3X4g9r1zvsQ8ReazS5k3W+GsxZkBbvDHK2raCJ67pYunxhHdPzIzui7gkfxHT5jx6OW5Veg4RkVvj8OxHLnWNK9BOn/36yF74q8EUHY3rJeLVm3tiZH/j9Rac/Bmk/djVvnshqacFEZErlBPfmSXVPiLm9G7TBFkHPEd8avVKbYJeqY7+/wlKQ0WHFvWREE+uC0aw9r46HF+s2W9qVkhfjeSj+qfijgFtUT8pHk9+Fd2lVruLhDUYpOQvYs7X91+MrRP8W29g6p39cf+VHbD4qSvx8xNXmn7f00O7onurRiACbumXinXPX4O5jzm6sSYodxljBrTF9n8MRbP6SejXNvDBZv8c1RtxcTXtQNo1pdX0xk5oLXryCuROGh5wPL7smjgsZMeOdJGwFgKFa9RZeno6Z2b6v6ybENHAuVpa3ST33koFJWeR/vIivDGqN/6iajg+v1UjzH/8crd9AOCqrilYpjRS6yXq8bO24JNV+zy2O/c1WmC9S8sGrovc6j2FGP3+ar9+PjNyJw1Hxp5C3BaCY0e6RwZ18jqzrzdEtI6Z033v6Z2U/IUIg7pJ8R6JH3AstZk7abhHe8GPj1zqts/NfVvj1vRUzBh7odfzqKetuKlvayx/ehDWPneNl3c4dGhRswbEwA7Nfe4fqIs0x170pPm7Kjv7KftouEOQ5C9EpPr2wUtcj7VzJr15Wx+8PrK3z9HL6e0cVUDv3tEP/xzZC22a1dOdWTa1aV28clNPzFJGXg/qFtiU6w2Tg2tGbNe8ZsDih3d7v7DZ2a4IGCMhDb5CRKj+Jgfv1UmMQ/dW+qultW1ez2u9/bt39MPp8iq3O42N44d4XVNaq0FyAkqUwYhN6ydhwo093LpJvnpzTyzaegwAsHh7nsf7cycNx9GiMmQfLkJifJwrXu105Pde1h7TV+w1HZfwTpK/EBFs8VNX4uRp73PobP9H4A2n1/f0nLqiSb0knT0d1r8wBDl5JUhrUQ8DJi4GAGx+8Vq0f2YeAEcvlm7nul+I+rdrijED2mLWxkOu5K9t2D63cR2cqxlLob6gfXj3hRjU7RyfyT+OaqYRiWShWsHOH5L8hYhgHVPMrb9cGwZ3OwfN6idhQPtmrkn6+rdrCiLCxvFD0GfCQlzWqQXOb9UIYy9uh4pqRvP6Seh8juNnGNGnNYZ0b4kTpytMrWhXNyne465l04vXok5CPLo877627qSbe2Lcd5uREBeHL+8fiGNFZXjwc88F1xvVSUDW36/Fze/9hg37a7r79mvbBOv3++7+a1adxDiUVRgvYBHsFOxWkN4+QgifnHlCPdYgJ68YLRvVcU3Bvb/wNM5tXKdWEtu36w7iqa+z8NfruqJXamMM7NAcnZ+bj+G9WuGd2/sBABZvO4Z7P3bkmFaN6+DhQZ1weecWaNfcMSHimfIqnC6vxKT52/HXoV1ddzLdzm2I7UeL/Ypnxd8G4bLXlgKouUsx6kkFAI8O7uS2pKg/rOrtI8lfCBEVDp88g+YNkpCcUNOLqvRsJRiOMRW+JgGcs+kw+rVtiuzDp/CnTxy56emhXXHnwHbo9eLPAIDWTerirdv6oFXjOmjeIAmlZ6uwJ78EF3VojrKKKiSoZpddtbsQ01fswXPDu+P2aatxpKgMfds2wYb9JzHr4UvRO8AFhGo1+RPRUAD/BhAP4ANmnqR5PRnAJwD6AygEcBsz53o7piR/IYRdzM46jCZ1E3FFl8B6Qe0vPI0NB05gRB9z62R7Y1Xy91nnT0TxAN4BMATAQQBriWg2M29V7XYvgBPM3ImIRgN4DcBtwQYnhBCRwNcMu760bV4PbZtH1robZirnBgDIYeY9zFwO4EsAIzT7jADwsfL4GwBXk5l1/YQQQoSFmeTfGsAB1fODyjbdfZQF34sAeAwLJKL7iCiTiDLz8z3nTRdCCFE7arW/ETO/z8zpzJyekhJY3ZkQQojgmUn+hwCo569NVbbp7kNECQAaw9HwK4QQIgKZSf5rAXQmovZElARgNIDZmn1mAxirPB4JYAmHqw+pEEIIn3z29mHmSiJ6BMBPcHT1nMHM2UQ0AUAmM88GMB3Ap0SUA+A4HBcIIYQQEcrU9A7MPA/APM228arHZQBGWRuaEEKIUAn/BBNCCCFqXdimdyCifACeSwyZ0wJAgYXh1BY7xm3HmAF7xm3HmAF7xm3HmAFH3PWZOejukmFL/sEgokwrhjfXNjvGbceYAXvGbceYAXvGbceYAWvjlmofIYSIQZL8hRAiBtk1+b8f7gACZMe47RgzYM+47RgzYM+47RgzYGHctqzzF0IIERy7lvyFEEIEQZK/EELEINslfyIaSkQ7iCiHiMaFOx41Isolos1EtJGIMpVtzYhoIRHtUv5vqmwnIpqi/BybiKhfLcY5g4jyiGiLapvfcRLRWGX/XUQ0Vu9cIY75RSI6pHzeG4noetVrzygx7yCi61Tba/X7Q0RtiGgpEW0lomwielzZHrGft5eYI/rzJqI6RLSGiLKUuF9StrcnogwlhpnKHGUgomTleY7yepqvn6cWY/6IiPaqPus+ynbrvh/MbJt/cMwttBtABwBJALIAdA93XKr4cgG00Gx7HcA45fE4AK8pj68HMB8AARgIIKMW47wCQD8AWwKNE0AzAHuU/5sqj5vWcswvAviLzr7dle9GMoD2yncmPhzfHwCtAPRTHjcEsFOJL2I/by8xR/TnrXxmDZTHiQAylM/wKwCjle1TATyoPH4IwFTl8WgAM739PLUc80cARursb9n3w24lfzOrikUa9SpnHwO4UbX9E3ZYDaAJEbWqjYCY+Vc4JuALJs7rACxk5uPMfALAQgBDazlmIyMAfMnMZ5l5L4AcOL47tf79YeYjzLxeeVwMYBscix9F7OftJWYjEfF5K59ZifI0UfnHAAbDscIg4PlZ661AaPTz1GbMRiz7ftgt+ZtZVSycGMDPRLSOiO5TtrVk5iPK46MAWiqPI+1n8TfOSIn/EeX2d4az6gQRGrNSrdAXjtKdLT5vTcxAhH/eRBRPRBsB5MGRAHcDOMmOFQa1MRitQFircWtjZmbnZz1R+azfIqJkbcya2PyO2W7JP9Jdxsz9AAwD8DARXaF+kR33ZxHft9YucQJ4D0BHAH0AHAHwr/CGY4yIGgD4FsCfmfmU+rVI/bx1Yo74z5uZq5i5DxyLTg0A0C3MIfmkjZmILgDwDByxXwhHVc7frD6v3ZK/mVXFwoaZDyn/5wH4Ho4v3zFndY7yf56ye6T9LP7GGfb4mfmY8odTDWAaam7NIypmIkqEI4l+zszfKZsj+vPWi9kun7cS60kASwFcDEfViHP6enUMRisQhiVuVcxDlao3ZuazAD5ECD5ruyV/M6uKhQUR1Seihs7HAK4FsAXuq5yNBTBLeTwbwF1K6/1AAEWqaoBw8DfOnwBcS0RNldv/a5VttUbTRnITHJ+3M+bRSm+O9gA6A1iDMHx/lDrk6QC2MfObqpci9vM2ijnSP28iSiGiJsrjugCGwNFesRSOFQYBz89abwVCo5+ntmLerioYEBxtFOrP2prvR6Ct1OH6B0dr90446vKeC3c8qrg6wNFDIAtAtjM2OOoQFwPYBWARgGZc08r/jvJzbAaQXouxfgHHbXsFHHWD9wYSJ4A/wNEYlgPgnjDE/KkS0yblj6KVav/nlJh3ABgWru8PgMvgqNLZBGCj8u/6SP68vcQc0Z83gF4ANijxbQEwXtneAY7knQPgawDJyvY6yvMc5fUOvn6eWox5ifJZbwHwGWp6BFn2/ZDpHYQQIgbZrdpHCCGEBST5CyFEDJLkL4QQMUiSvxBCxCBJ/kIIEYMk+QshRAyS5C+EEDHo/wENPLvj9eS02gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trVA31DsKtvI"
   },
   "source": [
    "## Actividad 5:\n",
    "- Se puede ver el grafico de como disminuye la perdida. Se ve bastante inestable la perdida y puede ser por el force teaching\n",
    "- Con la antencion se deberia poder disminuir esta inestabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fMd_AKuDWRE"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3e0XdoaGDZAI"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train_attention(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gm_Fk8NpDpX0"
   },
   "outputs": [],
   "source": [
    "def evaluate_attention(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_human, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_machine.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z15EGAQMD4sX"
   },
   "outputs": [],
   "source": [
    "def trainIters_attention(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train_attention(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xyB4YMzyKYQJ",
    "outputId": "b1b321ca-9590-462f-fc69-405f265455a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953400"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trainable_parameters(encoder1)+num_trainable_parameters(attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1n8fUsFLNgm"
   },
   "source": [
    "## Actividad 6\n",
    "Tiene `953400` parametros entrenables el modelo\n",
    "- Los tamaños de origen y destino no cambian. 15 y 8 respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "colab_type": "code",
    "id": "o4FgQPBNDvV5",
    "outputId": "1e6f7c20-402b-4c0b-ed5e-ec83169e0542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 33s (- 7m 21s) (1000 7%) 1.7066\n",
      "1m 6s (- 6m 36s) (2000 14%) 1.2459\n",
      "1m 38s (- 6m 0s) (3000 21%) 0.9911\n",
      "2m 10s (- 5m 26s) (4000 28%) 0.7961\n",
      "2m 42s (- 4m 52s) (5000 35%) 0.6728\n",
      "3m 15s (- 4m 20s) (6000 42%) 0.5626\n",
      "3m 47s (- 3m 47s) (7000 50%) 0.4391\n",
      "4m 19s (- 3m 14s) (8000 57%) 0.4468\n",
      "4m 52s (- 2m 42s) (9000 64%) 0.5072\n",
      "5m 24s (- 2m 9s) (10000 71%) 0.4223\n",
      "5m 56s (- 1m 37s) (11000 78%) 0.4345\n",
      "6m 27s (- 1m 4s) (12000 85%) 0.3041\n",
      "7m 0s (- 0m 32s) (13000 92%) 0.2605\n",
      "7m 32s (- 0m 0s) (14000 100%) 0.2777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_human.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_machine.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "plot_losses = trainIters_attention(encoder1, attn_decoder1, 14000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQKorCTjLo1l"
   },
   "source": [
    "## Actividad 7\n",
    "- Se ve mucho mas estable la perdida que en el caso sin atencion. Tiene sentido que exista menos confusion al tratar de asignar los pesos si existe una atencion que facilita esta configuracion. \n",
    "- Tuvo una velocidad de convergencia mucho mas grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "TErTY2fnETbl",
    "outputId": "436ef046-5539-445b-bcdd-5fab9d7289ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f10579fe438>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate_attention(\n",
    "    encoder1, attn_decoder1, \"jump thrice and look\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vQFUPhTaJSGY",
    "outputId": "d2ef6625-fb87-4d70-d604-acf9e4c9e935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = turn opposite right thrice and turn opposite left\n",
      "output = I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT <EOS>\n",
      "input = run right twice after walk right twice\n",
      "output = I_TURN_RIGHT I_WALK I_TURN_RIGHT I_WALK I_TURN_RIGHT I_RUN I_TURN_RIGHT I_RUN <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate_attention(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"turn opposite right thrice and turn opposite left\")\n",
    "evaluateAndShowAttention(\"run right twice after walk right twice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1xI62VWKGJT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaSrmnkVNgs6"
   },
   "source": [
    "## Actividad 8\n",
    "Se pueden ver las matrizes de los coeficientes de atencion de las dos primeras instancias del test. \n",
    "-  Es interesante que 'run' tenga poca antencion al inicia de la oracion. Es posbile que pocas oracions de entrenamiento inicieon con 'run'\n",
    "- Se puede ver una relacion temporal como entre las palabras del humano y la maquina.\n",
    "- Existen valores que tuvieron bastante atencion pero no fueron elegidos o preferidos para estos resultados."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMBD_SCAN_RNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
